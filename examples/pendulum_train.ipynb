{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from AEMG.data_utils import DynamicsDataset\n",
    "from AEMG.systems.utils import get_system\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from AEMG.models import *\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for:  cartpole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:17<00:00,  5.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# config_fname = \"config/pendulum_lqr_1K.txt\"\n",
    "# config_fname = \"config/physics_pendulum.txt\"\n",
    "# config_fname = \"config/hopper.txt\"\n",
    "config_fname = \"config/cartpole_lqr.txt\"\n",
    "\n",
    "with open(config_fname, 'r') as f:\n",
    "    config = eval(f.read())\n",
    "\n",
    "dataset = DynamicsDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = Encoder(config[\"high_dims\"],config[\"low_dims\"])\n",
    "dynamics = LatentDynamics(config[\"low_dims\"])\n",
    "decoder  = Decoder(config[\"low_dims\"],config[\"high_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(set(list(encoder.parameters()) + list(dynamics.parameters()) + list(decoder.parameters())), \n",
    "    lr=config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.001, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2848a0db654943b4dd76cdde9114a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 8.510286204516888\n",
      "Epoch: 0, Iteration: 200, Loss: 8.424584165215492\n",
      "Epoch: 0, Iteration: 300, Loss: 7.619055408984423\n",
      "Epoch: 0, Train Loss: 0.26547688173283773, Test Loss: 0.007615494455412491\n",
      "Epoch: 1, Iteration: 100, Loss: 0.6947640776634216\n",
      "Epoch: 1, Iteration: 200, Loss: 0.5681277327239513\n",
      "Epoch: 1, Iteration: 300, Loss: 0.49405514914542437\n",
      "Epoch: 1, Train Loss: 0.022062537471581364, Test Loss: 0.00437095335003027\n",
      "Epoch: 2, Iteration: 100, Loss: 0.4239791836589575\n",
      "Epoch: 2, Iteration: 200, Loss: 0.3908626737538725\n",
      "Epoch: 2, Iteration: 300, Loss: 0.3705483677331358\n",
      "Epoch: 2, Train Loss: 0.015476181193437312, Test Loss: 0.003538786292018503\n",
      "Epoch: 3, Iteration: 100, Loss: 0.3544613844715059\n",
      "Epoch: 3, Iteration: 200, Loss: 0.34005571180023253\n",
      "Epoch: 3, Iteration: 300, Loss: 0.32106692530214787\n",
      "Epoch: 3, Train Loss: 0.013290838180013845, Test Loss: 0.003089103559704171\n",
      "Epoch: 4, Iteration: 100, Loss: 0.30508106877096\n",
      "Epoch: 4, Iteration: 200, Loss: 0.2927777443546802\n",
      "Epoch: 4, Iteration: 300, Loss: 0.2811656887643039\n",
      "Epoch: 4, Train Loss: 0.011498968275359919, Test Loss: 0.0026591428622757037\n",
      "Epoch: 5, Iteration: 100, Loss: 0.26545497844927013\n",
      "Epoch: 5, Iteration: 200, Loss: 0.2619572684634477\n",
      "Epoch: 5, Iteration: 300, Loss: 0.25374243466649204\n",
      "Epoch: 5, Train Loss: 0.010364203610620702, Test Loss: 0.0025083756712793383\n",
      "Epoch: 6, Iteration: 100, Loss: 0.2547030078712851\n",
      "Epoch: 6, Iteration: 200, Loss: 0.24885772704146802\n",
      "Epoch: 6, Iteration: 300, Loss: 0.24867463973350823\n",
      "Epoch: 6, Train Loss: 0.009939428511286904, Test Loss: 0.0024348800802680174\n",
      "Epoch: 7, Iteration: 100, Loss: 0.24916561204008758\n",
      "Epoch: 7, Iteration: 200, Loss: 0.23910214414354414\n",
      "Epoch: 7, Iteration: 300, Loss: 0.24278416787274182\n",
      "Epoch: 7, Train Loss: 0.009697791300569997, Test Loss: 0.002376791601040468\n",
      "Epoch: 8, Iteration: 100, Loss: 0.23952923878096044\n",
      "Epoch: 8, Iteration: 200, Loss: 0.23979351681191474\n",
      "Epoch: 8, Iteration: 300, Loss: 0.24120110482908785\n",
      "Epoch: 8, Train Loss: 0.009502944991366995, Test Loss: 0.002349705046484458\n",
      "Epoch: 9, Iteration: 100, Loss: 0.2339982066769153\n",
      "Epoch: 9, Iteration: 200, Loss: 0.2323247753083706\n",
      "Epoch: 9, Iteration: 300, Loss: 0.23380355210974813\n",
      "Epoch: 9, Train Loss: 0.009285864927901974, Test Loss: 0.0022531840626207975\n",
      "Epoch: 10, Iteration: 100, Loss: 0.22696816828101873\n",
      "Epoch: 10, Iteration: 200, Loss: 0.2188611812889576\n",
      "Epoch: 10, Iteration: 300, Loss: 0.1991711250739172\n",
      "Epoch: 10, Train Loss: 0.008205287680557938, Test Loss: 0.001553993675732966\n",
      "Epoch: 11, Iteration: 100, Loss: 0.12934414815390483\n",
      "Epoch: 11, Iteration: 200, Loss: 0.07750010656309314\n",
      "Epoch: 11, Iteration: 300, Loss: 0.049577708763536066\n",
      "Epoch: 11, Train Loss: 0.0030173385601620353, Test Loss: 0.0004033656813655548\n",
      "Epoch: 12, Iteration: 100, Loss: 0.03730833195731975\n",
      "Epoch: 12, Iteration: 200, Loss: 0.0361883881414542\n",
      "Epoch: 12, Iteration: 300, Loss: 0.03488605783786625\n",
      "Epoch: 12, Train Loss: 0.0014128234779303799, Test Loss: 0.0003258145927960418\n",
      "Epoch: 13, Iteration: 100, Loss: 0.03194776839518454\n",
      "Epoch: 13, Iteration: 200, Loss: 0.03140510819503106\n",
      "Epoch: 13, Iteration: 300, Loss: 0.030834070363198407\n",
      "Epoch: 13, Train Loss: 0.0012356402373755082, Test Loss: 0.00030255716200437894\n",
      "Epoch: 14, Iteration: 100, Loss: 0.029482284357072785\n",
      "Epoch: 14, Iteration: 200, Loss: 0.029210015854914673\n",
      "Epoch: 14, Iteration: 300, Loss: 0.028644649588386528\n",
      "Epoch: 14, Train Loss: 0.0011590065568992773, Test Loss: 0.00032322809892855394\n",
      "Epoch: 15, Iteration: 100, Loss: 0.028135765562183224\n",
      "Epoch: 15, Iteration: 200, Loss: 0.028202635105117224\n",
      "Epoch: 15, Iteration: 300, Loss: 0.028572553346748464\n",
      "Epoch: 15, Train Loss: 0.0011103251236658927, Test Loss: 0.0002710643182712674\n",
      "Epoch: 16, Iteration: 100, Loss: 0.02730624779360369\n",
      "Epoch: 16, Iteration: 200, Loss: 0.027227704777033068\n",
      "Epoch: 16, Iteration: 300, Loss: 0.02608114185568411\n",
      "Epoch: 16, Train Loss: 0.001058155799479003, Test Loss: 0.0002614907346084983\n",
      "Epoch: 17, Iteration: 100, Loss: 0.02610493487736676\n",
      "Epoch: 17, Iteration: 200, Loss: 0.02544873606530018\n",
      "Epoch: 17, Iteration: 300, Loss: 0.024562475766288117\n",
      "Epoch: 17, Train Loss: 0.0010124174477448495, Test Loss: 0.00025310287835345285\n",
      "Epoch: 18, Iteration: 100, Loss: 0.024653012835187837\n",
      "Epoch: 18, Iteration: 200, Loss: 0.024885469727450982\n",
      "Epoch: 18, Iteration: 300, Loss: 0.024013398025999777\n",
      "Epoch: 18, Train Loss: 0.0009728566743133436, Test Loss: 0.00024559080196285296\n",
      "Epoch: 19, Iteration: 100, Loss: 0.02487588016083464\n",
      "Epoch: 19, Iteration: 200, Loss: 0.023392797171254642\n",
      "Epoch: 19, Iteration: 300, Loss: 0.02288050003699027\n",
      "Epoch: 19, Train Loss: 0.0009376610962004334, Test Loss: 0.00023011216391526047\n",
      "Epoch: 20, Iteration: 100, Loss: 0.022507514368044212\n",
      "Epoch: 20, Iteration: 200, Loss: 0.0223801709653344\n",
      "Epoch: 20, Iteration: 300, Loss: 0.02276905460166745\n",
      "Epoch: 20, Train Loss: 0.0008936456945297529, Test Loss: 0.0002400780837147583\n",
      "Epoch: 21, Iteration: 100, Loss: 0.021826903386681806\n",
      "Epoch: 21, Iteration: 200, Loss: 0.021266316340188496\n",
      "Epoch: 21, Iteration: 300, Loss: 0.021377275210397784\n",
      "Epoch: 21, Train Loss: 0.0008543607691535726, Test Loss: 0.00021120914112243648\n",
      "Epoch: 22, Iteration: 100, Loss: 0.022106975258793682\n",
      "Epoch: 22, Iteration: 200, Loss: 0.019937536533689126\n",
      "Epoch: 22, Iteration: 300, Loss: 0.020126126008108258\n",
      "Epoch: 22, Train Loss: 0.0008214039679439066, Test Loss: 0.00019983829826611028\n",
      "Epoch: 23, Iteration: 100, Loss: 0.020309721134253778\n",
      "Epoch: 23, Iteration: 200, Loss: 0.02038757360423915\n",
      "Epoch: 23, Iteration: 300, Loss: 0.01927818835247308\n",
      "Epoch: 23, Train Loss: 0.0007896001620839839, Test Loss: 0.00019361221820497694\n",
      "Epoch: 24, Iteration: 100, Loss: 0.018542246893048286\n",
      "Epoch: 24, Iteration: 200, Loss: 0.020711589080747217\n",
      "Epoch: 24, Iteration: 300, Loss: 0.01840040728711756\n",
      "Epoch: 24, Train Loss: 0.0007571014143603364, Test Loss: 0.00018731014271682013\n",
      "Epoch: 25, Iteration: 100, Loss: 0.01798901765141636\n",
      "Epoch: 25, Iteration: 200, Loss: 0.018369418452493846\n",
      "Epoch: 25, Iteration: 300, Loss: 0.018628249257744756\n",
      "Epoch: 25, Train Loss: 0.0007236315654266683, Test Loss: 0.0001783356615540226\n",
      "Epoch: 26, Iteration: 100, Loss: 0.018046612822217867\n",
      "Epoch: 26, Iteration: 200, Loss: 0.017956477575353347\n",
      "Epoch: 26, Iteration: 300, Loss: 0.016584211291046813\n",
      "Epoch: 26, Train Loss: 0.0007007192864116358, Test Loss: 0.00018327252421175254\n",
      "Epoch: 27, Iteration: 100, Loss: 0.016825319566123653\n",
      "Epoch: 27, Iteration: 200, Loss: 0.017706215228827205\n",
      "Epoch: 27, Iteration: 300, Loss: 0.01637483141530538\n",
      "Epoch: 27, Train Loss: 0.0006806754451467099, Test Loss: 0.0001857898424747976\n",
      "Epoch: 28, Iteration: 100, Loss: 0.01685459142026957\n",
      "Epoch: 28, Iteration: 200, Loss: 0.017325659784546588\n",
      "Epoch: 28, Iteration: 300, Loss: 0.015984734855010174\n",
      "Epoch: 28, Train Loss: 0.0006685604146452496, Test Loss: 0.00017996202748072016\n",
      "Epoch: 29, Iteration: 100, Loss: 0.016301144474709872\n",
      "Epoch: 29, Iteration: 200, Loss: 0.01769459022762021\n",
      "Epoch: 29, Iteration: 300, Loss: 0.015526833798503503\n",
      "Epoch: 29, Train Loss: 0.000655863234475039, Test Loss: 0.00015933512547713485\n",
      "Epoch: 30, Iteration: 100, Loss: 0.01606771838851273\n",
      "Epoch: 30, Iteration: 200, Loss: 0.01623305613611592\n",
      "Epoch: 30, Iteration: 300, Loss: 0.01649658870883286\n",
      "Epoch: 30, Train Loss: 0.0006544824506452586, Test Loss: 0.00016143187613780946\n",
      "Epoch: 31, Iteration: 100, Loss: 0.015346408355981112\n",
      "Epoch: 31, Iteration: 200, Loss: 0.01605692553857807\n",
      "Epoch: 31, Iteration: 300, Loss: 0.016215882511460222\n",
      "Epoch: 31, Train Loss: 0.0006360701129582144, Test Loss: 0.00015960968103735066\n",
      "Epoch: 32, Iteration: 100, Loss: 0.01640934342140099\n",
      "Epoch: 32, Iteration: 200, Loss: 0.015358387514424976\n",
      "Epoch: 32, Iteration: 300, Loss: 0.01606065376108745\n",
      "Epoch: 32, Train Loss: 0.0006328703466257964, Test Loss: 0.00015431102633790697\n",
      "Epoch: 33, Iteration: 100, Loss: 0.01614061045984272\n",
      "Epoch: 33, Iteration: 200, Loss: 0.015659086806408595\n",
      "Epoch: 33, Iteration: 300, Loss: 0.01540963830484543\n",
      "Epoch: 33, Train Loss: 0.0006221624676494609, Test Loss: 0.0001580692437950272\n",
      "Epoch: 34, Iteration: 100, Loss: 0.017099588956625666\n",
      "Epoch: 34, Iteration: 200, Loss: 0.01470304266695166\n",
      "Epoch: 34, Iteration: 300, Loss: 0.014951979435863905\n",
      "Epoch: 34, Train Loss: 0.0006235389197341253, Test Loss: 0.00016035290900617838\n",
      "Epoch: 35, Iteration: 100, Loss: 0.01473764763068175\n",
      "Epoch: 35, Iteration: 200, Loss: 0.01552642847673269\n",
      "Epoch: 35, Iteration: 300, Loss: 0.01611444976151688\n",
      "Epoch: 35, Train Loss: 0.0006106219391852195, Test Loss: 0.00015278492910792264\n",
      "Epoch: 36, Iteration: 100, Loss: 0.01552026789431693\n",
      "Epoch: 36, Iteration: 200, Loss: 0.01488101307040779\n",
      "Epoch: 36, Iteration: 300, Loss: 0.014995245670434088\n",
      "Epoch: 36, Train Loss: 0.0006142922090672523, Test Loss: 0.0001472268790185461\n",
      "Epoch: 37, Iteration: 100, Loss: 0.014608344616135582\n",
      "Epoch: 37, Iteration: 200, Loss: 0.014603817980969325\n",
      "Epoch: 37, Iteration: 300, Loss: 0.015613072893756907\n",
      "Epoch: 37, Train Loss: 0.0006000236484321727, Test Loss: 0.00015985484232888412\n",
      "Epoch: 38, Iteration: 100, Loss: 0.014641933426901232\n",
      "Epoch: 38, Iteration: 200, Loss: 0.015381370518298354\n",
      "Epoch: 38, Iteration: 300, Loss: 0.014738105157448445\n",
      "Epoch: 38, Train Loss: 0.0005892556976495771, Test Loss: 0.0001494172386994307\n",
      "Epoch: 39, Iteration: 100, Loss: 0.015633719202014618\n",
      "Epoch: 39, Iteration: 200, Loss: 0.014377019339008257\n",
      "Epoch: 39, Iteration: 300, Loss: 0.014290747167251538\n",
      "Epoch: 39, Train Loss: 0.0005879936338949603, Test Loss: 0.0002345283982012728\n",
      "Epoch: 40, Iteration: 100, Loss: 0.015401270713482518\n",
      "Epoch: 40, Iteration: 200, Loss: 0.01415545042254962\n",
      "Epoch: 40, Iteration: 300, Loss: 0.013940623452072032\n",
      "Epoch: 40, Train Loss: 0.0005841023621260976, Test Loss: 0.00014148960551595046\n",
      "Epoch: 41, Iteration: 100, Loss: 0.014530975415254943\n",
      "Epoch: 41, Iteration: 200, Loss: 0.014767417545954231\n",
      "Epoch: 41, Iteration: 300, Loss: 0.015128591352549847\n",
      "Epoch: 41, Train Loss: 0.0005877888893478157, Test Loss: 0.0001540670781060285\n",
      "Epoch: 42, Iteration: 100, Loss: 0.014212102425517514\n",
      "Epoch: 42, Iteration: 200, Loss: 0.014749791080248542\n",
      "Epoch: 42, Iteration: 300, Loss: 0.014320945963845588\n",
      "Epoch: 42, Train Loss: 0.0005784994457817166, Test Loss: 0.0001702215925330709\n",
      "Epoch: 43, Iteration: 100, Loss: 0.013952140041510575\n",
      "Epoch: 43, Iteration: 200, Loss: 0.014006309458636679\n",
      "Epoch: 43, Iteration: 300, Loss: 0.015034387091873214\n",
      "Epoch: 43, Train Loss: 0.0005707808892655465, Test Loss: 0.00014001381650720674\n",
      "Epoch: 44, Iteration: 100, Loss: 0.014728554029716179\n",
      "Epoch: 44, Iteration: 200, Loss: 0.01392194686923176\n",
      "Epoch: 44, Iteration: 300, Loss: 0.01360114539420465\n",
      "Epoch: 44, Train Loss: 0.0005648670555002382, Test Loss: 0.00022018624868090314\n",
      "Epoch: 45, Iteration: 100, Loss: 0.013957330083940178\n",
      "Epoch: 45, Iteration: 200, Loss: 0.01424693145963829\n",
      "Epoch: 45, Iteration: 300, Loss: 0.014109556897892617\n",
      "Epoch: 45, Train Loss: 0.0005619575466196168, Test Loss: 0.0001393705385181448\n",
      "Epoch: 46, Iteration: 100, Loss: 0.014231722037948202\n",
      "Epoch: 46, Iteration: 200, Loss: 0.01388624759420054\n",
      "Epoch: 46, Iteration: 300, Loss: 0.0141160881248652\n",
      "Epoch: 46, Train Loss: 0.0005596361159630235, Test Loss: 0.00014238070644685498\n",
      "Epoch: 47, Iteration: 100, Loss: 0.013968019324238412\n",
      "Epoch: 47, Iteration: 200, Loss: 0.013763434544671327\n",
      "Epoch: 47, Iteration: 300, Loss: 0.014083392219617963\n",
      "Epoch: 47, Train Loss: 0.0005496879186710468, Test Loss: 0.00013379990559769794\n",
      "Epoch: 48, Iteration: 100, Loss: 0.015106136786926072\n",
      "Epoch: 48, Iteration: 200, Loss: 0.013857374120561872\n",
      "Epoch: 48, Iteration: 300, Loss: 0.013162887313228566\n",
      "Epoch: 48, Train Loss: 0.0005634540449539872, Test Loss: 0.00014661981122023377\n",
      "Epoch: 49, Iteration: 100, Loss: 0.014119767074589618\n",
      "Epoch: 49, Iteration: 200, Loss: 0.013576649311289657\n",
      "Epoch: 49, Iteration: 300, Loss: 0.013652387162437662\n",
      "Epoch: 49, Train Loss: 0.0005459537133742508, Test Loss: 0.0001422132057265004\n",
      "Epoch: 50, Iteration: 100, Loss: 0.014171963557600975\n",
      "Epoch: 50, Iteration: 200, Loss: 0.013375255228311289\n",
      "Epoch: 50, Iteration: 300, Loss: 0.013555209668993484\n",
      "Epoch: 50, Train Loss: 0.0005479473528391009, Test Loss: 0.00013652350093458886\n",
      "Epoch: 51, Iteration: 100, Loss: 0.014094274331000634\n",
      "Epoch: 51, Iteration: 200, Loss: 0.013281796454975847\n",
      "Epoch: 51, Iteration: 300, Loss: 0.01418856190139195\n",
      "Epoch: 51, Train Loss: 0.0005485956087083191, Test Loss: 0.00013747535281113742\n",
      "Epoch: 52, Iteration: 100, Loss: 0.012571862236654852\n",
      "Epoch: 52, Iteration: 200, Loss: 0.014328212440887\n",
      "Epoch: 52, Iteration: 300, Loss: 0.01336215963237919\n",
      "Epoch: 52, Train Loss: 0.000539027880016143, Test Loss: 0.00016368831201521295\n",
      "Epoch: 53, Iteration: 100, Loss: 0.013932705420302227\n",
      "Epoch: 53, Iteration: 200, Loss: 0.013766241216217168\n",
      "Epoch: 53, Iteration: 300, Loss: 0.013562949425249826\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 53, Train Loss: 0.0005463143682663003, Test Loss: 0.00013379624451585019\n",
      "Epoch: 54, Iteration: 100, Loss: 0.012745124047796708\n",
      "Epoch: 54, Iteration: 200, Loss: 0.011998190777376294\n",
      "Epoch: 54, Iteration: 300, Loss: 0.011785191316448618\n",
      "Epoch: 54, Train Loss: 0.0004920069712301783, Test Loss: 0.00012843029538455142\n",
      "Epoch: 55, Iteration: 100, Loss: 0.012342340123723261\n",
      "Epoch: 55, Iteration: 200, Loss: 0.011792512676038314\n",
      "Epoch: 55, Iteration: 300, Loss: 0.013005500848521478\n",
      "Epoch: 55, Train Loss: 0.0004909534785763322, Test Loss: 0.00012852649487070164\n",
      "Epoch: 56, Iteration: 100, Loss: 0.01223188125732122\n",
      "Epoch: 56, Iteration: 200, Loss: 0.012681259999226313\n",
      "Epoch: 56, Iteration: 300, Loss: 0.011894246279553045\n",
      "Epoch: 56, Train Loss: 0.0004909407966572206, Test Loss: 0.0001282733770569825\n",
      "Epoch: 57, Iteration: 100, Loss: 0.012360548142169137\n",
      "Epoch: 57, Iteration: 200, Loss: 0.01213553733396111\n",
      "Epoch: 57, Iteration: 300, Loss: 0.01274832540366333\n",
      "Epoch: 57, Train Loss: 0.0004908426587659501, Test Loss: 0.00012843604864175767\n",
      "Epoch: 58, Iteration: 100, Loss: 0.01297309829533333\n",
      "Epoch: 58, Iteration: 200, Loss: 0.01194324177049566\n",
      "Epoch: 58, Iteration: 300, Loss: 0.012385058260406367\n",
      "Epoch: 58, Train Loss: 0.0004906185738983584, Test Loss: 0.00012819423129033182\n",
      "Epoch: 59, Iteration: 100, Loss: 0.01229143616365036\n",
      "Epoch: 59, Iteration: 200, Loss: 0.012444015075743664\n",
      "Epoch: 59, Iteration: 300, Loss: 0.012304951524129137\n",
      "Epoch: 59, Train Loss: 0.0004900221757293132, Test Loss: 0.00012828708001047604\n",
      "Epoch: 60, Iteration: 100, Loss: 0.012493529364292044\n",
      "Epoch: 60, Iteration: 200, Loss: 0.012300886221055407\n",
      "Epoch: 60, Iteration: 300, Loss: 0.011967983540671412\n",
      "Epoch: 60, Train Loss: 0.000489520292999775, Test Loss: 0.00012792122445894963\n",
      "Epoch: 61, Iteration: 100, Loss: 0.012352288831607439\n",
      "Epoch: 61, Iteration: 200, Loss: 0.012216271185025107\n",
      "Epoch: 61, Iteration: 300, Loss: 0.012336053463513963\n",
      "Epoch: 61, Train Loss: 0.000489593438951769, Test Loss: 0.00012814714571823528\n",
      "Epoch: 62, Iteration: 100, Loss: 0.012488294589275029\n",
      "Epoch: 62, Iteration: 200, Loss: 0.011723349642124958\n",
      "Epoch: 62, Iteration: 300, Loss: 0.012333764701907057\n",
      "Epoch: 62, Train Loss: 0.0004896412581204168, Test Loss: 0.0001284686969887206\n",
      "Epoch: 63, Iteration: 100, Loss: 0.012122432282922091\n",
      "Epoch: 63, Iteration: 200, Loss: 0.012415537858032621\n",
      "Epoch: 63, Iteration: 300, Loss: 0.012299706177145708\n",
      "Epoch: 63, Train Loss: 0.0004888438108781012, Test Loss: 0.00012804487958363228\n",
      "Epoch: 64, Iteration: 100, Loss: 0.012274671033082996\n",
      "Epoch: 64, Iteration: 200, Loss: 0.011978901362454053\n",
      "Epoch: 64, Iteration: 300, Loss: 0.012425499291566666\n",
      "Epoch: 64, Train Loss: 0.0004886905136146131, Test Loss: 0.00012730338044234158\n",
      "Epoch: 65, Iteration: 100, Loss: 0.011904236016562209\n",
      "Epoch: 65, Iteration: 200, Loss: 0.012372644174320158\n",
      "Epoch: 65, Iteration: 300, Loss: 0.012557912901684176\n",
      "Epoch: 65, Train Loss: 0.00048794557722093364, Test Loss: 0.00012727587867273767\n",
      "Epoch: 66, Iteration: 100, Loss: 0.012002816118183546\n",
      "Epoch: 66, Iteration: 200, Loss: 0.012261993149877526\n",
      "Epoch: 66, Iteration: 300, Loss: 0.01228852086205734\n",
      "Epoch: 66, Train Loss: 0.0004872233632678339, Test Loss: 0.00012870577716667052\n",
      "Epoch: 67, Iteration: 100, Loss: 0.012526429280114826\n",
      "Epoch: 67, Iteration: 200, Loss: 0.012062370195053518\n",
      "Epoch: 67, Iteration: 300, Loss: 0.011990437618806027\n",
      "Epoch: 67, Train Loss: 0.0004869392710274623, Test Loss: 0.0001270206692087291\n",
      "Epoch: 68, Iteration: 100, Loss: 0.012427916801243555\n",
      "Epoch: 68, Iteration: 200, Loss: 0.012110820709494874\n",
      "Epoch: 68, Iteration: 300, Loss: 0.012021199305308983\n",
      "Epoch: 68, Train Loss: 0.00048589402278369177, Test Loss: 0.00012705506108516405\n",
      "Epoch: 69, Iteration: 100, Loss: 0.012176262513094116\n",
      "Epoch: 69, Iteration: 200, Loss: 0.012505028880696045\n",
      "Epoch: 69, Iteration: 300, Loss: 0.011715048342011869\n",
      "Epoch: 69, Train Loss: 0.00048555855181868686, Test Loss: 0.00012726631791441288\n",
      "Epoch: 70, Iteration: 100, Loss: 0.011881631384312641\n",
      "Epoch: 70, Iteration: 200, Loss: 0.012120234088797588\n",
      "Epoch: 70, Iteration: 300, Loss: 0.01230317709269002\n",
      "Epoch: 70, Train Loss: 0.0004854797959908574, Test Loss: 0.00012698542552018422\n",
      "Epoch: 71, Iteration: 100, Loss: 0.012278626953047933\n",
      "Epoch: 71, Iteration: 200, Loss: 0.012226896396896336\n",
      "Epoch: 71, Iteration: 300, Loss: 0.011983964082901366\n",
      "Epoch: 71, Train Loss: 0.00048439152347201186, Test Loss: 0.00012655656542641004\n",
      "Epoch: 72, Iteration: 100, Loss: 0.011927890831429977\n",
      "Epoch: 72, Iteration: 200, Loss: 0.012760813056956977\n",
      "Epoch: 72, Iteration: 300, Loss: 0.011822445470897947\n",
      "Epoch: 72, Train Loss: 0.00048387818272194356, Test Loss: 0.0001261455144015496\n",
      "Epoch: 73, Iteration: 100, Loss: 0.012127328387578018\n",
      "Epoch: 73, Iteration: 200, Loss: 0.012051193953084294\n",
      "Epoch: 73, Iteration: 300, Loss: 0.01228868999533006\n",
      "Epoch: 73, Train Loss: 0.0004835947101006222, Test Loss: 0.00012604430185561308\n",
      "Epoch: 74, Iteration: 100, Loss: 0.012010190657747444\n",
      "Epoch: 74, Iteration: 200, Loss: 0.012254028748429846\n",
      "Epoch: 74, Iteration: 300, Loss: 0.012109539038647199\n",
      "Epoch: 74, Train Loss: 0.00048275247337590643, Test Loss: 0.0001266338042920868\n",
      "Epoch: 75, Iteration: 100, Loss: 0.012024435091007035\n",
      "Epoch: 75, Iteration: 200, Loss: 0.012185858984594233\n",
      "Epoch: 75, Iteration: 300, Loss: 0.012183077225927263\n",
      "Epoch: 75, Train Loss: 0.0004822215780654212, Test Loss: 0.00012615832796298717\n",
      "Epoch: 76, Iteration: 100, Loss: 0.011936123795749154\n",
      "Epoch: 76, Iteration: 200, Loss: 0.011846121960843448\n",
      "Epoch: 76, Iteration: 300, Loss: 0.012146157532697544\n",
      "Epoch: 76, Train Loss: 0.0004821261302532014, Test Loss: 0.00012605952510919864\n",
      "Epoch: 77, Iteration: 100, Loss: 0.012227103630721103\n",
      "Epoch: 77, Iteration: 200, Loss: 0.01229973959561903\n",
      "Epoch: 77, Iteration: 300, Loss: 0.01209231073880801\n",
      "Epoch: 77, Train Loss: 0.00048093216511128065, Test Loss: 0.0001257709964099292\n",
      "Epoch: 78, Iteration: 100, Loss: 0.012527621613116935\n",
      "Epoch: 78, Iteration: 200, Loss: 0.011673320055706427\n",
      "Epoch: 78, Iteration: 300, Loss: 0.012066693547239993\n",
      "Epoch: 78, Train Loss: 0.000480915086788543, Test Loss: 0.0001257861439785536\n",
      "Epoch: 79, Iteration: 100, Loss: 0.011850865368614905\n",
      "Epoch: 79, Iteration: 200, Loss: 0.011571866009035148\n",
      "Epoch: 79, Iteration: 300, Loss: 0.01240440018591471\n",
      "Epoch: 79, Train Loss: 0.00048075959784910083, Test Loss: 0.00012613371902411882\n",
      "Epoch: 80, Iteration: 100, Loss: 0.012209444641484879\n",
      "Epoch: 80, Iteration: 200, Loss: 0.012168751942226663\n",
      "Epoch: 80, Iteration: 300, Loss: 0.01186152495938586\n",
      "Epoch: 80, Train Loss: 0.0004795684256247658, Test Loss: 0.00012548400166254855\n",
      "Epoch: 81, Iteration: 100, Loss: 0.011707773868693039\n",
      "Epoch: 81, Iteration: 200, Loss: 0.012175317249784712\n",
      "Epoch: 81, Iteration: 300, Loss: 0.012022596194583457\n",
      "Epoch: 81, Train Loss: 0.0004790102217326027, Test Loss: 0.0001257406329761917\n",
      "Epoch: 82, Iteration: 100, Loss: 0.012082755369192455\n",
      "Epoch: 82, Iteration: 200, Loss: 0.011920318051124923\n",
      "Epoch: 82, Iteration: 300, Loss: 0.011876188349560834\n",
      "Epoch: 82, Train Loss: 0.00047910791203996514, Test Loss: 0.00012471741179397963\n",
      "Epoch: 83, Iteration: 100, Loss: 0.01215586086618714\n",
      "Epoch: 83, Iteration: 200, Loss: 0.0119605840955046\n",
      "Epoch: 83, Iteration: 300, Loss: 0.011497160779981641\n",
      "Epoch: 83, Train Loss: 0.0004789211104918612, Test Loss: 0.0001251223660914285\n",
      "Epoch: 84, Iteration: 100, Loss: 0.012008828263788018\n",
      "Epoch: 84, Iteration: 200, Loss: 0.01192622841335833\n",
      "Epoch: 84, Iteration: 300, Loss: 0.011958278679230716\n",
      "Epoch: 84, Train Loss: 0.00047765559729575133, Test Loss: 0.00012530625496868562\n",
      "Epoch: 85, Iteration: 100, Loss: 0.011653332054265775\n",
      "Epoch: 85, Iteration: 200, Loss: 0.011908037289686035\n",
      "Epoch: 85, Iteration: 300, Loss: 0.012284513119084295\n",
      "Epoch: 85, Train Loss: 0.00047811135953021016, Test Loss: 0.00012495841393811322\n",
      "Epoch: 86, Iteration: 100, Loss: 0.011939157149754465\n",
      "Epoch: 86, Iteration: 200, Loss: 0.01178750216786284\n",
      "Epoch: 86, Iteration: 300, Loss: 0.011845230699691456\n",
      "Epoch: 86, Train Loss: 0.0004773008273782439, Test Loss: 0.0001245194029083389\n",
      "Epoch: 87, Iteration: 100, Loss: 0.012010236801870633\n",
      "Epoch: 87, Iteration: 200, Loss: 0.01157865591085283\n",
      "Epoch: 87, Iteration: 300, Loss: 0.012178624936495908\n",
      "Epoch: 87, Train Loss: 0.0004763984514133928, Test Loss: 0.00012449984873474584\n",
      "Epoch: 88, Iteration: 100, Loss: 0.012037425462040119\n",
      "Epoch: 88, Iteration: 200, Loss: 0.01203242582414532\n",
      "Epoch: 88, Iteration: 300, Loss: 0.011848645241116174\n",
      "Epoch: 88, Train Loss: 0.0004764810309819781, Test Loss: 0.00012485184658741214\n",
      "Epoch: 89, Iteration: 100, Loss: 0.011568204376089852\n",
      "Epoch: 89, Iteration: 200, Loss: 0.011981798139458988\n",
      "Epoch: 89, Iteration: 300, Loss: 0.01230324873176869\n",
      "Epoch: 89, Train Loss: 0.00047593454505705766, Test Loss: 0.00012419049977024864\n",
      "Epoch: 90, Iteration: 100, Loss: 0.012124589691666188\n",
      "Epoch: 90, Iteration: 200, Loss: 0.011698199217789806\n",
      "Epoch: 90, Iteration: 300, Loss: 0.0118765530132805\n",
      "Epoch: 90, Train Loss: 0.0004750364094377932, Test Loss: 0.00012469563627802796\n",
      "Epoch: 91, Iteration: 100, Loss: 0.012164936961198691\n",
      "Epoch: 91, Iteration: 200, Loss: 0.011787602816184517\n",
      "Epoch: 91, Iteration: 300, Loss: 0.01166904147976311\n",
      "Epoch: 91, Train Loss: 0.0004750588137239995, Test Loss: 0.00012389444007979917\n",
      "Epoch: 92, Iteration: 100, Loss: 0.011549265807843767\n",
      "Epoch: 92, Iteration: 200, Loss: 0.012121493513404857\n",
      "Epoch: 92, Iteration: 300, Loss: 0.011425473596318625\n",
      "Epoch: 92, Train Loss: 0.0004740778115108734, Test Loss: 0.00012385689846381603\n",
      "Epoch: 93, Iteration: 100, Loss: 0.01203388981957687\n",
      "Epoch: 93, Iteration: 200, Loss: 0.012278607937332708\n",
      "Epoch: 93, Iteration: 300, Loss: 0.01147272976959357\n",
      "Epoch: 93, Train Loss: 0.00047349194525296985, Test Loss: 0.0001234397575374424\n",
      "Epoch: 94, Iteration: 100, Loss: 0.011774829697969835\n",
      "Epoch: 94, Iteration: 200, Loss: 0.011326418571115937\n",
      "Epoch: 94, Iteration: 300, Loss: 0.011978381255175918\n",
      "Epoch: 94, Train Loss: 0.00047367474131139724, Test Loss: 0.00012443088750851812\n",
      "Epoch: 95, Iteration: 100, Loss: 0.01200957091350574\n",
      "Epoch: 95, Iteration: 200, Loss: 0.012062955735018477\n",
      "Epoch: 95, Iteration: 300, Loss: 0.01189435159903951\n",
      "Epoch: 95, Train Loss: 0.00047307440917458883, Test Loss: 0.00012422048226634963\n",
      "Epoch: 96, Iteration: 100, Loss: 0.011837524558359291\n",
      "Epoch: 96, Iteration: 200, Loss: 0.011604977487877477\n",
      "Epoch: 96, Iteration: 300, Loss: 0.012099160187062807\n",
      "Epoch: 96, Train Loss: 0.0004723533034548475, Test Loss: 0.00012419252226143575\n",
      "Epoch: 97, Iteration: 100, Loss: 0.011382282580598257\n",
      "Epoch: 97, Iteration: 200, Loss: 0.011779671964177396\n",
      "Epoch: 97, Iteration: 300, Loss: 0.012174684219644405\n",
      "Epoch: 97, Train Loss: 0.0004722116598751415, Test Loss: 0.00012487101213485674\n",
      "Epoch: 98, Iteration: 100, Loss: 0.012121575484343339\n",
      "Epoch: 98, Iteration: 200, Loss: 0.012023986055282876\n",
      "Epoch: 98, Iteration: 300, Loss: 0.011590341753617395\n",
      "Epoch: 98, Train Loss: 0.0004724259441778467, Test Loss: 0.00012290076689900272\n",
      "Epoch: 99, Iteration: 100, Loss: 0.011647150611679535\n",
      "Epoch: 99, Iteration: 200, Loss: 0.012026799464365467\n",
      "Epoch: 99, Iteration: 300, Loss: 0.011466101692349184\n",
      "Epoch: 99, Train Loss: 0.0004712850278178614, Test Loss: 0.00012343042908469215\n",
      "Epoch: 100, Iteration: 100, Loss: 0.011496428080135956\n",
      "Epoch: 100, Iteration: 200, Loss: 0.012106120178941637\n",
      "Epoch: 100, Iteration: 300, Loss: 0.012203218866488896\n",
      "Epoch: 100, Train Loss: 0.0004712614933954139, Test Loss: 0.00012270702022432648\n",
      "Epoch: 101, Iteration: 100, Loss: 0.012033416584017687\n",
      "Epoch: 101, Iteration: 200, Loss: 0.011837581507279538\n",
      "Epoch: 101, Iteration: 300, Loss: 0.011905371538887266\n",
      "Epoch: 101, Train Loss: 0.0004702951208077438, Test Loss: 0.0001226150764232818\n",
      "Epoch: 102, Iteration: 100, Loss: 0.01174310318310745\n",
      "Epoch: 102, Iteration: 200, Loss: 0.011627734944340773\n",
      "Epoch: 102, Iteration: 300, Loss: 0.011971807674854062\n",
      "Epoch: 102, Train Loss: 0.00046990323630545635, Test Loss: 0.00012330877514839288\n",
      "Epoch: 103, Iteration: 100, Loss: 0.01169413747265935\n",
      "Epoch: 103, Iteration: 200, Loss: 0.012088006762496661\n",
      "Epoch: 103, Iteration: 300, Loss: 0.01141689534415491\n",
      "Epoch: 103, Train Loss: 0.00046995555595392795, Test Loss: 0.00012432314370297971\n",
      "Epoch: 104, Iteration: 100, Loss: 0.011592754373850767\n",
      "Epoch: 104, Iteration: 200, Loss: 0.011983183881966397\n",
      "Epoch: 104, Iteration: 300, Loss: 0.011750250880140811\n",
      "Epoch: 104, Train Loss: 0.00046989130215976173, Test Loss: 0.00012229698288288045\n",
      "Epoch: 105, Iteration: 100, Loss: 0.011908411644981243\n",
      "Epoch: 105, Iteration: 200, Loss: 0.011909711021871772\n",
      "Epoch: 105, Iteration: 300, Loss: 0.01131836721469881\n",
      "Epoch: 105, Train Loss: 0.00046934471444644456, Test Loss: 0.0001223871641769554\n",
      "Epoch: 106, Iteration: 100, Loss: 0.011403401316783857\n",
      "Epoch: 106, Iteration: 200, Loss: 0.012343053786025848\n",
      "Epoch: 106, Iteration: 300, Loss: 0.011757271538954228\n",
      "Epoch: 106, Train Loss: 0.00046905750764615487, Test Loss: 0.00012287064630972681\n",
      "Epoch: 107, Iteration: 100, Loss: 0.01233250972290989\n",
      "Epoch: 107, Iteration: 200, Loss: 0.0118867325145402\n",
      "Epoch: 107, Iteration: 300, Loss: 0.011007830889866455\n",
      "Epoch: 107, Train Loss: 0.000468742003259021, Test Loss: 0.0001221267389014757\n",
      "Epoch: 108, Iteration: 100, Loss: 0.011904985374712851\n",
      "Epoch: 108, Iteration: 200, Loss: 0.011938184492464643\n",
      "Epoch: 108, Iteration: 300, Loss: 0.0116563411502284\n",
      "Epoch: 108, Train Loss: 0.00046798608208209583, Test Loss: 0.00012281906419582474\n",
      "Epoch: 109, Iteration: 100, Loss: 0.011796468832471874\n",
      "Epoch: 109, Iteration: 200, Loss: 0.011698663351126015\n",
      "Epoch: 109, Iteration: 300, Loss: 0.01183716970263049\n",
      "Epoch: 109, Train Loss: 0.00046820147128022016, Test Loss: 0.00012208613898297867\n",
      "Epoch: 110, Iteration: 100, Loss: 0.011530898074852303\n",
      "Epoch: 110, Iteration: 200, Loss: 0.012251781088707503\n",
      "Epoch: 110, Iteration: 300, Loss: 0.01175302456249483\n",
      "Epoch: 110, Train Loss: 0.00046760565203736947, Test Loss: 0.00012176531845866943\n",
      "Epoch: 111, Iteration: 100, Loss: 0.011654737136268523\n",
      "Epoch: 111, Iteration: 200, Loss: 0.011492063727928326\n",
      "Epoch: 111, Iteration: 300, Loss: 0.011995138826023322\n",
      "Epoch: 111, Train Loss: 0.00046726389220716507, Test Loss: 0.00012175952214579645\n",
      "Epoch: 112, Iteration: 100, Loss: 0.01161310015959316\n",
      "Epoch: 112, Iteration: 200, Loss: 0.011969887033046689\n",
      "Epoch: 112, Iteration: 300, Loss: 0.011608700755459722\n",
      "Epoch: 112, Train Loss: 0.00046745558777438795, Test Loss: 0.0001216559341122137\n",
      "Epoch: 113, Iteration: 100, Loss: 0.011989006110525224\n",
      "Epoch: 113, Iteration: 200, Loss: 0.01147660135757178\n",
      "Epoch: 113, Iteration: 300, Loss: 0.011471708305180073\n",
      "Epoch: 113, Train Loss: 0.00046626900161614104, Test Loss: 0.00012191852724801308\n",
      "Epoch: 114, Iteration: 100, Loss: 0.011498878440761473\n",
      "Epoch: 114, Iteration: 200, Loss: 0.01154479174874723\n",
      "Epoch: 114, Iteration: 300, Loss: 0.012042102658597287\n",
      "Epoch: 114, Train Loss: 0.0004664978041546419, Test Loss: 0.00012153918537150435\n",
      "Epoch: 115, Iteration: 100, Loss: 0.011655751855869312\n",
      "Epoch: 115, Iteration: 200, Loss: 0.01188784335681703\n",
      "Epoch: 115, Iteration: 300, Loss: 0.011415713466703892\n",
      "Epoch: 115, Train Loss: 0.00046600806526089637, Test Loss: 0.00012284749286242078\n",
      "Epoch: 116, Iteration: 100, Loss: 0.011655189933662768\n",
      "Epoch: 116, Iteration: 200, Loss: 0.011654316578642465\n",
      "Epoch: 116, Iteration: 300, Loss: 0.011711013001331594\n",
      "Epoch: 116, Train Loss: 0.0004661882384116137, Test Loss: 0.00012156045704614371\n",
      "Epoch: 117, Iteration: 100, Loss: 0.01176289506838657\n",
      "Epoch: 117, Iteration: 200, Loss: 0.011485712610010523\n",
      "Epoch: 117, Iteration: 300, Loss: 0.011575618365895934\n",
      "Epoch: 117, Train Loss: 0.00046552865327631135, Test Loss: 0.00012096737333832635\n",
      "Epoch: 118, Iteration: 100, Loss: 0.011409137550799642\n",
      "Epoch: 118, Iteration: 200, Loss: 0.01181571175402496\n",
      "Epoch: 118, Iteration: 300, Loss: 0.011687561200233176\n",
      "Epoch: 118, Train Loss: 0.00046505158193728684, Test Loss: 0.00012182500148876138\n",
      "Epoch: 119, Iteration: 100, Loss: 0.011682098866003798\n",
      "Epoch: 119, Iteration: 200, Loss: 0.011724982592568267\n",
      "Epoch: 119, Iteration: 300, Loss: 0.012012091778160539\n",
      "Epoch: 119, Train Loss: 0.00046561739970653885, Test Loss: 0.0001209380690814908\n",
      "Epoch: 120, Iteration: 100, Loss: 0.011704746983014047\n",
      "Epoch: 120, Iteration: 200, Loss: 0.011818823411886115\n",
      "Epoch: 120, Iteration: 300, Loss: 0.011496032391733024\n",
      "Epoch: 120, Train Loss: 0.0004644154799176793, Test Loss: 0.00012122830050486169\n",
      "Epoch: 121, Iteration: 100, Loss: 0.011342163328663446\n",
      "Epoch: 121, Iteration: 200, Loss: 0.011556640005437657\n",
      "Epoch: 121, Iteration: 300, Loss: 0.01194952754303813\n",
      "Epoch: 121, Train Loss: 0.00046427742079723644, Test Loss: 0.00012104641079072139\n",
      "Epoch: 122, Iteration: 100, Loss: 0.011109925369964913\n",
      "Epoch: 122, Iteration: 200, Loss: 0.011741839873138815\n",
      "Epoch: 122, Iteration: 300, Loss: 0.011858372337883338\n",
      "Epoch: 122, Train Loss: 0.00046380688581848517, Test Loss: 0.00012127621774891747\n",
      "Epoch: 123, Iteration: 100, Loss: 0.011545773995749187\n",
      "Epoch: 123, Iteration: 200, Loss: 0.011846156856336165\n",
      "Epoch: 123, Iteration: 300, Loss: 0.011532332471688278\n",
      "Epoch   124: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 123, Train Loss: 0.0004641140972266511, Test Loss: 0.00012105858631818508\n",
      "Epoch: 124, Iteration: 100, Loss: 0.011895986310264561\n",
      "Epoch: 124, Iteration: 200, Loss: 0.011732225349987857\n",
      "Epoch: 124, Iteration: 300, Loss: 0.011024145744158886\n",
      "Epoch: 124, Train Loss: 0.00046052861235346934, Test Loss: 0.00012058209128000956\n",
      "Epoch: 125, Iteration: 100, Loss: 0.012197688774904236\n",
      "Epoch: 125, Iteration: 200, Loss: 0.010620268436468905\n",
      "Epoch: 125, Iteration: 300, Loss: 0.011917994306713808\n",
      "Epoch: 125, Train Loss: 0.00046049387786944264, Test Loss: 0.0001206479317204998\n",
      "Epoch: 126, Iteration: 100, Loss: 0.011217970306461211\n",
      "Epoch: 126, Iteration: 200, Loss: 0.011714789929101244\n",
      "Epoch: 126, Iteration: 300, Loss: 0.011412192507123109\n",
      "Epoch: 126, Train Loss: 0.00046060692063489564, Test Loss: 0.00012048666724591928\n",
      "Epoch: 127, Iteration: 100, Loss: 0.011224697085708613\n",
      "Epoch: 127, Iteration: 200, Loss: 0.011825769186543766\n",
      "Epoch: 127, Iteration: 300, Loss: 0.011403402379073668\n",
      "Epoch: 127, Train Loss: 0.0004606738825854917, Test Loss: 0.00012040328852215436\n",
      "Epoch: 128, Iteration: 100, Loss: 0.011937514049350284\n",
      "Epoch: 128, Iteration: 200, Loss: 0.011607600710703991\n",
      "Epoch: 128, Iteration: 300, Loss: 0.011449665718828328\n",
      "Epoch: 128, Train Loss: 0.00046032986019500387, Test Loss: 0.00012033276409018766\n",
      "Epoch: 129, Iteration: 100, Loss: 0.011780823104345473\n",
      "Epoch: 129, Iteration: 200, Loss: 0.01154405610577669\n",
      "Epoch: 129, Iteration: 300, Loss: 0.011510858130350243\n",
      "Epoch: 129, Train Loss: 0.00046058659397213257, Test Loss: 0.00012078799825514542\n",
      "Epoch: 130, Iteration: 100, Loss: 0.0114248415993643\n",
      "Epoch: 130, Iteration: 200, Loss: 0.011261656814895105\n",
      "Epoch: 130, Iteration: 300, Loss: 0.012028901939629577\n",
      "Epoch: 130, Train Loss: 0.00046028399803392473, Test Loss: 0.00012047197401209149\n",
      "Epoch: 131, Iteration: 100, Loss: 0.011724696611054242\n",
      "Epoch: 131, Iteration: 200, Loss: 0.011598356861213688\n",
      "Epoch: 131, Iteration: 300, Loss: 0.011334954171616118\n",
      "Epoch: 131, Train Loss: 0.0004605530747230472, Test Loss: 0.00012035541230849874\n",
      "Epoch: 132, Iteration: 100, Loss: 0.011477185849798843\n",
      "Epoch: 132, Iteration: 200, Loss: 0.01154643576592207\n",
      "Epoch: 132, Iteration: 300, Loss: 0.011629258246102836\n",
      "Epoch: 132, Train Loss: 0.00046027863542813345, Test Loss: 0.00012074669406895727\n",
      "Epoch: 133, Iteration: 100, Loss: 0.011502408611704595\n",
      "Epoch: 133, Iteration: 200, Loss: 0.011707427518558688\n",
      "Epoch: 133, Iteration: 300, Loss: 0.01146337972022593\n",
      "Epoch   134: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 133, Train Loss: 0.00046029301699594184, Test Loss: 0.00012052766122878049\n",
      "Epoch: 134, Iteration: 100, Loss: 0.011756438783777412\n",
      "Epoch: 134, Iteration: 200, Loss: 0.011403538650483824\n",
      "Epoch: 134, Iteration: 300, Loss: 0.011885809250088641\n",
      "Epoch: 134, Train Loss: 0.0004598095358758037, Test Loss: 0.00012020399059185797\n",
      "Epoch: 135, Iteration: 100, Loss: 0.01147608691098867\n",
      "Epoch: 135, Iteration: 200, Loss: 0.011502617191581521\n",
      "Epoch: 135, Iteration: 300, Loss: 0.011789357391535304\n",
      "Epoch: 135, Train Loss: 0.0004598007953504326, Test Loss: 0.00012050914095376244\n",
      "Epoch: 136, Iteration: 100, Loss: 0.011532443793839775\n",
      "Epoch: 136, Iteration: 200, Loss: 0.011594639941904461\n",
      "Epoch: 136, Iteration: 300, Loss: 0.01168341509765014\n",
      "Epoch: 136, Train Loss: 0.000459791406027171, Test Loss: 0.00012037266979231644\n",
      "Epoch: 137, Iteration: 100, Loss: 0.011505749869684223\n",
      "Epoch: 137, Iteration: 200, Loss: 0.01117412240273552\n",
      "Epoch: 137, Iteration: 300, Loss: 0.011794292800914263\n",
      "Epoch: 137, Train Loss: 0.00045978064934894703, Test Loss: 0.00012039330905872234\n",
      "Epoch: 138, Iteration: 100, Loss: 0.011371926797437482\n",
      "Epoch: 138, Iteration: 200, Loss: 0.011691699182847515\n",
      "Epoch: 138, Iteration: 300, Loss: 0.011386216421669815\n",
      "Epoch: 138, Train Loss: 0.0004597770464372552, Test Loss: 0.00012029668966730692\n",
      "Epoch: 139, Iteration: 100, Loss: 0.011513376310176682\n",
      "Epoch: 139, Iteration: 200, Loss: 0.011522282686200924\n",
      "Epoch: 139, Iteration: 300, Loss: 0.011409985207137652\n",
      "Epoch: 139, Train Loss: 0.0004600151164454802, Test Loss: 0.00012028413316470177\n",
      "Epoch: 140, Iteration: 100, Loss: 0.011897202173713595\n",
      "Epoch: 140, Iteration: 200, Loss: 0.011991410137852654\n",
      "Epoch: 140, Iteration: 300, Loss: 0.010716212236729916\n",
      "Epoch   141: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 140, Train Loss: 0.00045976892586847883, Test Loss: 0.0001202302036168457\n",
      "Epoch: 141, Iteration: 100, Loss: 0.011449757312220754\n",
      "Epoch: 141, Iteration: 200, Loss: 0.01172275302815251\n",
      "Epoch: 141, Iteration: 300, Loss: 0.011796788749052212\n",
      "Epoch: 141, Train Loss: 0.00045981589993840014, Test Loss: 0.00012019485641467702\n",
      "Epoch: 142, Iteration: 100, Loss: 0.01121781617985107\n",
      "Epoch: 142, Iteration: 200, Loss: 0.011940386277274229\n",
      "Epoch: 142, Iteration: 300, Loss: 0.011621677360381\n",
      "Epoch: 142, Train Loss: 0.00045959348572818944, Test Loss: 0.00012052112190561734\n",
      "Epoch: 143, Iteration: 100, Loss: 0.01136914897870156\n",
      "Epoch: 143, Iteration: 200, Loss: 0.011644124493614072\n",
      "Epoch: 143, Iteration: 300, Loss: 0.011260113918979187\n",
      "Epoch: 143, Train Loss: 0.0004596122345957934, Test Loss: 0.00012060384496808839\n",
      "Epoch: 144, Iteration: 100, Loss: 0.012167786990175955\n",
      "Epoch: 144, Iteration: 200, Loss: 0.011352786372299306\n",
      "Epoch: 144, Iteration: 300, Loss: 0.01114491386761074\n",
      "Epoch: 144, Train Loss: 0.0004597208836579758, Test Loss: 0.00012018640992800542\n",
      "Epoch: 145, Iteration: 100, Loss: 0.011848403773910832\n",
      "Epoch: 145, Iteration: 200, Loss: 0.011552981923159678\n",
      "Epoch: 145, Iteration: 300, Loss: 0.011288957146462053\n",
      "Epoch: 145, Train Loss: 0.00045978966448546063, Test Loss: 0.00012038446832016187\n",
      "Epoch: 146, Iteration: 100, Loss: 0.011191611898539122\n",
      "Epoch: 146, Iteration: 200, Loss: 0.011404604207200464\n",
      "Epoch: 146, Iteration: 300, Loss: 0.011670842708554119\n",
      "Epoch   147: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 146, Train Loss: 0.0004596699178626895, Test Loss: 0.00012056131729587298\n",
      "Epoch: 147, Iteration: 100, Loss: 0.011891730842762627\n",
      "Epoch: 147, Iteration: 200, Loss: 0.010968018272251356\n",
      "Epoch: 147, Iteration: 300, Loss: 0.011881995938892942\n",
      "Epoch: 147, Train Loss: 0.0004598712785644745, Test Loss: 0.00012037223706036307\n",
      "Epoch: 148, Iteration: 100, Loss: 0.01244091409171233\n",
      "Epoch: 148, Iteration: 200, Loss: 0.011402433636249043\n",
      "Epoch: 148, Iteration: 300, Loss: 0.010891055819229223\n",
      "Epoch: 148, Train Loss: 0.0004596782843888372, Test Loss: 0.00012023366734771942\n",
      "Epoch: 149, Iteration: 100, Loss: 0.011780486227507936\n",
      "Epoch: 149, Iteration: 200, Loss: 0.011355508941051085\n",
      "Epoch: 149, Iteration: 300, Loss: 0.011254524160904111\n",
      "Epoch: 149, Train Loss: 0.0004596648460701732, Test Loss: 0.00012021436633244541\n",
      "Epoch: 150, Iteration: 100, Loss: 0.011283901003480423\n",
      "Epoch: 150, Iteration: 200, Loss: 0.011320544414047617\n",
      "Epoch: 150, Iteration: 300, Loss: 0.011816256206657272\n",
      "Epoch: 150, Train Loss: 0.0004596770022450898, Test Loss: 0.00012037973684743622\n",
      "Epoch: 151, Iteration: 100, Loss: 0.011335744631651323\n",
      "Epoch: 151, Iteration: 200, Loss: 0.011881922015163582\n",
      "Epoch: 151, Iteration: 300, Loss: 0.011467940770671703\n",
      "Epoch: 151, Train Loss: 0.0004597337592150728, Test Loss: 0.00012021462489147526\n",
      "Epoch: 152, Iteration: 100, Loss: 0.011440312591730617\n",
      "Epoch: 152, Iteration: 200, Loss: 0.011362474582711002\n",
      "Epoch: 152, Iteration: 300, Loss: 0.011917565054318402\n",
      "Epoch: 152, Train Loss: 0.0004596952329069909, Test Loss: 0.00012050180142528148\n",
      "Epoch: 153, Iteration: 100, Loss: 0.011720303416950628\n",
      "Epoch: 153, Iteration: 200, Loss: 0.011694382992573082\n",
      "Epoch: 153, Iteration: 300, Loss: 0.011312253183859866\n",
      "Epoch: 153, Train Loss: 0.00045972964462353703, Test Loss: 0.00012018921912273334\n",
      "Epoch: 154, Iteration: 100, Loss: 0.011547878486453556\n",
      "Epoch: 154, Iteration: 200, Loss: 0.011227714749111328\n",
      "Epoch: 154, Iteration: 300, Loss: 0.01142510252611828\n",
      "Epoch: 154, Train Loss: 0.0004597319028332479, Test Loss: 0.00012037939367727041\n",
      "Epoch: 155, Iteration: 100, Loss: 0.011433940540882759\n",
      "Epoch: 155, Iteration: 200, Loss: 0.011257304351602215\n",
      "Epoch: 155, Iteration: 300, Loss: 0.011747509339329554\n",
      "Epoch: 155, Train Loss: 0.00045970554173877225, Test Loss: 0.00012022396842120983\n",
      "Epoch: 156, Iteration: 100, Loss: 0.01161927470820956\n",
      "Epoch: 156, Iteration: 200, Loss: 0.011224196896364447\n",
      "Epoch: 156, Iteration: 300, Loss: 0.011456735352112446\n",
      "Epoch: 156, Train Loss: 0.00045966395817831465, Test Loss: 0.00012035300520419133\n",
      "Epoch: 157, Iteration: 100, Loss: 0.011449411616922589\n",
      "Epoch: 157, Iteration: 200, Loss: 0.011183358736161608\n",
      "Epoch: 157, Iteration: 300, Loss: 0.011763499562221114\n",
      "Epoch: 157, Train Loss: 0.0004596780945013455, Test Loss: 0.00012016209157738239\n",
      "Epoch: 158, Iteration: 100, Loss: 0.011789254858740605\n",
      "Epoch: 158, Iteration: 200, Loss: 0.011212580102437641\n",
      "Epoch: 158, Iteration: 300, Loss: 0.011589141126023605\n",
      "Epoch: 158, Train Loss: 0.0004597348571345738, Test Loss: 0.00012026527118221041\n",
      "Epoch: 159, Iteration: 100, Loss: 0.01147902197408257\n",
      "Epoch: 159, Iteration: 200, Loss: 0.01171102228545351\n",
      "Epoch: 159, Iteration: 300, Loss: 0.011401062722143251\n",
      "Epoch: 159, Train Loss: 0.0004596255192942389, Test Loss: 0.00012016914918125828\n",
      "Epoch: 160, Iteration: 100, Loss: 0.011440809241321404\n",
      "Epoch: 160, Iteration: 200, Loss: 0.011680306983180344\n",
      "Epoch: 160, Iteration: 300, Loss: 0.011361742206645431\n",
      "Epoch: 160, Train Loss: 0.0004599082954113909, Test Loss: 0.00012044280447298661\n",
      "Epoch: 161, Iteration: 100, Loss: 0.011920924534933874\n",
      "Epoch: 161, Iteration: 200, Loss: 0.011388649225409608\n",
      "Epoch: 161, Iteration: 300, Loss: 0.01150684398089652\n",
      "Epoch: 161, Train Loss: 0.00045987578830801755, Test Loss: 0.000120238069302076\n",
      "Epoch: 162, Iteration: 100, Loss: 0.011495302169350907\n",
      "Epoch: 162, Iteration: 200, Loss: 0.011688795078953262\n",
      "Epoch: 162, Iteration: 300, Loss: 0.011772194346121978\n",
      "Epoch: 162, Train Loss: 0.0004596373661284421, Test Loss: 0.0001202861123001827\n",
      "Epoch: 163, Iteration: 100, Loss: 0.011531997406564187\n",
      "Epoch: 163, Iteration: 200, Loss: 0.011313405739201698\n",
      "Epoch: 163, Iteration: 300, Loss: 0.01140708500315668\n",
      "Epoch: 163, Train Loss: 0.0004599801668706446, Test Loss: 0.00012030959151536419\n",
      "Epoch: 164, Iteration: 100, Loss: 0.01181240810547024\n",
      "Epoch: 164, Iteration: 200, Loss: 0.011109529456007294\n",
      "Epoch: 164, Iteration: 300, Loss: 0.011685569254041184\n",
      "Epoch: 164, Train Loss: 0.0004596694641279719, Test Loss: 0.00012044285442956156\n",
      "Epoch: 165, Iteration: 100, Loss: 0.01149332649220014\n",
      "Epoch: 165, Iteration: 200, Loss: 0.011365282945916988\n",
      "Epoch: 165, Iteration: 300, Loss: 0.011881158803589642\n",
      "Epoch: 165, Train Loss: 0.000459719461920856, Test Loss: 0.00012014267909743813\n",
      "Epoch: 166, Iteration: 100, Loss: 0.010969511524308473\n",
      "Epoch: 166, Iteration: 200, Loss: 0.012145207401772495\n",
      "Epoch: 166, Iteration: 300, Loss: 0.011538247592397965\n",
      "Epoch: 166, Train Loss: 0.00045981135460267266, Test Loss: 0.00012025773786572433\n",
      "Epoch: 167, Iteration: 100, Loss: 0.011746719646907877\n",
      "Epoch: 167, Iteration: 200, Loss: 0.011575431759411003\n",
      "Epoch: 167, Iteration: 300, Loss: 0.011248342962062452\n",
      "Epoch: 167, Train Loss: 0.0004596871990871318, Test Loss: 0.00012034068258236806\n",
      "Epoch: 168, Iteration: 100, Loss: 0.011534332461451413\n",
      "Epoch: 168, Iteration: 200, Loss: 0.011578711397305597\n",
      "Epoch: 168, Iteration: 300, Loss: 0.011664227189612575\n",
      "Epoch: 168, Train Loss: 0.00045964819129067953, Test Loss: 0.00012017848746030179\n",
      "Epoch: 169, Iteration: 100, Loss: 0.011980436036537867\n",
      "Epoch: 169, Iteration: 200, Loss: 0.011512010358273983\n",
      "Epoch: 169, Iteration: 300, Loss: 0.011158475768752396\n",
      "Epoch: 169, Train Loss: 0.0004599484806753138, Test Loss: 0.00012022103073457064\n",
      "Epoch: 170, Iteration: 100, Loss: 0.011766598596295808\n",
      "Epoch: 170, Iteration: 200, Loss: 0.011260229774052277\n",
      "Epoch: 170, Iteration: 300, Loss: 0.011514526930113789\n",
      "Epoch: 170, Train Loss: 0.0004596943991347552, Test Loss: 0.00012011780736148213\n",
      "Epoch: 171, Iteration: 100, Loss: 0.011308676359476522\n",
      "Epoch: 171, Iteration: 200, Loss: 0.011784962516685482\n",
      "Epoch: 171, Iteration: 300, Loss: 0.011630459819571115\n"
     ]
    }
   ],
   "source": [
    "train_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "test_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "\n",
    "patience = config[\"patience\"]\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"])):\n",
    "    loss_ae1_train = 0\n",
    "    loss_ae2_train = 0\n",
    "    loss_dyn_train = 0\n",
    "\n",
    "    current_train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    warmup = 1 if epoch <= config[\"warmup\"] else 0\n",
    "\n",
    "    encoder.train()\n",
    "    dynamics.train()\n",
    "    decoder.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i, (x_t, x_tau) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z_t = encoder(x_t)\n",
    "        x_t_pred = decoder(z_t)\n",
    "\n",
    "        z_tau_pred = dynamics(z_t)\n",
    "        z_tau = encoder(x_tau)\n",
    "        x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_ae1 = criterion(x_t, x_t_pred)\n",
    "        loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "        loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "        loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "        # Backward pass\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += loss_total.item()\n",
    "        epoch_train_loss += loss_total.item()\n",
    "\n",
    "        loss_ae1_train += loss_ae1.item()\n",
    "        loss_ae2_train += loss_ae2.item()\n",
    "        loss_dyn_train += loss_dyn.item() * warmup\n",
    "        counter += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Epoch: {}, Iteration: {}, Loss: {}\".format(epoch, i+1, current_train_loss))\n",
    "            current_train_loss = 0\n",
    "        \n",
    "    train_losses['loss_ae1'].append(loss_ae1_train / counter)\n",
    "    train_losses['loss_ae2'].append(loss_ae2_train / counter)\n",
    "    train_losses['loss_dyn'].append(loss_dyn_train / counter)\n",
    "    train_losses['loss_total'].append(epoch_train_loss / counter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_ae1_test = 0\n",
    "        loss_ae2_test = 0\n",
    "        loss_dyn_test = 0\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        encoder.eval()\n",
    "        dynamics.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        counter = 0\n",
    "        for i, (x_t, x_tau) in enumerate(test_loader,0):\n",
    "            # Forward pass\n",
    "            z_t = encoder(x_t)\n",
    "            x_t_pred = decoder(z_t)\n",
    "\n",
    "            z_tau_pred = dynamics(z_t)\n",
    "            z_tau = encoder(x_tau)\n",
    "            x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_ae1 = criterion(x_t, x_t_pred)\n",
    "            loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "            loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "            loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "            epoch_test_loss += loss_total.item()\n",
    "\n",
    "            loss_ae1_test += loss_ae1.item()\n",
    "            loss_ae2_test += loss_ae2.item()\n",
    "            loss_dyn_test += loss_dyn.item() * warmup\n",
    "            counter += 1\n",
    "\n",
    "        test_losses['loss_ae1'].append(loss_ae1_test / counter)\n",
    "        test_losses['loss_ae2'].append(loss_ae2_test / counter)\n",
    "        test_losses['loss_dyn'].append(loss_dyn_test / counter)\n",
    "        test_losses['loss_total'].append(epoch_test_loss / counter)\n",
    "\n",
    "        if epoch >= patience and np.mean(test_losses['loss_total'][-patience:]) >= np.mean(test_losses['loss_total'][-patience:-1]):\n",
    "            break\n",
    "\n",
    "        if epoch >= config[\"warmup\"]:\n",
    "            scheduler.step(epoch_test_loss / counter)\n",
    "        \n",
    "    print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch, epoch_train_loss / counter, epoch_test_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(encoder, os.path.join(config[\"model_dir\"], \"encoder.pt\"))\n",
    "torch.save(dynamics, os.path.join(config[\"model_dir\"], \"dynamics.pt\"))\n",
    "torch.save(decoder, os.path.join(config[\"model_dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_dir doesn't exist, create it\n",
    "if not os.path.exists(config[\"log_dir\"]):\n",
    "    os.makedirs(config[\"log_dir\"])\n",
    "\n",
    "# Save the losses as a pickle file\n",
    "with open(os.path.join(config[\"log_dir\"], \"losses.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": train_losses, \"test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
