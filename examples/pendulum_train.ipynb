{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from AEMG.data_utils import DynamicsDataset\n",
    "from AEMG.systems.utils import get_system\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from AEMG.models import *\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:18<00:00, 54.29it/s]\n"
     ]
    }
   ],
   "source": [
    "system = get_system(\"pendulum\")\n",
    "# config_fname = \"config/pendulum_lqr_1K.txt\"\n",
    "config_fname = \"config/physics_pendulum.txt\"\n",
    "\n",
    "with open(config_fname, 'r') as f:\n",
    "    config = eval(f.read())\n",
    "\n",
    "dataset = DynamicsDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = Encoder(config[\"high_dims\"],config[\"low_dims\"])\n",
    "dynamics = LatentDynamics(config[\"low_dims\"])\n",
    "decoder  = Decoder(config[\"low_dims\"],config[\"high_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(set(list(encoder.parameters()) + list(dynamics.parameters()) + list(decoder.parameters())), \n",
    "    lr=config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.001, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9dd431f07a4e9bbd229b6ab773a632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 9.459350265562534\n",
      "Epoch: 0, Iteration: 200, Loss: 4.056779481470585\n",
      "Epoch: 0, Iteration: 300, Loss: 3.823234047740698\n",
      "Epoch: 0, Iteration: 400, Loss: 3.574104430153966\n",
      "Epoch: 0, Iteration: 500, Loss: 2.288802775554359\n",
      "Epoch: 0, Iteration: 600, Loss: 1.3192967604845762\n",
      "Epoch: 0, Iteration: 700, Loss: 0.9652189007028937\n",
      "Epoch: 0, Iteration: 800, Loss: 0.9100759937427938\n",
      "Epoch: 0, Train Loss: 0.12419732134093955, Test Loss: 0.008248115559258768\n",
      "Epoch: 1, Iteration: 100, Loss: 0.7449058112688363\n",
      "Epoch: 1, Iteration: 200, Loss: 0.7068242002278566\n",
      "Epoch: 1, Iteration: 300, Loss: 0.6522354981862009\n",
      "Epoch: 1, Iteration: 400, Loss: 0.6369671998545527\n",
      "Epoch: 1, Iteration: 500, Loss: 0.6081682019867003\n",
      "Epoch: 1, Iteration: 600, Loss: 0.5928704009857029\n",
      "Epoch: 1, Iteration: 700, Loss: 0.57197310263291\n",
      "Epoch: 1, Iteration: 800, Loss: 0.5463225045241416\n",
      "Epoch: 1, Train Loss: 0.024919536333822986, Test Loss: 0.004736754204219525\n",
      "Epoch: 2, Iteration: 100, Loss: 0.40663552354089916\n",
      "Epoch: 2, Iteration: 200, Loss: 0.31131629017181695\n",
      "Epoch: 2, Iteration: 300, Loss: 0.28930335654877126\n",
      "Epoch: 2, Iteration: 400, Loss: 0.26644627086352557\n",
      "Epoch: 2, Iteration: 500, Loss: 0.24304780643433332\n",
      "Epoch: 2, Iteration: 600, Loss: 0.22052734531462193\n",
      "Epoch: 2, Iteration: 700, Loss: 0.18045556591823697\n",
      "Epoch: 2, Iteration: 800, Loss: 0.14913137326948345\n",
      "Epoch: 2, Train Loss: 0.009963060833818455, Test Loss: 0.00133189282187986\n",
      "Epoch: 3, Iteration: 100, Loss: 0.1304544709273614\n",
      "Epoch: 3, Iteration: 200, Loss: 0.12451922567561269\n",
      "Epoch: 3, Iteration: 300, Loss: 0.11979889875510707\n",
      "Epoch: 3, Iteration: 400, Loss: 0.11782964708982036\n",
      "Epoch: 3, Iteration: 500, Loss: 0.11224665370536968\n",
      "Epoch: 3, Iteration: 600, Loss: 0.10995761712547392\n",
      "Epoch: 3, Iteration: 700, Loss: 0.1083426564000547\n",
      "Epoch: 3, Iteration: 800, Loss: 0.10306236625183374\n",
      "Epoch: 3, Train Loss: 0.00457659417501552, Test Loss: 0.0009721014675499725\n",
      "Epoch: 4, Iteration: 100, Loss: 0.0971912372042425\n",
      "Epoch: 4, Iteration: 200, Loss: 0.0949182808981277\n",
      "Epoch: 4, Iteration: 300, Loss: 0.09218042378779501\n",
      "Epoch: 4, Iteration: 400, Loss: 0.09022792766336352\n",
      "Epoch: 4, Iteration: 500, Loss: 0.08649357268586755\n",
      "Epoch: 4, Iteration: 600, Loss: 0.08798853505868465\n",
      "Epoch: 4, Iteration: 700, Loss: 0.08597601554356515\n",
      "Epoch: 4, Iteration: 800, Loss: 0.08448189659975469\n",
      "Epoch: 4, Train Loss: 0.0035731282145802087, Test Loss: 0.0008008006165095944\n",
      "Epoch: 5, Iteration: 100, Loss: 0.08154237602138892\n",
      "Epoch: 5, Iteration: 200, Loss: 0.07910082925809547\n",
      "Epoch: 5, Iteration: 300, Loss: 0.07844722736626863\n",
      "Epoch: 5, Iteration: 400, Loss: 0.07588933827355504\n",
      "Epoch: 5, Iteration: 500, Loss: 0.07332982734078541\n",
      "Epoch: 5, Iteration: 600, Loss: 0.07290551258483902\n",
      "Epoch: 5, Iteration: 700, Loss: 0.07300270086852834\n",
      "Epoch: 5, Iteration: 800, Loss: 0.07202152966056019\n",
      "Epoch: 5, Train Loss: 0.003017819769817838, Test Loss: 0.0007190469554239189\n",
      "Epoch: 6, Iteration: 100, Loss: 0.07051816373132169\n",
      "Epoch: 6, Iteration: 200, Loss: 0.06830421925405972\n",
      "Epoch: 6, Iteration: 300, Loss: 0.06918892514659092\n",
      "Epoch: 6, Iteration: 400, Loss: 0.06782741792267188\n",
      "Epoch: 6, Iteration: 500, Loss: 0.06737777456874028\n",
      "Epoch: 6, Iteration: 600, Loss: 0.06683977990178391\n",
      "Epoch: 6, Iteration: 700, Loss: 0.06529935181606561\n",
      "Epoch: 6, Iteration: 800, Loss: 0.06481654039816931\n",
      "Epoch: 6, Train Loss: 0.0026893938244062945, Test Loss: 0.0006247151440249816\n",
      "Epoch: 7, Iteration: 100, Loss: 0.06379856669809669\n",
      "Epoch: 7, Iteration: 200, Loss: 0.0624948708282318\n",
      "Epoch: 7, Iteration: 300, Loss: 0.062502242741175\n",
      "Epoch: 7, Iteration: 400, Loss: 0.062098226306261495\n",
      "Epoch: 7, Iteration: 500, Loss: 0.061493013316066936\n",
      "Epoch: 7, Iteration: 600, Loss: 0.060761371918488294\n",
      "Epoch: 7, Iteration: 700, Loss: 0.061017443891614676\n",
      "Epoch: 7, Iteration: 800, Loss: 0.060898165771504864\n",
      "Epoch: 7, Train Loss: 0.002469741389013758, Test Loss: 0.0006014553442155333\n",
      "Epoch: 8, Iteration: 100, Loss: 0.058137073123361915\n",
      "Epoch: 8, Iteration: 200, Loss: 0.059593337384285405\n",
      "Epoch: 8, Iteration: 300, Loss: 0.05901736638043076\n",
      "Epoch: 8, Iteration: 400, Loss: 0.05721760482992977\n",
      "Epoch: 8, Iteration: 500, Loss: 0.05855217532371171\n",
      "Epoch: 8, Iteration: 600, Loss: 0.05780418301583268\n",
      "Epoch: 8, Iteration: 700, Loss: 0.055601391970412806\n",
      "Epoch: 8, Iteration: 800, Loss: 0.055695489136269316\n",
      "Epoch: 8, Train Loss: 0.00230581867175975, Test Loss: 0.0005608714092576507\n",
      "Epoch: 9, Iteration: 100, Loss: 0.054941428679740056\n",
      "Epoch: 9, Iteration: 200, Loss: 0.05470208384213038\n",
      "Epoch: 9, Iteration: 300, Loss: 0.05587235020357184\n",
      "Epoch: 9, Iteration: 400, Loss: 0.05508087945054285\n",
      "Epoch: 9, Iteration: 500, Loss: 0.05345797032350674\n",
      "Epoch: 9, Iteration: 600, Loss: 0.05449745734222233\n",
      "Epoch: 9, Iteration: 700, Loss: 0.053435558365890756\n",
      "Epoch: 9, Iteration: 800, Loss: 0.053449712722795084\n",
      "Epoch: 9, Train Loss: 0.002172846271584566, Test Loss: 0.0005223281630035919\n",
      "Epoch: 10, Iteration: 100, Loss: 0.052930639853002504\n",
      "Epoch: 10, Iteration: 200, Loss: 0.05212954303715378\n",
      "Epoch: 10, Iteration: 300, Loss: 0.05286672612419352\n",
      "Epoch: 10, Iteration: 400, Loss: 0.051551233016652986\n",
      "Epoch: 10, Iteration: 500, Loss: 0.05093431941349991\n",
      "Epoch: 10, Iteration: 600, Loss: 0.0504383078077808\n",
      "Epoch: 10, Iteration: 700, Loss: 0.05078961010440253\n",
      "Epoch: 10, Iteration: 800, Loss: 0.04993947033653967\n",
      "Epoch: 10, Train Loss: 0.002049444223059908, Test Loss: 0.00048435209085628356\n",
      "Epoch: 11, Iteration: 100, Loss: 0.050832457083743066\n",
      "Epoch: 11, Iteration: 200, Loss: 0.05003354643122293\n",
      "Epoch: 11, Iteration: 300, Loss: 0.05012944818008691\n",
      "Epoch: 11, Iteration: 400, Loss: 0.04986091100727208\n",
      "Epoch: 11, Iteration: 500, Loss: 0.04830476894858293\n",
      "Epoch: 11, Iteration: 600, Loss: 0.046958605002146214\n",
      "Epoch: 11, Iteration: 700, Loss: 0.04955159479868598\n",
      "Epoch: 11, Iteration: 800, Loss: 0.04747659055283293\n",
      "Epoch: 11, Train Loss: 0.00195524209497651, Test Loss: 0.00045797072121903543\n",
      "Epoch: 12, Iteration: 100, Loss: 0.048100729880388826\n",
      "Epoch: 12, Iteration: 200, Loss: 0.046941438369685784\n",
      "Epoch: 12, Iteration: 300, Loss: 0.04740062661585398\n",
      "Epoch: 12, Iteration: 400, Loss: 0.04656947511830367\n",
      "Epoch: 12, Iteration: 500, Loss: 0.045894979033619165\n",
      "Epoch: 12, Iteration: 600, Loss: 0.04543755928170867\n",
      "Epoch: 12, Iteration: 700, Loss: 0.0464309950475581\n",
      "Epoch: 12, Iteration: 800, Loss: 0.04523801716277376\n",
      "Epoch: 12, Train Loss: 0.0018574267762341893, Test Loss: 0.0004551780088493506\n",
      "Epoch: 13, Iteration: 100, Loss: 0.0450136330910027\n",
      "Epoch: 13, Iteration: 200, Loss: 0.04530857066856697\n",
      "Epoch: 13, Iteration: 300, Loss: 0.044958269922062755\n",
      "Epoch: 13, Iteration: 400, Loss: 0.045044420898193493\n",
      "Epoch: 13, Iteration: 500, Loss: 0.04511270488728769\n",
      "Epoch: 13, Iteration: 600, Loss: 0.04444156392128207\n",
      "Epoch: 13, Iteration: 700, Loss: 0.04544601318775676\n",
      "Epoch: 13, Iteration: 800, Loss: 0.04446372532402165\n",
      "Epoch: 13, Train Loss: 0.0017959306067261364, Test Loss: 0.0004318017599200787\n",
      "Epoch: 14, Iteration: 100, Loss: 0.043481580389197916\n",
      "Epoch: 14, Iteration: 200, Loss: 0.04403368043131195\n",
      "Epoch: 14, Iteration: 300, Loss: 0.04456553471391089\n",
      "Epoch: 14, Iteration: 400, Loss: 0.04586058671702631\n",
      "Epoch: 14, Iteration: 500, Loss: 0.0441246063564904\n",
      "Epoch: 14, Iteration: 600, Loss: 0.042933837830787525\n",
      "Epoch: 14, Iteration: 700, Loss: 0.042510808940278366\n",
      "Epoch: 14, Iteration: 800, Loss: 0.044556997483596206\n",
      "Epoch: 14, Train Loss: 0.0017596419304094283, Test Loss: 0.0004555792512891445\n",
      "Epoch: 15, Iteration: 100, Loss: 0.04312717032735236\n",
      "Epoch: 15, Iteration: 200, Loss: 0.04231708717998117\n",
      "Epoch: 15, Iteration: 300, Loss: 0.0440033363411203\n",
      "Epoch: 15, Iteration: 400, Loss: 0.04380036212387495\n",
      "Epoch: 15, Iteration: 500, Loss: 0.042129233392188326\n",
      "Epoch: 15, Iteration: 600, Loss: 0.04410470277070999\n",
      "Epoch: 15, Iteration: 700, Loss: 0.04338951234240085\n",
      "Epoch: 15, Iteration: 800, Loss: 0.04156579967821017\n",
      "Epoch: 15, Train Loss: 0.0017156443586261848, Test Loss: 0.0004115593084509695\n",
      "Epoch: 16, Iteration: 100, Loss: 0.04163449280895293\n",
      "Epoch: 16, Iteration: 200, Loss: 0.04224415650242008\n",
      "Epoch: 16, Iteration: 300, Loss: 0.04295302217360586\n",
      "Epoch: 16, Iteration: 400, Loss: 0.042839945293962955\n",
      "Epoch: 16, Iteration: 500, Loss: 0.04115506960079074\n",
      "Epoch: 16, Iteration: 600, Loss: 0.04131943947868422\n",
      "Epoch: 16, Iteration: 700, Loss: 0.04214142874116078\n",
      "Epoch: 16, Iteration: 800, Loss: 0.04221540840808302\n",
      "Epoch: 16, Train Loss: 0.0016809512440207487, Test Loss: 0.00040268368722896696\n",
      "Epoch: 17, Iteration: 100, Loss: 0.04115994949825108\n",
      "Epoch: 17, Iteration: 200, Loss: 0.04197259672218934\n",
      "Epoch: 17, Iteration: 300, Loss: 0.041185512352967635\n",
      "Epoch: 17, Iteration: 400, Loss: 0.042274367122445256\n",
      "Epoch: 17, Iteration: 500, Loss: 0.04169078732957132\n",
      "Epoch: 17, Iteration: 600, Loss: 0.04152013879502192\n",
      "Epoch: 17, Iteration: 700, Loss: 0.04034920845879242\n",
      "Epoch: 17, Iteration: 800, Loss: 0.04051750554936007\n",
      "Epoch: 17, Train Loss: 0.0016498288431007432, Test Loss: 0.0003798021849090328\n",
      "Epoch: 18, Iteration: 100, Loss: 0.04020695487270132\n",
      "Epoch: 18, Iteration: 200, Loss: 0.039990220771869645\n",
      "Epoch: 18, Iteration: 300, Loss: 0.03872125857742503\n",
      "Epoch: 18, Iteration: 400, Loss: 0.03975299029843882\n",
      "Epoch: 18, Iteration: 500, Loss: 0.038368978624930605\n",
      "Epoch: 18, Iteration: 600, Loss: 0.037409851676784456\n",
      "Epoch: 18, Iteration: 700, Loss: 0.03683564733364619\n",
      "Epoch: 18, Iteration: 800, Loss: 0.03831033385358751\n",
      "Epoch: 18, Train Loss: 0.0015498774127781494, Test Loss: 0.0003676192402037092\n",
      "Epoch: 19, Iteration: 100, Loss: 0.03791565398569219\n",
      "Epoch: 19, Iteration: 200, Loss: 0.03580014448380098\n",
      "Epoch: 19, Iteration: 300, Loss: 0.03653311255038716\n",
      "Epoch: 19, Iteration: 400, Loss: 0.03751979931257665\n",
      "Epoch: 19, Iteration: 500, Loss: 0.03629516012733802\n",
      "Epoch: 19, Iteration: 600, Loss: 0.0368329313932918\n",
      "Epoch: 19, Iteration: 700, Loss: 0.03587450526538305\n",
      "Epoch: 19, Iteration: 800, Loss: 0.03630640564369969\n",
      "Epoch: 19, Train Loss: 0.0014536968586737857, Test Loss: 0.00037157874833899744\n",
      "Epoch: 20, Iteration: 100, Loss: 0.03645593882538378\n",
      "Epoch: 20, Iteration: 200, Loss: 0.03515278286067769\n",
      "Epoch: 20, Iteration: 300, Loss: 0.03502626871340908\n",
      "Epoch: 20, Iteration: 400, Loss: 0.035053593150223605\n",
      "Epoch: 20, Iteration: 500, Loss: 0.034829937358153984\n",
      "Epoch: 20, Iteration: 600, Loss: 0.033450835704570636\n",
      "Epoch: 20, Iteration: 700, Loss: 0.033681774424621835\n",
      "Epoch: 20, Iteration: 800, Loss: 0.034877671918366104\n",
      "Epoch: 20, Train Loss: 0.0014005663108229765, Test Loss: 0.00036552883008675227\n",
      "Epoch: 21, Iteration: 100, Loss: 0.03498885428416543\n",
      "Epoch: 21, Iteration: 200, Loss: 0.03252234279352706\n",
      "Epoch: 21, Iteration: 300, Loss: 0.03586025070399046\n",
      "Epoch: 21, Iteration: 400, Loss: 0.03443640560726635\n",
      "Epoch: 21, Iteration: 500, Loss: 0.03416693169856444\n",
      "Epoch: 21, Iteration: 600, Loss: 0.032364072802010924\n",
      "Epoch: 21, Iteration: 700, Loss: 0.033186404180014506\n",
      "Epoch: 21, Iteration: 800, Loss: 0.031302913441322744\n",
      "Epoch: 21, Train Loss: 0.0013362217244819827, Test Loss: 0.0003899930292860635\n",
      "Epoch: 22, Iteration: 100, Loss: 0.03241867801989429\n",
      "Epoch: 22, Iteration: 200, Loss: 0.031371878925710917\n",
      "Epoch: 22, Iteration: 300, Loss: 0.03275186703831423\n",
      "Epoch: 22, Iteration: 400, Loss: 0.03235423963633366\n",
      "Epoch: 22, Iteration: 500, Loss: 0.03199794229294639\n",
      "Epoch: 22, Iteration: 600, Loss: 0.030938055642764084\n",
      "Epoch: 22, Iteration: 700, Loss: 0.031233341753249988\n",
      "Epoch: 22, Iteration: 800, Loss: 0.031871217986918055\n",
      "Epoch: 22, Train Loss: 0.001275867813326899, Test Loss: 0.0003136466416875802\n",
      "Epoch: 23, Iteration: 100, Loss: 0.03222661315521691\n",
      "Epoch: 23, Iteration: 200, Loss: 0.0336483398132259\n",
      "Epoch: 23, Iteration: 300, Loss: 0.030818825311143883\n",
      "Epoch: 23, Iteration: 400, Loss: 0.03099153433868196\n",
      "Epoch: 23, Iteration: 500, Loss: 0.02986279208562337\n",
      "Epoch: 23, Iteration: 600, Loss: 0.03198883688310161\n",
      "Epoch: 23, Iteration: 700, Loss: 0.033329040961689316\n",
      "Epoch: 23, Iteration: 800, Loss: 0.031497097996179946\n",
      "Epoch: 23, Train Loss: 0.0012708495899875034, Test Loss: 0.0003074121732969687\n",
      "Epoch: 24, Iteration: 100, Loss: 0.030422241834457964\n",
      "Epoch: 24, Iteration: 200, Loss: 0.030074664871790446\n",
      "Epoch: 24, Iteration: 300, Loss: 0.02978949117823504\n",
      "Epoch: 24, Iteration: 400, Loss: 0.028974114131415263\n",
      "Epoch: 24, Iteration: 500, Loss: 0.03016307536745444\n",
      "Epoch: 24, Iteration: 600, Loss: 0.029979703336721286\n",
      "Epoch: 24, Iteration: 700, Loss: 0.029704072774620727\n",
      "Epoch: 24, Iteration: 800, Loss: 0.031426108223968185\n",
      "Epoch: 24, Train Loss: 0.0012093544316699103, Test Loss: 0.00029371517504112634\n",
      "Epoch: 25, Iteration: 100, Loss: 0.0315567097713938\n",
      "Epoch: 25, Iteration: 200, Loss: 0.030365859434823506\n",
      "Epoch: 25, Iteration: 300, Loss: 0.03000027540838346\n",
      "Epoch: 25, Iteration: 400, Loss: 0.02970597191597335\n",
      "Epoch: 25, Iteration: 500, Loss: 0.02974942949367687\n",
      "Epoch: 25, Iteration: 600, Loss: 0.031623965318431146\n",
      "Epoch: 25, Iteration: 700, Loss: 0.02976689243223518\n",
      "Epoch: 25, Iteration: 800, Loss: 0.030146759643685073\n",
      "Epoch: 25, Train Loss: 0.001230704859191496, Test Loss: 0.0003673231115453117\n",
      "Epoch: 26, Iteration: 100, Loss: 0.03200062966789119\n",
      "Epoch: 26, Iteration: 200, Loss: 0.029213186004199088\n",
      "Epoch: 26, Iteration: 300, Loss: 0.028794804005883634\n",
      "Epoch: 26, Iteration: 400, Loss: 0.03237841266673058\n",
      "Epoch: 26, Iteration: 500, Loss: 0.03130605317710433\n",
      "Epoch: 26, Iteration: 600, Loss: 0.030379113057279028\n",
      "Epoch: 26, Iteration: 700, Loss: 0.029767598913167603\n",
      "Epoch: 26, Iteration: 800, Loss: 0.02936216237139888\n",
      "Epoch: 26, Train Loss: 0.0012115179673738538, Test Loss: 0.0002881867378818146\n",
      "Epoch: 27, Iteration: 100, Loss: 0.029848885285900906\n",
      "Epoch: 27, Iteration: 200, Loss: 0.03143975131388288\n",
      "Epoch: 27, Iteration: 300, Loss: 0.03158252532011829\n",
      "Epoch: 27, Iteration: 400, Loss: 0.03409855630889069\n",
      "Epoch: 27, Iteration: 500, Loss: 0.03285201167454943\n",
      "Epoch: 27, Iteration: 600, Loss: 0.03088440738792997\n",
      "Epoch: 27, Iteration: 700, Loss: 0.03206480690278113\n",
      "Epoch: 27, Iteration: 800, Loss: 0.03040299804706592\n",
      "Epoch: 27, Train Loss: 0.0012757014241799162, Test Loss: 0.00032864361718427904\n",
      "Epoch: 28, Iteration: 100, Loss: 0.029049554155790247\n",
      "Epoch: 28, Iteration: 200, Loss: 0.030122634969302453\n",
      "Epoch: 28, Iteration: 300, Loss: 0.032654883223585784\n",
      "Epoch: 28, Iteration: 400, Loss: 0.029987931760842912\n",
      "Epoch: 28, Iteration: 500, Loss: 0.027823006254038773\n",
      "Epoch: 28, Iteration: 600, Loss: 0.03232016318361275\n",
      "Epoch: 28, Iteration: 700, Loss: 0.029136795550584793\n",
      "Epoch: 28, Iteration: 800, Loss: 0.02748550048272591\n",
      "Epoch: 28, Train Loss: 0.0011877294587967103, Test Loss: 0.00028316817019179825\n",
      "Epoch: 29, Iteration: 100, Loss: 0.030031853239051998\n",
      "Epoch: 29, Iteration: 200, Loss: 0.026919188050669618\n",
      "Epoch: 29, Iteration: 300, Loss: 0.02748957122093998\n",
      "Epoch: 29, Iteration: 400, Loss: 0.029746478408924304\n",
      "Epoch: 29, Iteration: 500, Loss: 0.027587879812926985\n",
      "Epoch: 29, Iteration: 600, Loss: 0.0321868344763061\n",
      "Epoch: 29, Iteration: 700, Loss: 0.03313049842836335\n",
      "Epoch: 29, Iteration: 800, Loss: 0.030195674567949027\n",
      "Epoch: 29, Train Loss: 0.001182847407787034, Test Loss: 0.0002820636917625593\n",
      "Epoch: 30, Iteration: 100, Loss: 0.027633857025648467\n",
      "Epoch: 30, Iteration: 200, Loss: 0.030716941560967825\n",
      "Epoch: 30, Iteration: 300, Loss: 0.028731039274134673\n",
      "Epoch: 30, Iteration: 400, Loss: 0.027926815499085933\n",
      "Epoch: 30, Iteration: 500, Loss: 0.028789780335500836\n",
      "Epoch: 30, Iteration: 600, Loss: 0.03014525269099977\n",
      "Epoch: 30, Iteration: 700, Loss: 0.028897859345306642\n",
      "Epoch: 30, Iteration: 800, Loss: 0.030364660880877636\n",
      "Epoch: 30, Train Loss: 0.0011701038380074079, Test Loss: 0.0003227814797297\n",
      "Epoch: 31, Iteration: 100, Loss: 0.029180474986787885\n",
      "Epoch: 31, Iteration: 200, Loss: 0.02974777929193806\n",
      "Epoch: 31, Iteration: 300, Loss: 0.02927193106734194\n",
      "Epoch: 31, Iteration: 400, Loss: 0.027660115956678055\n",
      "Epoch: 31, Iteration: 500, Loss: 0.029026106669334695\n",
      "Epoch: 31, Iteration: 600, Loss: 0.030046363914152607\n",
      "Epoch: 31, Iteration: 700, Loss: 0.02948303410084918\n",
      "Epoch: 31, Iteration: 800, Loss: 0.030293282034108415\n",
      "Epoch: 31, Train Loss: 0.0011712014329783868, Test Loss: 0.00029116362217418884\n",
      "Epoch: 32, Iteration: 100, Loss: 0.02759747269738\n",
      "Epoch: 32, Iteration: 200, Loss: 0.02841677240212448\n",
      "Epoch: 32, Iteration: 300, Loss: 0.026963175914715976\n",
      "Epoch: 32, Iteration: 400, Loss: 0.028162322400021367\n",
      "Epoch: 32, Iteration: 500, Loss: 0.02682429643755313\n",
      "Epoch: 32, Iteration: 600, Loss: 0.027252188825514168\n",
      "Epoch: 32, Iteration: 700, Loss: 0.027236255526077002\n",
      "Epoch: 32, Iteration: 800, Loss: 0.030824996370938607\n",
      "Epoch: 32, Train Loss: 0.0011208600464275896, Test Loss: 0.000291735476699941\n",
      "Epoch: 33, Iteration: 100, Loss: 0.030514976431732066\n",
      "Epoch: 33, Iteration: 200, Loss: 0.03143169452960137\n",
      "Epoch: 33, Iteration: 300, Loss: 0.029063931142445654\n",
      "Epoch: 33, Iteration: 400, Loss: 0.028130468519520946\n",
      "Epoch: 33, Iteration: 500, Loss: 0.026669651793781668\n",
      "Epoch: 33, Iteration: 600, Loss: 0.027553967767744325\n",
      "Epoch: 33, Iteration: 700, Loss: 0.027043732538004406\n",
      "Epoch: 33, Iteration: 800, Loss: 0.02668598348100204\n",
      "Epoch: 33, Train Loss: 0.001137428259069834, Test Loss: 0.0003450202934947165\n",
      "Epoch: 34, Iteration: 100, Loss: 0.027520046816789545\n",
      "Epoch: 34, Iteration: 200, Loss: 0.027266692675766535\n",
      "Epoch: 34, Iteration: 300, Loss: 0.027995986718451604\n",
      "Epoch: 34, Iteration: 400, Loss: 0.026652781423763372\n",
      "Epoch: 34, Iteration: 500, Loss: 0.027528320570127107\n",
      "Epoch: 34, Iteration: 600, Loss: 0.03003671401529573\n",
      "Epoch: 34, Iteration: 700, Loss: 0.03103296823974233\n",
      "Epoch: 34, Iteration: 800, Loss: 0.028910234454087913\n",
      "Epoch: 34, Train Loss: 0.0011316964088017465, Test Loss: 0.0002738111688646012\n",
      "Epoch: 35, Iteration: 100, Loss: 0.028905708575621247\n",
      "Epoch: 35, Iteration: 200, Loss: 0.02623638698423747\n",
      "Epoch: 35, Iteration: 300, Loss: 0.026295113246305846\n",
      "Epoch: 35, Iteration: 400, Loss: 0.027649113646475598\n",
      "Epoch: 35, Iteration: 500, Loss: 0.026121420654817484\n",
      "Epoch: 35, Iteration: 600, Loss: 0.027507880979101174\n",
      "Epoch: 35, Iteration: 700, Loss: 0.028281231876462698\n",
      "Epoch: 35, Iteration: 800, Loss: 0.026427750242874026\n",
      "Epoch: 35, Train Loss: 0.0010879198135513048, Test Loss: 0.0002756341003618645\n",
      "Epoch: 36, Iteration: 100, Loss: 0.027554550993954763\n",
      "Epoch: 36, Iteration: 200, Loss: 0.026404060801723972\n",
      "Epoch: 36, Iteration: 300, Loss: 0.027657120779622346\n",
      "Epoch: 36, Iteration: 400, Loss: 0.02707832577289082\n",
      "Epoch: 36, Iteration: 500, Loss: 0.02711860310228076\n",
      "Epoch: 36, Iteration: 600, Loss: 0.027670102281263098\n",
      "Epoch: 36, Iteration: 700, Loss: 0.02739327130257152\n",
      "Epoch: 36, Iteration: 800, Loss: 0.027386163317714818\n",
      "Epoch: 36, Train Loss: 0.001097741181917128, Test Loss: 0.0002540204046194309\n",
      "Epoch: 37, Iteration: 100, Loss: 0.027528412276296876\n",
      "Epoch: 37, Iteration: 200, Loss: 0.028071676744730212\n",
      "Epoch: 37, Iteration: 300, Loss: 0.02697448509570677\n",
      "Epoch: 37, Iteration: 400, Loss: 0.025621475346270017\n",
      "Epoch: 37, Iteration: 500, Loss: 0.02553036095923744\n",
      "Epoch: 37, Iteration: 600, Loss: 0.026706692224252038\n",
      "Epoch: 37, Iteration: 700, Loss: 0.029139081496396102\n",
      "Epoch: 37, Iteration: 800, Loss: 0.02596121707756538\n",
      "Epoch: 37, Train Loss: 0.001077016644541406, Test Loss: 0.0002499067173328697\n",
      "Epoch: 38, Iteration: 100, Loss: 0.02725477606873028\n",
      "Epoch: 38, Iteration: 200, Loss: 0.026855593649088405\n",
      "Epoch: 38, Iteration: 300, Loss: 0.025166589563013986\n",
      "Epoch: 38, Iteration: 400, Loss: 0.02553704484307673\n",
      "Epoch: 38, Iteration: 500, Loss: 0.027471077162772417\n",
      "Epoch: 38, Iteration: 600, Loss: 0.026990799568011425\n",
      "Epoch: 38, Iteration: 700, Loss: 0.027938907413044944\n",
      "Epoch: 38, Iteration: 800, Loss: 0.027143813917064108\n",
      "Epoch: 38, Train Loss: 0.0010873009643194477, Test Loss: 0.0003521701304643299\n",
      "Epoch: 39, Iteration: 100, Loss: 0.02950834196235519\n",
      "Epoch: 39, Iteration: 200, Loss: 0.02791216292825993\n",
      "Epoch: 39, Iteration: 300, Loss: 0.025998186771175824\n",
      "Epoch: 39, Iteration: 400, Loss: 0.02749117232451681\n",
      "Epoch: 39, Iteration: 500, Loss: 0.027239766452112235\n",
      "Epoch: 39, Iteration: 600, Loss: 0.02518972881080117\n",
      "Epoch: 39, Iteration: 700, Loss: 0.02683341648662463\n",
      "Epoch: 39, Iteration: 800, Loss: 0.026231635551084764\n",
      "Epoch: 39, Train Loss: 0.0010791370901466917, Test Loss: 0.0002552650303983297\n",
      "Epoch: 40, Iteration: 100, Loss: 0.026354456116678193\n",
      "Epoch: 40, Iteration: 200, Loss: 0.027183544749277644\n",
      "Epoch: 40, Iteration: 300, Loss: 0.026999600158887915\n",
      "Epoch: 40, Iteration: 400, Loss: 0.031717870238935575\n",
      "Epoch: 40, Iteration: 500, Loss: 0.024622605298645794\n",
      "Epoch: 40, Iteration: 600, Loss: 0.025202084027114324\n",
      "Epoch: 40, Iteration: 700, Loss: 0.026774290046887472\n",
      "Epoch: 40, Iteration: 800, Loss: 0.027207700652070343\n",
      "Epoch: 40, Train Loss: 0.001077219051623305, Test Loss: 0.0002969077375609093\n",
      "Epoch: 41, Iteration: 100, Loss: 0.026389581966213882\n",
      "Epoch: 41, Iteration: 200, Loss: 0.025454164933762513\n",
      "Epoch: 41, Iteration: 300, Loss: 0.02765077391813975\n",
      "Epoch: 41, Iteration: 400, Loss: 0.028943107710801996\n",
      "Epoch: 41, Iteration: 500, Loss: 0.025846344578894787\n",
      "Epoch: 41, Iteration: 600, Loss: 0.026755444443551823\n",
      "Epoch: 41, Iteration: 700, Loss: 0.028431959231966175\n",
      "Epoch: 41, Iteration: 800, Loss: 0.03007039675139822\n",
      "Epoch: 41, Train Loss: 0.0011058405970737193, Test Loss: 0.00026576713387144104\n",
      "Epoch: 42, Iteration: 100, Loss: 0.026948129598167725\n",
      "Epoch: 42, Iteration: 200, Loss: 0.026137949258554727\n",
      "Epoch: 42, Iteration: 300, Loss: 0.025207110622432083\n",
      "Epoch: 42, Iteration: 400, Loss: 0.026448369841091335\n",
      "Epoch: 42, Iteration: 500, Loss: 0.025845317664789036\n",
      "Epoch: 42, Iteration: 600, Loss: 0.02675560129864607\n",
      "Epoch: 42, Iteration: 700, Loss: 0.026524834684096277\n",
      "Epoch: 42, Iteration: 800, Loss: 0.026783793917275034\n",
      "Epoch: 42, Train Loss: 0.0010501161884395233, Test Loss: 0.00024539484391106973\n",
      "Epoch: 43, Iteration: 100, Loss: 0.02589132201683242\n",
      "Epoch: 43, Iteration: 200, Loss: 0.02601701786625199\n",
      "Epoch: 43, Iteration: 300, Loss: 0.025420259742531925\n",
      "Epoch: 43, Iteration: 400, Loss: 0.02585940485005267\n",
      "Epoch: 43, Iteration: 500, Loss: 0.025354607379995286\n",
      "Epoch: 43, Iteration: 600, Loss: 0.027032175959902816\n",
      "Epoch: 43, Iteration: 700, Loss: 0.026007993132225238\n",
      "Epoch: 43, Iteration: 800, Loss: 0.026353278808528557\n",
      "Epoch: 43, Train Loss: 0.0010395488777672143, Test Loss: 0.00026340295843917377\n",
      "Epoch: 44, Iteration: 100, Loss: 0.028805613474105485\n",
      "Epoch: 44, Iteration: 200, Loss: 0.02786897448822856\n",
      "Epoch: 44, Iteration: 300, Loss: 0.02675275543879252\n",
      "Epoch: 44, Iteration: 400, Loss: 0.027527097088750452\n",
      "Epoch: 44, Iteration: 500, Loss: 0.0265894355106866\n",
      "Epoch: 44, Iteration: 600, Loss: 0.02829138026572764\n",
      "Epoch: 44, Iteration: 700, Loss: 0.027779851166997105\n",
      "Epoch: 44, Iteration: 800, Loss: 0.027266387900453992\n",
      "Epoch: 44, Train Loss: 0.0011006387277594273, Test Loss: 0.0002662915537806554\n",
      "Epoch: 45, Iteration: 100, Loss: 0.02627269207732752\n",
      "Epoch: 45, Iteration: 200, Loss: 0.02742546732770279\n",
      "Epoch: 45, Iteration: 300, Loss: 0.02618963121494744\n",
      "Epoch: 45, Iteration: 400, Loss: 0.027050583317759447\n",
      "Epoch: 45, Iteration: 500, Loss: 0.02502795353939291\n",
      "Epoch: 45, Iteration: 600, Loss: 0.026840582853765227\n",
      "Epoch: 45, Iteration: 700, Loss: 0.026428825469338335\n",
      "Epoch: 45, Iteration: 800, Loss: 0.026254053809680045\n",
      "Epoch: 45, Train Loss: 0.0010570348650839369, Test Loss: 0.0002531494928120468\n",
      "Epoch: 46, Iteration: 100, Loss: 0.02551886488799937\n",
      "Epoch: 46, Iteration: 200, Loss: 0.025915521749993786\n",
      "Epoch: 46, Iteration: 300, Loss: 0.02508306864183396\n",
      "Epoch: 46, Iteration: 400, Loss: 0.024416188840405084\n",
      "Epoch: 46, Iteration: 500, Loss: 0.026566995191387832\n",
      "Epoch: 46, Iteration: 600, Loss: 0.025599513071938418\n",
      "Epoch: 46, Iteration: 700, Loss: 0.0260650729178451\n",
      "Epoch: 46, Iteration: 800, Loss: 0.02560125061427243\n",
      "Epoch: 46, Train Loss: 0.001021514745219396, Test Loss: 0.00025403361313135154\n",
      "Epoch: 47, Iteration: 100, Loss: 0.027518095172126777\n",
      "Epoch: 47, Iteration: 200, Loss: 0.0249487462133402\n",
      "Epoch: 47, Iteration: 300, Loss: 0.024310918990522623\n",
      "Epoch: 47, Iteration: 400, Loss: 0.025594682490918785\n",
      "Epoch: 47, Iteration: 500, Loss: 0.026170894227107055\n",
      "Epoch: 47, Iteration: 600, Loss: 0.025745716644451022\n",
      "Epoch: 47, Iteration: 700, Loss: 0.025315378472441807\n",
      "Epoch: 47, Iteration: 800, Loss: 0.026732040190836415\n",
      "Epoch: 47, Train Loss: 0.001032687768097087, Test Loss: 0.0002578560901727433\n",
      "Epoch: 48, Iteration: 100, Loss: 0.02543031437380705\n",
      "Epoch: 48, Iteration: 200, Loss: 0.0260483246674994\n",
      "Epoch: 48, Iteration: 300, Loss: 0.025364526809426025\n",
      "Epoch: 48, Iteration: 400, Loss: 0.028779084401321597\n",
      "Epoch: 48, Iteration: 500, Loss: 0.02588482276769355\n",
      "Epoch: 48, Iteration: 600, Loss: 0.026842665305593982\n",
      "Epoch: 48, Iteration: 700, Loss: 0.02620244624267798\n",
      "Epoch: 48, Iteration: 800, Loss: 0.02422187852789648\n",
      "Epoch: 48, Train Loss: 0.0010398022898484647, Test Loss: 0.00024285927830126634\n",
      "Epoch: 49, Iteration: 100, Loss: 0.024584805447375402\n",
      "Epoch: 49, Iteration: 200, Loss: 0.02759594438248314\n",
      "Epoch: 49, Iteration: 300, Loss: 0.024160785003914498\n",
      "Epoch: 49, Iteration: 400, Loss: 0.025751629975275137\n",
      "Epoch: 49, Iteration: 500, Loss: 0.028532283744425513\n",
      "Epoch: 49, Iteration: 600, Loss: 0.02576776430942118\n",
      "Epoch: 49, Iteration: 700, Loss: 0.024534901240258478\n",
      "Epoch: 49, Iteration: 800, Loss: 0.024894797985325567\n",
      "Epoch: 49, Train Loss: 0.0010272103082415464, Test Loss: 0.0002586971904705858\n",
      "Epoch: 50, Iteration: 100, Loss: 0.024625299323815852\n",
      "Epoch: 50, Iteration: 200, Loss: 0.024580791519838385\n",
      "Epoch: 50, Iteration: 300, Loss: 0.02701783888915088\n",
      "Epoch: 50, Iteration: 400, Loss: 0.025778338924283162\n",
      "Epoch: 50, Iteration: 500, Loss: 0.024902991324779578\n",
      "Epoch: 50, Iteration: 600, Loss: 0.025647710877819918\n",
      "Epoch: 50, Iteration: 700, Loss: 0.02394381350313779\n",
      "Epoch: 50, Iteration: 800, Loss: 0.024151889694621786\n",
      "Epoch: 50, Train Loss: 0.001003748002035054, Test Loss: 0.0002426058226313296\n",
      "Epoch: 51, Iteration: 100, Loss: 0.02623320385464467\n",
      "Epoch: 51, Iteration: 200, Loss: 0.025942786392988637\n",
      "Epoch: 51, Iteration: 300, Loss: 0.02687175339087844\n",
      "Epoch: 51, Iteration: 400, Loss: 0.024771746844635345\n",
      "Epoch: 51, Iteration: 500, Loss: 0.02638553161523305\n",
      "Epoch: 51, Iteration: 600, Loss: 0.025715225870953873\n",
      "Epoch: 51, Iteration: 700, Loss: 0.0242191125289537\n",
      "Epoch: 51, Iteration: 800, Loss: 0.025417356213438325\n",
      "Epoch: 51, Train Loss: 0.0010280657948846693, Test Loss: 0.0002564698252381654\n",
      "Epoch: 52, Iteration: 100, Loss: 0.02670255677367095\n",
      "Epoch: 52, Iteration: 200, Loss: 0.02487004727299791\n",
      "Epoch: 52, Iteration: 300, Loss: 0.024193128643673845\n",
      "Epoch: 52, Iteration: 400, Loss: 0.02615797294129152\n",
      "Epoch: 52, Iteration: 500, Loss: 0.02544853485596832\n",
      "Epoch: 52, Iteration: 600, Loss: 0.02422539159306325\n",
      "Epoch: 52, Iteration: 700, Loss: 0.02551602192397695\n",
      "Epoch: 52, Iteration: 800, Loss: 0.027076306330855004\n",
      "Epoch: 52, Train Loss: 0.0010227206107314831, Test Loss: 0.0002637859695421506\n",
      "Epoch: 53, Iteration: 100, Loss: 0.024139191722497344\n",
      "Epoch: 53, Iteration: 200, Loss: 0.02551696400041692\n",
      "Epoch: 53, Iteration: 300, Loss: 0.02506157678726595\n",
      "Epoch: 53, Iteration: 400, Loss: 0.024203019609558396\n",
      "Epoch: 53, Iteration: 500, Loss: 0.02496912667993456\n",
      "Epoch: 53, Iteration: 600, Loss: 0.02427931448619347\n",
      "Epoch: 53, Iteration: 700, Loss: 0.02730725141009316\n",
      "Epoch: 53, Iteration: 800, Loss: 0.025542494957335293\n"
     ]
    }
   ],
   "source": [
    "train_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "test_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "\n",
    "patience = config[\"patience\"]\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"])):\n",
    "    loss_ae1_train = 0\n",
    "    loss_ae2_train = 0\n",
    "    loss_dyn_train = 0\n",
    "\n",
    "    current_train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    warmup = 1 if epoch <= config[\"warmup\"] else 0\n",
    "\n",
    "    encoder.train()\n",
    "    dynamics.train()\n",
    "    decoder.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i, (x_t, x_tau) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z_t = encoder(x_t)\n",
    "        x_t_pred = decoder(z_t)\n",
    "\n",
    "        z_tau_pred = dynamics(z_t)\n",
    "        z_tau = encoder(x_tau)\n",
    "        x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_ae1 = criterion(x_t, x_t_pred)\n",
    "        loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "        loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "        loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "        # Backward pass\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += loss_total.item()\n",
    "        epoch_train_loss += loss_total.item()\n",
    "\n",
    "        loss_ae1_train += loss_ae1.item()\n",
    "        loss_ae2_train += loss_ae2.item()\n",
    "        loss_dyn_train += loss_dyn.item() * warmup\n",
    "        counter += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Epoch: {}, Iteration: {}, Loss: {}\".format(epoch, i+1, current_train_loss))\n",
    "            current_train_loss = 0\n",
    "        \n",
    "    train_losses['loss_ae1'].append(loss_ae1_train / counter)\n",
    "    train_losses['loss_ae2'].append(loss_ae2_train / counter)\n",
    "    train_losses['loss_dyn'].append(loss_dyn_train / counter)\n",
    "    train_losses['loss_total'].append(epoch_train_loss / counter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_ae1_test = 0\n",
    "        loss_ae2_test = 0\n",
    "        loss_dyn_test = 0\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        encoder.eval()\n",
    "        dynamics.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        counter = 0\n",
    "        for i, (x_t, x_tau) in enumerate(test_loader,0):\n",
    "            # Forward pass\n",
    "            z_t = encoder(x_t)\n",
    "            x_t_pred = decoder(z_t)\n",
    "\n",
    "            z_tau_pred = dynamics(z_t)\n",
    "            z_tau = encoder(x_tau)\n",
    "            x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_ae1 = criterion(x_t, x_t_pred)\n",
    "            loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "            loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "            loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "            epoch_test_loss += loss_total.item()\n",
    "\n",
    "            loss_ae1_test += loss_ae1.item()\n",
    "            loss_ae2_test += loss_ae2.item()\n",
    "            loss_dyn_test += loss_dyn.item() * warmup\n",
    "            counter += 1\n",
    "\n",
    "        test_losses['loss_ae1'].append(loss_ae1_test / counter)\n",
    "        test_losses['loss_ae2'].append(loss_ae2_test / counter)\n",
    "        test_losses['loss_dyn'].append(loss_dyn_test / counter)\n",
    "        test_losses['loss_total'].append(epoch_test_loss / counter)\n",
    "\n",
    "        if epoch >= patience and np.mean(test_losses['loss_total'][-patience:]) >= np.mean(test_losses['loss_total'][-patience:-1]):\n",
    "            break\n",
    "\n",
    "        if epoch >= config[\"warmup\"]:\n",
    "            scheduler.step(epoch_test_loss / counter)\n",
    "        \n",
    "    print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch, epoch_train_loss / counter, epoch_test_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(encoder, os.path.join(config[\"model_dir\"], \"encoder.pt\"))\n",
    "torch.save(dynamics, os.path.join(config[\"model_dir\"], \"dynamics.pt\"))\n",
    "torch.save(decoder, os.path.join(config[\"model_dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_dir doesn't exist, create it\n",
    "if not os.path.exists(config[\"log_dir\"]):\n",
    "    os.makedirs(config[\"log_dir\"])\n",
    "\n",
    "# Save the losses as a pickle file\n",
    "with open(os.path.join(config[\"log_dir\"], \"losses.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": train_losses, \"test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
