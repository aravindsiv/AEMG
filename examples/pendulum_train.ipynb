{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from AEMG.data_utils import DynamicsDataset\n",
    "from AEMG.systems.utils import get_system\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from AEMG.models import *\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for:  cartpole\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:09<00:00, 10.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# config_fname = \"config/pendulum_lqr_1K.txt\"\n",
    "# config_fname = \"config/physics_pendulum.txt\"\n",
    "# config_fname = \"config/hopper.txt\"\n",
    "config_fname = \"config/cartpole_lqr.txt\"\n",
    "\n",
    "with open(config_fname, 'r') as f:\n",
    "    config = eval(f.read())\n",
    "\n",
    "dataset = DynamicsDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  399960\n",
      "Test size:  99991\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "print(\"Train size: \", len(train_dataset))\n",
    "print(\"Test size: \", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = Encoder(config[\"high_dims\"],config[\"low_dims\"])\n",
    "dynamics = LatentDynamics(config[\"low_dims\"])\n",
    "decoder  = Decoder(config[\"low_dims\"],config[\"high_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(set(list(encoder.parameters()) + list(dynamics.parameters()) + list(decoder.parameters())), \n",
    "    lr=config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.001, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb6051f66414d829f7909648fe31cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 12.52175573259592\n",
      "Epoch: 0, Iteration: 200, Loss: 7.023273665457964\n",
      "Epoch: 0, Iteration: 300, Loss: 0.7398978075943887\n",
      "Epoch: 0, Train Loss: 0.21253305822800922, Test Loss: 0.005345302505646737\n",
      "Epoch: 1, Iteration: 100, Loss: 0.47468661330640316\n",
      "Epoch: 1, Iteration: 200, Loss: 0.4110873220488429\n",
      "Epoch: 1, Iteration: 300, Loss: 0.3715329410042614\n",
      "Epoch: 1, Train Loss: 0.016036899990820308, Test Loss: 0.003357100568958843\n",
      "Epoch: 2, Iteration: 100, Loss: 0.3296960638836026\n",
      "Epoch: 2, Iteration: 200, Loss: 0.31517870258539915\n",
      "Epoch: 2, Iteration: 300, Loss: 0.3165636216290295\n",
      "Epoch: 2, Train Loss: 0.012679125500690877, Test Loss: 0.003065244104637175\n",
      "Epoch: 3, Iteration: 100, Loss: 0.30243758019059896\n",
      "Epoch: 3, Iteration: 200, Loss: 0.30344660580158234\n",
      "Epoch: 3, Iteration: 300, Loss: 0.289488960057497\n",
      "Epoch: 3, Train Loss: 0.011768834330426643, Test Loss: 0.00284458075802089\n",
      "Epoch: 4, Iteration: 100, Loss: 0.2789980892557651\n",
      "Epoch: 4, Iteration: 200, Loss: 0.2456200240412727\n",
      "Epoch: 4, Iteration: 300, Loss: 0.16142287710681558\n",
      "Epoch: 4, Train Loss: 0.008137286581902062, Test Loss: 0.0012133850339486512\n",
      "Epoch: 5, Iteration: 100, Loss: 0.1123675222042948\n",
      "Epoch: 5, Iteration: 200, Loss: 0.11023343104170635\n",
      "Epoch: 5, Iteration: 300, Loss: 0.1032807506271638\n",
      "Epoch: 5, Train Loss: 0.004235631515443021, Test Loss: 0.0009714186806896967\n",
      "Epoch: 6, Iteration: 100, Loss: 0.09493307047523558\n",
      "Epoch: 6, Iteration: 200, Loss: 0.0918545299791731\n",
      "Epoch: 6, Iteration: 300, Loss: 0.08879951888229698\n",
      "Epoch: 6, Train Loss: 0.003561715056706334, Test Loss: 0.0008474502775447481\n",
      "Epoch: 7, Iteration: 100, Loss: 0.08196242625126615\n",
      "Epoch: 7, Iteration: 200, Loss: 0.07854468340519816\n",
      "Epoch: 7, Iteration: 300, Loss: 0.08004584774607792\n",
      "Epoch: 7, Train Loss: 0.0031360772735919158, Test Loss: 0.0007583154986104073\n",
      "Epoch: 8, Iteration: 100, Loss: 0.0728865284472704\n",
      "Epoch: 8, Iteration: 200, Loss: 0.07201253017410636\n",
      "Epoch: 8, Iteration: 300, Loss: 0.06939700414659455\n",
      "Epoch: 8, Train Loss: 0.0028009292992112246, Test Loss: 0.0006793668877917855\n",
      "Epoch: 9, Iteration: 100, Loss: 0.06488036684459075\n",
      "Epoch: 9, Iteration: 200, Loss: 0.06480482147890143\n",
      "Epoch: 9, Iteration: 300, Loss: 0.060908940591616556\n",
      "Epoch: 9, Train Loss: 0.0025294683544007986, Test Loss: 0.0006303315393018479\n",
      "Epoch: 10, Iteration: 100, Loss: 0.060447728290455416\n",
      "Epoch: 10, Iteration: 200, Loss: 0.06124062152230181\n",
      "Epoch: 10, Iteration: 300, Loss: 0.05948170513147488\n",
      "Epoch: 10, Train Loss: 0.002402964359617374, Test Loss: 0.000604340535165666\n",
      "Epoch: 11, Iteration: 100, Loss: 0.057810116646578535\n",
      "Epoch: 11, Iteration: 200, Loss: 0.05816001835046336\n",
      "Epoch: 11, Iteration: 300, Loss: 0.05882818278041668\n",
      "Epoch: 11, Train Loss: 0.002315933009543057, Test Loss: 0.0006063437527069366\n",
      "Epoch: 12, Iteration: 100, Loss: 0.057666244014399126\n",
      "Epoch: 12, Iteration: 200, Loss: 0.05717744427965954\n",
      "Epoch: 12, Iteration: 300, Loss: 0.056019031297182664\n",
      "Epoch: 12, Train Loss: 0.0022407139242241842, Test Loss: 0.000562053784663903\n",
      "Epoch: 13, Iteration: 100, Loss: 0.056397571868728846\n",
      "Epoch: 13, Iteration: 200, Loss: 0.05250567159964703\n",
      "Epoch: 13, Iteration: 300, Loss: 0.05376774846808985\n",
      "Epoch: 13, Train Loss: 0.002153831440836609, Test Loss: 0.0005421791619464413\n",
      "Epoch: 14, Iteration: 100, Loss: 0.05349471399676986\n",
      "Epoch: 14, Iteration: 200, Loss: 0.04992288089124486\n",
      "Epoch: 14, Iteration: 300, Loss: 0.05253557025571354\n",
      "Epoch: 14, Train Loss: 0.0020800592390848895, Test Loss: 0.000538258910730329\n",
      "Epoch: 15, Iteration: 100, Loss: 0.05131171824177727\n",
      "Epoch: 15, Iteration: 200, Loss: 0.04977661365410313\n",
      "Epoch: 15, Iteration: 300, Loss: 0.049038476980058476\n",
      "Epoch: 15, Train Loss: 0.001996435648203371, Test Loss: 0.0005017443763790652\n",
      "Epoch: 16, Iteration: 100, Loss: 0.0488897432514932\n",
      "Epoch: 16, Iteration: 200, Loss: 0.048530794971156865\n",
      "Epoch: 16, Iteration: 300, Loss: 0.047860055026831105\n",
      "Epoch: 16, Train Loss: 0.0019213504800026553, Test Loss: 0.00048022635776445044\n",
      "Epoch: 17, Iteration: 100, Loss: 0.044573590595973656\n",
      "Epoch: 17, Iteration: 200, Loss: 0.04725672325002961\n",
      "Epoch: 17, Iteration: 300, Loss: 0.04642697097733617\n",
      "Epoch: 17, Train Loss: 0.0018462560561898981, Test Loss: 0.0004666159924343038\n",
      "Epoch: 18, Iteration: 100, Loss: 0.04544088034890592\n",
      "Epoch: 18, Iteration: 200, Loss: 0.04559760079428088\n",
      "Epoch: 18, Iteration: 300, Loss: 0.04327736151753925\n",
      "Epoch: 18, Train Loss: 0.0017892458841111986, Test Loss: 0.00045733349706635485\n",
      "Epoch: 19, Iteration: 100, Loss: 0.04671763032092713\n",
      "Epoch: 19, Iteration: 200, Loss: 0.04251953394850716\n",
      "Epoch: 19, Iteration: 300, Loss: 0.042727571562863886\n",
      "Epoch: 19, Train Loss: 0.0017498703691300613, Test Loss: 0.0004468842868975896\n",
      "Epoch: 20, Iteration: 100, Loss: 0.04304239968769252\n",
      "Epoch: 20, Iteration: 200, Loss: 0.04417342698434368\n",
      "Epoch: 20, Iteration: 300, Loss: 0.04168286742060445\n",
      "Epoch: 20, Train Loss: 0.0017246723811175407, Test Loss: 0.00043982026797994894\n",
      "Epoch: 21, Iteration: 100, Loss: 0.04344244851381518\n",
      "Epoch: 21, Iteration: 200, Loss: 0.04216790589271113\n",
      "Epoch: 21, Iteration: 300, Loss: 0.0425956406397745\n",
      "Epoch: 21, Train Loss: 0.00169625051428887, Test Loss: 0.0004311889467632626\n",
      "Epoch: 22, Iteration: 100, Loss: 0.04292212109430693\n",
      "Epoch: 22, Iteration: 200, Loss: 0.04227330222784076\n",
      "Epoch: 22, Iteration: 300, Loss: 0.0418089966988191\n",
      "Epoch: 22, Train Loss: 0.001674738025402042, Test Loss: 0.0004277490787695124\n",
      "Epoch: 23, Iteration: 100, Loss: 0.04254167163162492\n",
      "Epoch: 23, Iteration: 200, Loss: 0.04110067372675985\n",
      "Epoch: 23, Iteration: 300, Loss: 0.04133392943185754\n",
      "Epoch: 23, Train Loss: 0.001658504228618433, Test Loss: 0.0004298141308138337\n",
      "Epoch: 24, Iteration: 100, Loss: 0.03964083280880004\n",
      "Epoch: 24, Iteration: 200, Loss: 0.04200173665594775\n",
      "Epoch: 24, Iteration: 300, Loss: 0.04148498876020312\n",
      "Epoch: 24, Train Loss: 0.0016325818327533519, Test Loss: 0.00042487448660481\n",
      "Epoch: 25, Iteration: 100, Loss: 0.04108415365044493\n",
      "Epoch: 25, Iteration: 200, Loss: 0.04005943324591499\n",
      "Epoch: 25, Iteration: 300, Loss: 0.04029386237380095\n",
      "Epoch: 25, Train Loss: 0.001616212755277734, Test Loss: 0.00041515827734303677\n",
      "Epoch: 26, Iteration: 100, Loss: 0.03905911641777493\n",
      "Epoch: 26, Iteration: 200, Loss: 0.04003619482682552\n",
      "Epoch: 26, Iteration: 300, Loss: 0.040916918456787243\n",
      "Epoch: 26, Train Loss: 0.0015820147686730595, Test Loss: 0.0004065045182609798\n",
      "Epoch: 27, Iteration: 100, Loss: 0.0396238109387923\n",
      "Epoch: 27, Iteration: 200, Loss: 0.03884095301327761\n",
      "Epoch: 27, Iteration: 300, Loss: 0.03904906698153354\n",
      "Epoch: 27, Train Loss: 0.001567192145353611, Test Loss: 0.0004022254974448255\n",
      "Epoch: 28, Iteration: 100, Loss: 0.04030421945208218\n",
      "Epoch: 28, Iteration: 200, Loss: 0.03849572368199006\n",
      "Epoch: 28, Iteration: 300, Loss: 0.03761454396590125\n",
      "Epoch: 28, Train Loss: 0.001540689733608778, Test Loss: 0.00038531670636112554\n",
      "Epoch: 29, Iteration: 100, Loss: 0.037780636732350104\n",
      "Epoch: 29, Iteration: 200, Loss: 0.038191525905858725\n",
      "Epoch: 29, Iteration: 300, Loss: 0.03761774513986893\n",
      "Epoch: 29, Train Loss: 0.0015183295993721208, Test Loss: 0.00038117601397228714\n",
      "Epoch: 30, Iteration: 100, Loss: 0.037256169016472995\n",
      "Epoch: 30, Iteration: 200, Loss: 0.038745852158172056\n",
      "Epoch: 30, Iteration: 300, Loss: 0.03789027521270327\n",
      "Epoch: 30, Train Loss: 0.0014901571001023606, Test Loss: 0.00037346806508436683\n",
      "Epoch: 31, Iteration: 100, Loss: 0.036816852385527454\n",
      "Epoch: 31, Iteration: 200, Loss: 0.03688987276109401\n",
      "Epoch: 31, Iteration: 300, Loss: 0.037117366955499165\n",
      "Epoch: 31, Train Loss: 0.0014834549736554678, Test Loss: 0.0004004265368222354\n",
      "Epoch: 32, Iteration: 100, Loss: 0.03623439861985389\n",
      "Epoch: 32, Iteration: 200, Loss: 0.03648041776614264\n",
      "Epoch: 32, Iteration: 300, Loss: 0.037102600355865434\n",
      "Epoch: 32, Train Loss: 0.0014705384485435445, Test Loss: 0.0003817637168270137\n",
      "Epoch: 33, Iteration: 100, Loss: 0.035320811890414916\n",
      "Epoch: 33, Iteration: 200, Loss: 0.03558944270480424\n",
      "Epoch: 33, Iteration: 300, Loss: 0.03687584183353465\n",
      "Epoch: 33, Train Loss: 0.001450501542718967, Test Loss: 0.0003681379343366383\n",
      "Epoch: 34, Iteration: 100, Loss: 0.03520788374589756\n",
      "Epoch: 34, Iteration: 200, Loss: 0.035705655202036723\n",
      "Epoch: 34, Iteration: 300, Loss: 0.03725645085796714\n",
      "Epoch: 34, Train Loss: 0.0014395837518220236, Test Loss: 0.0003703738931312264\n",
      "Epoch: 35, Iteration: 100, Loss: 0.03640892125258688\n",
      "Epoch: 35, Iteration: 200, Loss: 0.037095049337949604\n",
      "Epoch: 35, Iteration: 300, Loss: 0.03510841132083442\n",
      "Epoch: 35, Train Loss: 0.0014378561621070935, Test Loss: 0.0003672384062119075\n",
      "Epoch: 36, Iteration: 100, Loss: 0.03594207232526969\n",
      "Epoch: 36, Iteration: 200, Loss: 0.0347222491982393\n",
      "Epoch: 36, Iteration: 300, Loss: 0.0355643265938852\n",
      "Epoch: 36, Train Loss: 0.0014220705481272247, Test Loss: 0.0003570412248206724\n",
      "Epoch: 37, Iteration: 100, Loss: 0.03590457432437688\n",
      "Epoch: 37, Iteration: 200, Loss: 0.034879112005000934\n",
      "Epoch: 37, Iteration: 300, Loss: 0.03603473391558509\n",
      "Epoch: 37, Train Loss: 0.001414353824137444, Test Loss: 0.00036146301922877793\n",
      "Epoch: 38, Iteration: 100, Loss: 0.03560944536002353\n",
      "Epoch: 38, Iteration: 200, Loss: 0.03470384948013816\n",
      "Epoch: 38, Iteration: 300, Loss: 0.03539363191521261\n",
      "Epoch: 38, Train Loss: 0.0014042852963772317, Test Loss: 0.0003593627668439639\n",
      "Epoch: 39, Iteration: 100, Loss: 0.03570953231246676\n",
      "Epoch: 39, Iteration: 200, Loss: 0.036865707428660244\n",
      "Epoch: 39, Iteration: 300, Loss: 0.03402149832982104\n",
      "Epoch: 39, Train Loss: 0.0014031011485304608, Test Loss: 0.0003509300806065451\n",
      "Epoch: 40, Iteration: 100, Loss: 0.03436316103034187\n",
      "Epoch: 40, Iteration: 200, Loss: 0.03440591145772487\n",
      "Epoch: 40, Iteration: 300, Loss: 0.03572658967459574\n",
      "Epoch: 40, Train Loss: 0.0013868388495935431, Test Loss: 0.0003818996269636008\n",
      "Epoch: 41, Iteration: 100, Loss: 0.033897228189744055\n",
      "Epoch: 41, Iteration: 200, Loss: 0.03493490946129896\n",
      "Epoch: 41, Iteration: 300, Loss: 0.034216867585200816\n",
      "Epoch: 41, Train Loss: 0.0013786352388632046, Test Loss: 0.0003693319816434547\n",
      "Epoch: 42, Iteration: 100, Loss: 0.03357224939099979\n",
      "Epoch: 42, Iteration: 200, Loss: 0.03407403381424956\n",
      "Epoch: 42, Iteration: 300, Loss: 0.0351841181400232\n",
      "Epoch: 42, Train Loss: 0.0013770267956270076, Test Loss: 0.0003622960617316754\n",
      "Epoch: 43, Iteration: 100, Loss: 0.03570630101603456\n",
      "Epoch: 43, Iteration: 200, Loss: 0.03453010262455791\n",
      "Epoch: 43, Iteration: 300, Loss: 0.032545774229220115\n",
      "Epoch: 43, Train Loss: 0.0013671867577335322, Test Loss: 0.00034274057840348734\n",
      "Epoch: 44, Iteration: 100, Loss: 0.035525300685549155\n",
      "Epoch: 44, Iteration: 200, Loss: 0.034301910040085204\n",
      "Epoch: 44, Iteration: 300, Loss: 0.03404122681240551\n",
      "Epoch: 44, Train Loss: 0.0013593523467627202, Test Loss: 0.0003417244812708861\n",
      "Epoch: 45, Iteration: 100, Loss: 0.03330296964850277\n",
      "Epoch: 45, Iteration: 200, Loss: 0.03446096123661846\n",
      "Epoch: 45, Iteration: 300, Loss: 0.03420304240717087\n",
      "Epoch: 45, Train Loss: 0.001353752680986464, Test Loss: 0.00035177081201477357\n",
      "Epoch: 46, Iteration: 100, Loss: 0.033868579324916936\n",
      "Epoch: 46, Iteration: 200, Loss: 0.03349873668048531\n",
      "Epoch: 46, Iteration: 300, Loss: 0.03289640411094297\n",
      "Epoch: 46, Train Loss: 0.0013456128464004367, Test Loss: 0.0003485485958291826\n",
      "Epoch: 47, Iteration: 100, Loss: 0.034953415946802124\n",
      "Epoch: 47, Iteration: 200, Loss: 0.03381147862819489\n",
      "Epoch: 47, Iteration: 300, Loss: 0.033128309340099804\n",
      "Epoch: 47, Train Loss: 0.0013445633645015008, Test Loss: 0.0003397907586281226\n",
      "Epoch: 48, Iteration: 100, Loss: 0.03394979966105893\n",
      "Epoch: 48, Iteration: 200, Loss: 0.03374983313551638\n",
      "Epoch: 48, Iteration: 300, Loss: 0.03259500795684289\n",
      "Epoch: 48, Train Loss: 0.0013390813377443511, Test Loss: 0.00036360947899306573\n",
      "Epoch: 49, Iteration: 100, Loss: 0.03328059511841275\n",
      "Epoch: 49, Iteration: 200, Loss: 0.03386361360026058\n",
      "Epoch: 49, Iteration: 300, Loss: 0.03252407918625977\n",
      "Epoch: 49, Train Loss: 0.001328193476870276, Test Loss: 0.0003384644623277994\n",
      "Epoch: 50, Iteration: 100, Loss: 0.033616030123084784\n",
      "Epoch: 50, Iteration: 200, Loss: 0.0326596824452281\n",
      "Epoch: 50, Iteration: 300, Loss: 0.033853470464237034\n",
      "Epoch: 50, Train Loss: 0.0013350106299705139, Test Loss: 0.000342263107791505\n",
      "Epoch: 51, Iteration: 100, Loss: 0.03291624404664617\n",
      "Epoch: 51, Iteration: 200, Loss: 0.032725699464208446\n",
      "Epoch: 51, Iteration: 300, Loss: 0.03282508009579033\n",
      "Epoch: 51, Train Loss: 0.0013102564125616408, Test Loss: 0.00033604017591308235\n",
      "Epoch: 52, Iteration: 100, Loss: 0.03340679596294649\n",
      "Epoch: 52, Iteration: 200, Loss: 0.034022870706394315\n",
      "Epoch: 52, Iteration: 300, Loss: 0.03185155655955896\n",
      "Epoch: 52, Train Loss: 0.0013175226493479628, Test Loss: 0.0003337352869233915\n",
      "Epoch: 53, Iteration: 100, Loss: 0.034132124346797355\n",
      "Epoch: 53, Iteration: 200, Loss: 0.03170769523421768\n",
      "Epoch: 53, Iteration: 300, Loss: 0.03303702517587226\n",
      "Epoch: 53, Train Loss: 0.001314422824421223, Test Loss: 0.0003387966455787192\n",
      "Epoch: 54, Iteration: 100, Loss: 0.03341198054840788\n",
      "Epoch: 54, Iteration: 200, Loss: 0.03195298001810443\n",
      "Epoch: 54, Iteration: 300, Loss: 0.033141915904707275\n",
      "Epoch: 54, Train Loss: 0.0013078425237873321, Test Loss: 0.000355171231251229\n",
      "Epoch: 55, Iteration: 100, Loss: 0.03413498766894918\n",
      "Epoch: 55, Iteration: 200, Loss: 0.03238003088335972\n",
      "Epoch: 55, Iteration: 300, Loss: 0.03146598918829113\n",
      "Epoch: 55, Train Loss: 0.001298552754392125, Test Loss: 0.0003303776121560522\n",
      "Epoch: 56, Iteration: 100, Loss: 0.03307527315337211\n",
      "Epoch: 56, Iteration: 200, Loss: 0.03182729263789952\n",
      "Epoch: 56, Iteration: 300, Loss: 0.03225471338373609\n",
      "Epoch: 56, Train Loss: 0.001296363767695005, Test Loss: 0.00033248846368075404\n",
      "Epoch: 57, Iteration: 100, Loss: 0.03236443951027468\n",
      "Epoch: 57, Iteration: 200, Loss: 0.03292165540915448\n",
      "Epoch: 57, Iteration: 300, Loss: 0.03267679436248727\n",
      "Epoch: 57, Train Loss: 0.0012960032557258478, Test Loss: 0.00032858154023712385\n",
      "Epoch: 58, Iteration: 100, Loss: 0.032321491453330964\n",
      "Epoch: 58, Iteration: 200, Loss: 0.032215034545515664\n",
      "Epoch: 58, Iteration: 300, Loss: 0.03069892192434054\n",
      "Epoch: 58, Train Loss: 0.0012806523468034171, Test Loss: 0.0003350578107139362\n",
      "Epoch: 59, Iteration: 100, Loss: 0.03219620086019859\n",
      "Epoch: 59, Iteration: 200, Loss: 0.03177214850438759\n",
      "Epoch: 59, Iteration: 300, Loss: 0.03226210059074219\n",
      "Epoch: 59, Train Loss: 0.0012792566879082244, Test Loss: 0.00032882044482049626\n",
      "Epoch: 60, Iteration: 100, Loss: 0.032519114727620035\n",
      "Epoch: 60, Iteration: 200, Loss: 0.03270244617306162\n",
      "Epoch: 60, Iteration: 300, Loss: 0.030622203659731895\n",
      "Epoch: 60, Train Loss: 0.0012784022915091518, Test Loss: 0.0003284494088468503\n",
      "Epoch: 61, Iteration: 100, Loss: 0.03291314821399283\n",
      "Epoch: 61, Iteration: 200, Loss: 0.03086287743644789\n",
      "Epoch: 61, Iteration: 300, Loss: 0.031236649898346514\n",
      "Epoch: 61, Train Loss: 0.001269669813693178, Test Loss: 0.00034385219698044835\n",
      "Epoch: 62, Iteration: 100, Loss: 0.030982704760390334\n",
      "Epoch: 62, Iteration: 200, Loss: 0.032205831361352466\n",
      "Epoch: 62, Iteration: 300, Loss: 0.032113993423990905\n",
      "Epoch: 62, Train Loss: 0.0012676303535676086, Test Loss: 0.00032117572638006616\n",
      "Epoch: 63, Iteration: 100, Loss: 0.03164063116128091\n",
      "Epoch: 63, Iteration: 200, Loss: 0.03128199273487553\n",
      "Epoch: 63, Iteration: 300, Loss: 0.032515965329366736\n",
      "Epoch: 63, Train Loss: 0.0012661800069416094, Test Loss: 0.0003250747075962492\n",
      "Epoch: 64, Iteration: 100, Loss: 0.03203397031757049\n",
      "Epoch: 64, Iteration: 200, Loss: 0.03050877091300208\n",
      "Epoch: 64, Iteration: 300, Loss: 0.031804781741811894\n",
      "Epoch: 64, Train Loss: 0.0012546661704878456, Test Loss: 0.00032538386986576666\n",
      "Epoch: 65, Iteration: 100, Loss: 0.03101086114475038\n",
      "Epoch: 65, Iteration: 200, Loss: 0.031549614723189734\n",
      "Epoch: 65, Iteration: 300, Loss: 0.03172136109787971\n"
     ]
    }
   ],
   "source": [
    "train_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "test_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "\n",
    "patience = config[\"patience\"]\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"])):\n",
    "    loss_ae1_train = 0\n",
    "    loss_ae2_train = 0\n",
    "    loss_dyn_train = 0\n",
    "\n",
    "    current_train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    warmup = 1 if epoch <= config[\"warmup\"] else 0\n",
    "\n",
    "    encoder.train()\n",
    "    dynamics.train()\n",
    "    decoder.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i, (x_t, x_tau) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z_t = encoder(x_t)\n",
    "        x_t_pred = decoder(z_t)\n",
    "\n",
    "        z_tau_pred = dynamics(z_t)\n",
    "        z_tau = encoder(x_tau)\n",
    "        x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_ae1 = criterion(x_t, x_t_pred)\n",
    "        loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "        loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "        loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "        # Backward pass\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += loss_total.item()\n",
    "        epoch_train_loss += loss_total.item()\n",
    "\n",
    "        loss_ae1_train += loss_ae1.item()\n",
    "        loss_ae2_train += loss_ae2.item()\n",
    "        loss_dyn_train += loss_dyn.item() * warmup\n",
    "        counter += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Epoch: {}, Iteration: {}, Loss: {}\".format(epoch, i+1, current_train_loss))\n",
    "            current_train_loss = 0\n",
    "        \n",
    "    train_losses['loss_ae1'].append(loss_ae1_train / counter)\n",
    "    train_losses['loss_ae2'].append(loss_ae2_train / counter)\n",
    "    train_losses['loss_dyn'].append(loss_dyn_train / counter)\n",
    "    train_losses['loss_total'].append(epoch_train_loss / counter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_ae1_test = 0\n",
    "        loss_ae2_test = 0\n",
    "        loss_dyn_test = 0\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        encoder.eval()\n",
    "        dynamics.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        counter = 0\n",
    "        for i, (x_t, x_tau) in enumerate(test_loader,0):\n",
    "            # Forward pass\n",
    "            z_t = encoder(x_t)\n",
    "            x_t_pred = decoder(z_t)\n",
    "\n",
    "            z_tau_pred = dynamics(z_t)\n",
    "            z_tau = encoder(x_tau)\n",
    "            x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_ae1 = criterion(x_t, x_t_pred)\n",
    "            loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "            loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "            loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "            epoch_test_loss += loss_total.item()\n",
    "\n",
    "            loss_ae1_test += loss_ae1.item()\n",
    "            loss_ae2_test += loss_ae2.item()\n",
    "            loss_dyn_test += loss_dyn.item() * warmup\n",
    "            counter += 1\n",
    "\n",
    "        test_losses['loss_ae1'].append(loss_ae1_test / counter)\n",
    "        test_losses['loss_ae2'].append(loss_ae2_test / counter)\n",
    "        test_losses['loss_dyn'].append(loss_dyn_test / counter)\n",
    "        test_losses['loss_total'].append(epoch_test_loss / counter)\n",
    "\n",
    "        if epoch >= patience and np.mean(test_losses['loss_total'][-patience:]) >= np.mean(test_losses['loss_total'][-patience:-1]):\n",
    "            break\n",
    "\n",
    "        if epoch >= config[\"warmup\"]:\n",
    "            scheduler.step(epoch_test_loss / counter)\n",
    "        \n",
    "    print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch, epoch_train_loss / counter, epoch_test_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(encoder, os.path.join(config[\"model_dir\"], \"encoder.pt\"))\n",
    "torch.save(dynamics, os.path.join(config[\"model_dir\"], \"dynamics.pt\"))\n",
    "torch.save(decoder, os.path.join(config[\"model_dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_dir doesn't exist, create it\n",
    "if not os.path.exists(config[\"log_dir\"]):\n",
    "    os.makedirs(config[\"log_dir\"])\n",
    "\n",
    "# Save the losses as a pickle file\n",
    "with open(os.path.join(config[\"log_dir\"], \"losses.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": train_losses, \"test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7a46472cb0ecb73bc1710bd9fdb3f2a0ed9ce65e1d30c1116a30b435b8e8314a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
