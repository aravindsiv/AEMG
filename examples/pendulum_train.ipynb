{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from AEMG.data_utils import DynamicsDataset\n",
    "from AEMG.systems.utils import get_system\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from AEMG.models import *\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for:  physics_pendulum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:30<00:00, 32.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# config_fname = \"config/pendulum_lqr_1K.txt\"\n",
    "config_fname = \"config/physics_pendulum.txt\"\n",
    "# config_fname = \"config/hopper.txt\"\n",
    "\n",
    "with open(config_fname, 'r') as f:\n",
    "    config = eval(f.read())\n",
    "\n",
    "dataset = DynamicsDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = Encoder(config[\"high_dims\"],config[\"low_dims\"])\n",
    "dynamics = LatentDynamics(config[\"low_dims\"])\n",
    "decoder  = Decoder(config[\"low_dims\"],config[\"high_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(set(list(encoder.parameters()) + list(dynamics.parameters()) + list(decoder.parameters())), \n",
    "    lr=config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.001, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6e22add13a941a09f39ebf05153d038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 5.799330146983266\n",
      "Epoch: 0, Iteration: 200, Loss: 3.116762340068817\n",
      "Epoch: 0, Iteration: 300, Loss: 2.7566737243905663\n",
      "Epoch: 0, Iteration: 400, Loss: 1.4036371521651745\n",
      "Epoch: 0, Iteration: 500, Loss: 1.2406577551737428\n",
      "Epoch: 0, Iteration: 600, Loss: 1.2367299851030111\n",
      "Epoch: 0, Iteration: 700, Loss: 1.181992569938302\n",
      "Epoch: 0, Iteration: 800, Loss: 1.0595833156257868\n",
      "Epoch: 0, Train Loss: 0.08387403534385099, Test Loss: 0.0069988203728273855\n",
      "Epoch: 1, Iteration: 100, Loss: 0.643575509544462\n",
      "Epoch: 1, Iteration: 200, Loss: 0.600460265763104\n",
      "Epoch: 1, Iteration: 300, Loss: 0.5905587156303227\n",
      "Epoch: 1, Iteration: 400, Loss: 0.5720366602763534\n",
      "Epoch: 1, Iteration: 500, Loss: 0.5700590014457703\n",
      "Epoch: 1, Iteration: 600, Loss: 0.5494118130300194\n",
      "Epoch: 1, Iteration: 700, Loss: 0.5351232746616006\n",
      "Epoch: 1, Iteration: 800, Loss: 0.45590485306456685\n",
      "Epoch: 1, Train Loss: 0.021903899781082727, Test Loss: 0.0034672205313002547\n",
      "Epoch: 2, Iteration: 100, Loss: 0.30094134085811675\n",
      "Epoch: 2, Iteration: 200, Loss: 0.24910130014177412\n",
      "Epoch: 2, Iteration: 300, Loss: 0.20402371976524591\n",
      "Epoch: 2, Iteration: 400, Loss: 0.1847420867998153\n",
      "Epoch: 2, Iteration: 500, Loss: 0.1776610993547365\n",
      "Epoch: 2, Iteration: 600, Loss: 0.16605505801271647\n",
      "Epoch: 2, Iteration: 700, Loss: 0.15997264464385808\n",
      "Epoch: 2, Iteration: 800, Loss: 0.15593442972749472\n",
      "Epoch: 2, Train Loss: 0.007822010825073441, Test Loss: 0.0015076423493388309\n",
      "Epoch: 3, Iteration: 100, Loss: 0.14769813453312963\n",
      "Epoch: 3, Iteration: 200, Loss: 0.1476515321410261\n",
      "Epoch: 3, Iteration: 300, Loss: 0.1424713067826815\n",
      "Epoch: 3, Iteration: 400, Loss: 0.1376035389257595\n",
      "Epoch: 3, Iteration: 500, Loss: 0.12736641586525366\n",
      "Epoch: 3, Iteration: 600, Loss: 0.1216434573289007\n",
      "Epoch: 3, Iteration: 700, Loss: 0.11436635174322873\n",
      "Epoch: 3, Iteration: 800, Loss: 0.10927390999859199\n",
      "Epoch: 3, Train Loss: 0.005141540396731075, Test Loss: 0.001022362686878657\n",
      "Epoch: 4, Iteration: 100, Loss: 0.10416936257388443\n",
      "Epoch: 4, Iteration: 200, Loss: 0.09804104769136757\n",
      "Epoch: 4, Iteration: 300, Loss: 0.09185565490042791\n",
      "Epoch: 4, Iteration: 400, Loss: 0.09301088401116431\n",
      "Epoch: 4, Iteration: 500, Loss: 0.08516463136766106\n",
      "Epoch: 4, Iteration: 600, Loss: 0.07966162089724094\n",
      "Epoch: 4, Iteration: 700, Loss: 0.07511082076234743\n",
      "Epoch: 4, Iteration: 800, Loss: 0.07015559094725177\n",
      "Epoch: 4, Train Loss: 0.003409386927168391, Test Loss: 0.0006452331150144118\n",
      "Epoch: 5, Iteration: 100, Loss: 0.0623273377714213\n",
      "Epoch: 5, Iteration: 200, Loss: 0.06100283656269312\n",
      "Epoch: 5, Iteration: 300, Loss: 0.05943586822832003\n",
      "Epoch: 5, Iteration: 400, Loss: 0.057635569624835625\n",
      "Epoch: 5, Iteration: 500, Loss: 0.05473802544292994\n",
      "Epoch: 5, Iteration: 600, Loss: 0.054777357348939404\n",
      "Epoch: 5, Iteration: 700, Loss: 0.0543193737976253\n",
      "Epoch: 5, Iteration: 800, Loss: 0.052586249221349135\n",
      "Epoch: 5, Train Loss: 0.0022631274298637186, Test Loss: 0.0005181078498993817\n",
      "Epoch: 6, Iteration: 100, Loss: 0.052268061772338115\n",
      "Epoch: 6, Iteration: 200, Loss: 0.04938180369208567\n",
      "Epoch: 6, Iteration: 300, Loss: 0.04897835454903543\n",
      "Epoch: 6, Iteration: 400, Loss: 0.049897492135642096\n",
      "Epoch: 6, Iteration: 500, Loss: 0.04942166924593039\n",
      "Epoch: 6, Iteration: 600, Loss: 0.050583398377057165\n",
      "Epoch: 6, Iteration: 700, Loss: 0.04729146181489341\n",
      "Epoch: 6, Iteration: 800, Loss: 0.046072725643171\n",
      "Epoch: 6, Train Loss: 0.0019632654451817214, Test Loss: 0.0004767503082533218\n",
      "Epoch: 7, Iteration: 100, Loss: 0.04649851185968146\n",
      "Epoch: 7, Iteration: 200, Loss: 0.04484685786883347\n",
      "Epoch: 7, Iteration: 300, Loss: 0.04515419440576807\n",
      "Epoch: 7, Iteration: 400, Loss: 0.04527715234144125\n",
      "Epoch: 7, Iteration: 500, Loss: 0.045498591469367966\n",
      "Epoch: 7, Iteration: 600, Loss: 0.04417735571041703\n",
      "Epoch: 7, Iteration: 700, Loss: 0.0446944210852962\n",
      "Epoch: 7, Iteration: 800, Loss: 0.043554380972636864\n",
      "Epoch: 7, Train Loss: 0.0017962990333055056, Test Loss: 0.00042398924050006387\n",
      "Epoch: 8, Iteration: 100, Loss: 0.044077909988118336\n",
      "Epoch: 8, Iteration: 200, Loss: 0.043794688885100186\n",
      "Epoch: 8, Iteration: 300, Loss: 0.041731004224857315\n",
      "Epoch: 8, Iteration: 400, Loss: 0.042231105209793895\n",
      "Epoch: 8, Iteration: 500, Loss: 0.040184962505009025\n",
      "Epoch: 8, Iteration: 600, Loss: 0.04146656268858351\n",
      "Epoch: 8, Iteration: 700, Loss: 0.04198403569171205\n",
      "Epoch: 8, Iteration: 800, Loss: 0.039719686610624194\n",
      "Epoch: 8, Train Loss: 0.0016668603373329966, Test Loss: 0.00039607248086690665\n",
      "Epoch: 9, Iteration: 100, Loss: 0.03908214796683751\n",
      "Epoch: 9, Iteration: 200, Loss: 0.03969771999982186\n",
      "Epoch: 9, Iteration: 300, Loss: 0.040188666695030406\n",
      "Epoch: 9, Iteration: 400, Loss: 0.039155820006271824\n",
      "Epoch: 9, Iteration: 500, Loss: 0.03733666450716555\n",
      "Epoch: 9, Iteration: 600, Loss: 0.03886536951176822\n",
      "Epoch: 9, Iteration: 700, Loss: 0.03772807976929471\n",
      "Epoch: 9, Iteration: 800, Loss: 0.03905317361932248\n",
      "Epoch: 9, Train Loss: 0.001544328840817835, Test Loss: 0.0003670399216376883\n",
      "Epoch: 10, Iteration: 100, Loss: 0.03760943283850793\n",
      "Epoch: 10, Iteration: 200, Loss: 0.03789053352375049\n",
      "Epoch: 10, Iteration: 300, Loss: 0.037090656434884295\n",
      "Epoch: 10, Iteration: 400, Loss: 0.033925764364539646\n",
      "Epoch: 10, Iteration: 500, Loss: 0.03602018720994238\n",
      "Epoch: 10, Iteration: 600, Loss: 0.03750539373140782\n",
      "Epoch: 10, Iteration: 700, Loss: 0.03571432898752391\n",
      "Epoch: 10, Iteration: 800, Loss: 0.03404879705340136\n",
      "Epoch: 10, Train Loss: 0.001442949965885377, Test Loss: 0.00034953573079180125\n",
      "Epoch: 11, Iteration: 100, Loss: 0.036554595120833255\n",
      "Epoch: 11, Iteration: 200, Loss: 0.033628175166086294\n",
      "Epoch: 11, Iteration: 300, Loss: 0.03382742950634565\n",
      "Epoch: 11, Iteration: 400, Loss: 0.03343134201713838\n",
      "Epoch: 11, Iteration: 500, Loss: 0.03204552760871593\n",
      "Epoch: 11, Iteration: 600, Loss: 0.035799218487227336\n",
      "Epoch: 11, Iteration: 700, Loss: 0.03501951164798811\n",
      "Epoch: 11, Iteration: 800, Loss: 0.03334577514033299\n",
      "Epoch: 11, Train Loss: 0.0013646481946864501, Test Loss: 0.00032154018655802086\n",
      "Epoch: 12, Iteration: 100, Loss: 0.03254260966787115\n",
      "Epoch: 12, Iteration: 200, Loss: 0.03422942514589522\n",
      "Epoch: 12, Iteration: 300, Loss: 0.03299459314439446\n",
      "Epoch: 12, Iteration: 400, Loss: 0.03252664947649464\n",
      "Epoch: 12, Iteration: 500, Loss: 0.032183730756514706\n",
      "Epoch: 12, Iteration: 600, Loss: 0.03267081209924072\n",
      "Epoch: 12, Iteration: 700, Loss: 0.03081722701608669\n",
      "Epoch: 12, Iteration: 800, Loss: 0.031189985253149644\n",
      "Epoch: 12, Train Loss: 0.0012979028201885266, Test Loss: 0.00030442420479609764\n",
      "Epoch: 13, Iteration: 100, Loss: 0.03310526703717187\n",
      "Epoch: 13, Iteration: 200, Loss: 0.03089845371141564\n",
      "Epoch: 13, Iteration: 300, Loss: 0.03054278073250316\n",
      "Epoch: 13, Iteration: 400, Loss: 0.03146710911823902\n",
      "Epoch: 13, Iteration: 500, Loss: 0.03285042371135205\n",
      "Epoch: 13, Iteration: 600, Loss: 0.030726181328645907\n",
      "Epoch: 13, Iteration: 700, Loss: 0.03091787768062204\n",
      "Epoch: 13, Iteration: 800, Loss: 0.03164504063897766\n",
      "Epoch: 13, Train Loss: 0.0012542912488022913, Test Loss: 0.00030000437663434305\n",
      "Epoch: 14, Iteration: 100, Loss: 0.03039723337860778\n",
      "Epoch: 14, Iteration: 200, Loss: 0.031117728489334695\n",
      "Epoch: 14, Iteration: 300, Loss: 0.031296143803047016\n",
      "Epoch: 14, Iteration: 400, Loss: 0.031079955952009186\n",
      "Epoch: 14, Iteration: 500, Loss: 0.030580669874325395\n",
      "Epoch: 14, Iteration: 600, Loss: 0.0322081278427504\n",
      "Epoch: 14, Iteration: 700, Loss: 0.030264473010902293\n",
      "Epoch: 14, Iteration: 800, Loss: 0.028389124272507615\n",
      "Epoch: 14, Train Loss: 0.0012218246174514821, Test Loss: 0.00029530418921393855\n",
      "Epoch: 15, Iteration: 100, Loss: 0.029806190461385995\n",
      "Epoch: 15, Iteration: 200, Loss: 0.030241829706938006\n",
      "Epoch: 15, Iteration: 300, Loss: 0.029546577730798163\n",
      "Epoch: 15, Iteration: 400, Loss: 0.03183654070016928\n",
      "Epoch: 15, Iteration: 500, Loss: 0.031281746487366036\n",
      "Epoch: 15, Iteration: 600, Loss: 0.029234488189104013\n",
      "Epoch: 15, Iteration: 700, Loss: 0.02854992232460063\n",
      "Epoch: 15, Iteration: 800, Loss: 0.030459843736025505\n",
      "Epoch: 15, Train Loss: 0.0011977043225973223, Test Loss: 0.00028594718313378336\n",
      "Epoch: 16, Iteration: 100, Loss: 0.029737233562627807\n",
      "Epoch: 16, Iteration: 200, Loss: 0.02969408490753267\n",
      "Epoch: 16, Iteration: 300, Loss: 0.029861537463148125\n",
      "Epoch: 16, Iteration: 400, Loss: 0.028228334616869688\n",
      "Epoch: 16, Iteration: 500, Loss: 0.031093364115804434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Iteration: 600, Loss: 0.03025722385791596\n",
      "Epoch: 16, Iteration: 700, Loss: 0.029314720261027105\n",
      "Epoch: 16, Iteration: 800, Loss: 0.028882417202112265\n",
      "Epoch: 16, Train Loss: 0.0011743670542414522, Test Loss: 0.00029051863979367477\n",
      "Epoch: 17, Iteration: 100, Loss: 0.030886536871548742\n",
      "Epoch: 17, Iteration: 200, Loss: 0.028583197825355455\n",
      "Epoch: 17, Iteration: 300, Loss: 0.03033670663717203\n",
      "Epoch: 17, Iteration: 400, Loss: 0.02642940721125342\n",
      "Epoch: 17, Iteration: 500, Loss: 0.027079086561570875\n",
      "Epoch: 17, Iteration: 600, Loss: 0.028912921610753983\n",
      "Epoch: 17, Iteration: 700, Loss: 0.03139105210721027\n",
      "Epoch: 17, Iteration: 800, Loss: 0.026880713805439882\n",
      "Epoch: 17, Train Loss: 0.0011608554329404174, Test Loss: 0.0002864723480662722\n",
      "Epoch: 18, Iteration: 100, Loss: 0.0284871712065069\n",
      "Epoch: 18, Iteration: 200, Loss: 0.028279514619498514\n",
      "Epoch: 18, Iteration: 300, Loss: 0.028185363349621184\n",
      "Epoch: 18, Iteration: 400, Loss: 0.02975408366182819\n",
      "Epoch: 18, Iteration: 500, Loss: 0.02847946270776447\n",
      "Epoch: 18, Iteration: 600, Loss: 0.029866248049074784\n",
      "Epoch: 18, Iteration: 700, Loss: 0.02841643574356567\n",
      "Epoch: 18, Iteration: 800, Loss: 0.027865326919709332\n",
      "Epoch: 18, Train Loss: 0.0011396077601079611, Test Loss: 0.0002765211455585631\n",
      "Epoch: 19, Iteration: 100, Loss: 0.027719604768208228\n",
      "Epoch: 19, Iteration: 200, Loss: 0.027515258399944287\n",
      "Epoch: 19, Iteration: 300, Loss: 0.027738309901906177\n",
      "Epoch: 19, Iteration: 400, Loss: 0.028072427987353876\n",
      "Epoch: 19, Iteration: 500, Loss: 0.029674204211914912\n",
      "Epoch: 19, Iteration: 600, Loss: 0.02739226128323935\n",
      "Epoch: 19, Iteration: 700, Loss: 0.027711148344678804\n",
      "Epoch: 19, Iteration: 800, Loss: 0.029395310702966526\n",
      "Epoch: 19, Train Loss: 0.0011274703888562252, Test Loss: 0.0002722792651267195\n",
      "Epoch: 20, Iteration: 100, Loss: 0.027311332640238106\n",
      "Epoch: 20, Iteration: 200, Loss: 0.02913151602842845\n",
      "Epoch: 20, Iteration: 300, Loss: 0.02754322458349634\n",
      "Epoch: 20, Iteration: 400, Loss: 0.028213590281666256\n",
      "Epoch: 20, Iteration: 500, Loss: 0.02750931226182729\n",
      "Epoch: 20, Iteration: 600, Loss: 0.026905588048975915\n",
      "Epoch: 20, Iteration: 700, Loss: 0.030051944311708212\n",
      "Epoch: 20, Iteration: 800, Loss: 0.027816813060780987\n",
      "Epoch: 20, Train Loss: 0.0011165739418768652, Test Loss: 0.00026873182056186783\n",
      "Epoch: 21, Iteration: 100, Loss: 0.026783397755934857\n",
      "Epoch: 21, Iteration: 200, Loss: 0.027679115009959787\n",
      "Epoch: 21, Iteration: 300, Loss: 0.027283253744826652\n",
      "Epoch: 21, Iteration: 400, Loss: 0.027990310481982306\n",
      "Epoch: 21, Iteration: 500, Loss: 0.027281346585368738\n",
      "Epoch: 21, Iteration: 600, Loss: 0.02777055873593781\n",
      "Epoch: 21, Iteration: 700, Loss: 0.027302009832055774\n",
      "Epoch: 21, Iteration: 800, Loss: 0.02715841455210466\n",
      "Epoch: 21, Train Loss: 0.00109506084207742, Test Loss: 0.00026424083220570447\n",
      "Epoch: 22, Iteration: 100, Loss: 0.027796369162388146\n",
      "Epoch: 22, Iteration: 200, Loss: 0.02742793707147939\n",
      "Epoch: 22, Iteration: 300, Loss: 0.028779742351616733\n",
      "Epoch: 22, Iteration: 400, Loss: 0.026457832485903054\n",
      "Epoch: 22, Iteration: 500, Loss: 0.02651660962146707\n",
      "Epoch: 22, Iteration: 600, Loss: 0.026221951651677955\n",
      "Epoch: 22, Iteration: 700, Loss: 0.027024034046917222\n",
      "Epoch: 22, Iteration: 800, Loss: 0.02674957517592702\n",
      "Epoch: 22, Train Loss: 0.0010805308595421984, Test Loss: 0.0002668407502064277\n",
      "Epoch: 23, Iteration: 100, Loss: 0.02720548422075808\n",
      "Epoch: 23, Iteration: 200, Loss: 0.02813875293941237\n",
      "Epoch: 23, Iteration: 300, Loss: 0.026648659630154725\n",
      "Epoch: 23, Iteration: 400, Loss: 0.02631031919736415\n",
      "Epoch: 23, Iteration: 500, Loss: 0.0268078734370647\n",
      "Epoch: 23, Iteration: 600, Loss: 0.02591588541690726\n",
      "Epoch: 23, Iteration: 700, Loss: 0.027024311231798492\n",
      "Epoch: 23, Iteration: 800, Loss: 0.025019234642968513\n",
      "Epoch: 23, Train Loss: 0.0010625942739178043, Test Loss: 0.0002512088150530735\n",
      "Epoch: 24, Iteration: 100, Loss: 0.027221871830988675\n",
      "Epoch: 24, Iteration: 200, Loss: 0.02536709416017402\n",
      "Epoch: 24, Iteration: 300, Loss: 0.026323693280573934\n",
      "Epoch: 24, Iteration: 400, Loss: 0.02693007665220648\n",
      "Epoch: 24, Iteration: 500, Loss: 0.027504079160280526\n",
      "Epoch: 24, Iteration: 600, Loss: 0.026573897266644053\n",
      "Epoch: 24, Iteration: 700, Loss: 0.02607483662723098\n",
      "Epoch: 24, Iteration: 800, Loss: 0.024747009345446713\n",
      "Epoch: 24, Train Loss: 0.001046620109214205, Test Loss: 0.0002744639072153658\n",
      "Epoch: 25, Iteration: 100, Loss: 0.0258093523880234\n",
      "Epoch: 25, Iteration: 200, Loss: 0.026116814711713232\n",
      "Epoch: 25, Iteration: 300, Loss: 0.025465921193244867\n",
      "Epoch: 25, Iteration: 400, Loss: 0.027739831813960336\n",
      "Epoch: 25, Iteration: 500, Loss: 0.024429250741377473\n",
      "Epoch: 25, Iteration: 600, Loss: 0.024572618247475475\n",
      "Epoch: 25, Iteration: 700, Loss: 0.025274730505771004\n",
      "Epoch: 25, Iteration: 800, Loss: 0.024842909158905968\n",
      "Epoch: 25, Train Loss: 0.0010200217167209498, Test Loss: 0.00024354017480381779\n",
      "Epoch: 26, Iteration: 100, Loss: 0.02416780687053688\n",
      "Epoch: 26, Iteration: 200, Loss: 0.024985247669974342\n",
      "Epoch: 26, Iteration: 300, Loss: 0.025093720978475176\n",
      "Epoch: 26, Iteration: 400, Loss: 0.023701436097326223\n",
      "Epoch: 26, Iteration: 500, Loss: 0.02429552550893277\n",
      "Epoch: 26, Iteration: 600, Loss: 0.025927987247996498\n",
      "Epoch: 26, Iteration: 700, Loss: 0.0242046387938899\n",
      "Epoch: 26, Iteration: 800, Loss: 0.023345399582467508\n",
      "Epoch: 26, Train Loss: 0.0009759113442532285, Test Loss: 0.00023614486839439397\n",
      "Epoch: 27, Iteration: 100, Loss: 0.02425235402188264\n",
      "Epoch: 27, Iteration: 200, Loss: 0.023835560932639055\n",
      "Epoch: 27, Iteration: 300, Loss: 0.024209882445575204\n",
      "Epoch: 27, Iteration: 400, Loss: 0.02279309894220205\n",
      "Epoch: 27, Iteration: 500, Loss: 0.022359519578458276\n",
      "Epoch: 27, Iteration: 600, Loss: 0.024760695727309212\n",
      "Epoch: 27, Iteration: 700, Loss: 0.021349586866563186\n",
      "Epoch: 27, Iteration: 800, Loss: 0.024337765498785302\n",
      "Epoch: 27, Train Loss: 0.0009319023638163676, Test Loss: 0.00023065783479729387\n",
      "Epoch: 28, Iteration: 100, Loss: 0.024356410693144426\n",
      "Epoch: 28, Iteration: 200, Loss: 0.023189070430817083\n",
      "Epoch: 28, Iteration: 300, Loss: 0.02160767530585872\n",
      "Epoch: 28, Iteration: 400, Loss: 0.02216913428128464\n",
      "Epoch: 28, Iteration: 500, Loss: 0.021820579888299108\n",
      "Epoch: 28, Iteration: 600, Loss: 0.02237064656219445\n",
      "Epoch: 28, Iteration: 700, Loss: 0.02251010610780213\n",
      "Epoch: 28, Iteration: 800, Loss: 0.023142623918829486\n",
      "Epoch: 28, Train Loss: 0.0009093304092546109, Test Loss: 0.0002124985881916834\n",
      "Epoch: 29, Iteration: 100, Loss: 0.020628064434276894\n",
      "Epoch: 29, Iteration: 200, Loss: 0.022483926069980953\n",
      "Epoch: 29, Iteration: 300, Loss: 0.020792253599211108\n",
      "Epoch: 29, Iteration: 400, Loss: 0.023313435005547944\n",
      "Epoch: 29, Iteration: 500, Loss: 0.020541759397019632\n",
      "Epoch: 29, Iteration: 600, Loss: 0.02427588855061913\n",
      "Epoch: 29, Iteration: 700, Loss: 0.023157202027505264\n",
      "Epoch: 29, Iteration: 800, Loss: 0.019979238939413335\n",
      "Epoch: 29, Train Loss: 0.0008746453812856176, Test Loss: 0.00021328996998108032\n",
      "Epoch: 30, Iteration: 100, Loss: 0.021137714400538243\n",
      "Epoch: 30, Iteration: 200, Loss: 0.024426506155577954\n",
      "Epoch: 30, Iteration: 300, Loss: 0.02207212217763299\n",
      "Epoch: 30, Iteration: 400, Loss: 0.020652009377954528\n",
      "Epoch: 30, Iteration: 500, Loss: 0.02029717565892497\n",
      "Epoch: 30, Iteration: 600, Loss: 0.019849087882903405\n",
      "Epoch: 30, Iteration: 700, Loss: 0.020324089637142606\n",
      "Epoch: 30, Iteration: 800, Loss: 0.02134116972592892\n",
      "Epoch: 30, Train Loss: 0.0008527927764332067, Test Loss: 0.00020572651077575396\n",
      "Epoch: 31, Iteration: 100, Loss: 0.02214625763008371\n",
      "Epoch: 31, Iteration: 200, Loss: 0.022533087481861003\n",
      "Epoch: 31, Iteration: 300, Loss: 0.021405516687082127\n",
      "Epoch: 31, Iteration: 400, Loss: 0.02015049803594593\n",
      "Epoch: 31, Iteration: 500, Loss: 0.02064912376954453\n",
      "Epoch: 31, Iteration: 600, Loss: 0.02034076398558682\n",
      "Epoch: 31, Iteration: 700, Loss: 0.02011050454893848\n",
      "Epoch: 31, Iteration: 800, Loss: 0.022094126034062356\n",
      "Epoch: 31, Train Loss: 0.0008455154936312545, Test Loss: 0.00020047934852395903\n",
      "Epoch: 32, Iteration: 100, Loss: 0.01963598671136424\n",
      "Epoch: 32, Iteration: 200, Loss: 0.020114925740926992\n",
      "Epoch: 32, Iteration: 300, Loss: 0.022246358086704277\n",
      "Epoch: 32, Iteration: 400, Loss: 0.020940602538757958\n",
      "Epoch: 32, Iteration: 500, Loss: 0.020436556209460832\n",
      "Epoch: 32, Iteration: 600, Loss: 0.021726426697568968\n",
      "Epoch: 32, Iteration: 700, Loss: 0.02130453896097606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Iteration: 800, Loss: 0.02058110455982387\n",
      "Epoch: 32, Train Loss: 0.000833499206341144, Test Loss: 0.0002077306089797865\n",
      "Epoch: 33, Iteration: 100, Loss: 0.019259098618931603\n",
      "Epoch: 33, Iteration: 200, Loss: 0.021226897813903634\n",
      "Epoch: 33, Iteration: 300, Loss: 0.01972089896298712\n",
      "Epoch: 33, Iteration: 400, Loss: 0.020209313413943164\n",
      "Epoch: 33, Iteration: 500, Loss: 0.021129453009052668\n",
      "Epoch: 33, Iteration: 600, Loss: 0.023470743748475797\n",
      "Epoch: 33, Iteration: 700, Loss: 0.019415783441218082\n",
      "Epoch: 33, Iteration: 800, Loss: 0.02139683380664792\n",
      "Epoch: 33, Train Loss: 0.0008261591215595629, Test Loss: 0.00019200016260965722\n",
      "Epoch: 34, Iteration: 100, Loss: 0.02187681778013939\n",
      "Epoch: 34, Iteration: 200, Loss: 0.020316156980697997\n",
      "Epoch: 34, Iteration: 300, Loss: 0.019875047641107813\n",
      "Epoch: 34, Iteration: 400, Loss: 0.02113784826360643\n",
      "Epoch: 34, Iteration: 500, Loss: 0.022089364611019846\n",
      "Epoch: 34, Iteration: 600, Loss: 0.019592848992033396\n",
      "Epoch: 34, Iteration: 700, Loss: 0.018204208274255507\n",
      "Epoch: 34, Iteration: 800, Loss: 0.020589398955053184\n",
      "Epoch: 34, Train Loss: 0.0008190514635893763, Test Loss: 0.00019263068413931758\n",
      "Epoch: 35, Iteration: 100, Loss: 0.020238102624716703\n",
      "Epoch: 35, Iteration: 200, Loss: 0.020121413625020068\n",
      "Epoch: 35, Iteration: 300, Loss: 0.021570577591774054\n",
      "Epoch: 35, Iteration: 400, Loss: 0.02162399241933599\n",
      "Epoch: 35, Iteration: 500, Loss: 0.01894357482524356\n",
      "Epoch: 35, Iteration: 600, Loss: 0.02000427484017564\n",
      "Epoch: 35, Iteration: 700, Loss: 0.020610902487533167\n",
      "Epoch: 35, Iteration: 800, Loss: 0.02018186802160926\n",
      "Epoch: 35, Train Loss: 0.000815558278764126, Test Loss: 0.0001983146970125753\n",
      "Epoch: 36, Iteration: 100, Loss: 0.0189007842054707\n",
      "Epoch: 36, Iteration: 200, Loss: 0.020122022782743443\n",
      "Epoch: 36, Iteration: 300, Loss: 0.0213050231905072\n",
      "Epoch: 36, Iteration: 400, Loss: 0.019995416601886973\n",
      "Epoch: 36, Iteration: 500, Loss: 0.020289287975174375\n",
      "Epoch: 36, Iteration: 600, Loss: 0.019742284239328\n",
      "Epoch: 36, Iteration: 700, Loss: 0.01972889986791415\n",
      "Epoch: 36, Iteration: 800, Loss: 0.020260694123862777\n",
      "Epoch: 36, Train Loss: 0.000805489753300407, Test Loss: 0.0001935694844713945\n",
      "Epoch: 37, Iteration: 100, Loss: 0.019904316795873456\n",
      "Epoch: 37, Iteration: 200, Loss: 0.020076658685866278\n",
      "Epoch: 37, Iteration: 300, Loss: 0.019485967517539393\n",
      "Epoch: 37, Iteration: 400, Loss: 0.01982799896359211\n",
      "Epoch: 37, Iteration: 500, Loss: 0.018304739336599596\n",
      "Epoch: 37, Iteration: 600, Loss: 0.020494704956945498\n",
      "Epoch: 37, Iteration: 700, Loss: 0.02111351807252504\n",
      "Epoch: 37, Iteration: 800, Loss: 0.01977146039280342\n",
      "Epoch: 37, Train Loss: 0.0007984473187751222, Test Loss: 0.00018803138458239101\n",
      "Epoch: 38, Iteration: 100, Loss: 0.022685098374495283\n",
      "Epoch: 38, Iteration: 200, Loss: 0.019108029315248132\n",
      "Epoch: 38, Iteration: 300, Loss: 0.019947414883063175\n",
      "Epoch: 38, Iteration: 400, Loss: 0.019800238551397342\n",
      "Epoch: 38, Iteration: 500, Loss: 0.01869802421424538\n",
      "Epoch: 38, Iteration: 600, Loss: 0.02064708869875176\n",
      "Epoch: 38, Iteration: 700, Loss: 0.020758920887601562\n",
      "Epoch: 38, Iteration: 800, Loss: 0.019017457780137192\n",
      "Epoch: 38, Train Loss: 0.000799338630958488, Test Loss: 0.00018647803188611\n",
      "Epoch: 39, Iteration: 100, Loss: 0.019828820302791428\n",
      "Epoch: 39, Iteration: 200, Loss: 0.02002716122660786\n",
      "Epoch: 39, Iteration: 300, Loss: 0.018207549139333423\n",
      "Epoch: 39, Iteration: 400, Loss: 0.019016057798580732\n",
      "Epoch: 39, Iteration: 500, Loss: 0.01993246250640368\n",
      "Epoch: 39, Iteration: 600, Loss: 0.02050275349029107\n",
      "Epoch: 39, Iteration: 700, Loss: 0.02077951959654456\n",
      "Epoch: 39, Iteration: 800, Loss: 0.01947161507268902\n",
      "Epoch: 39, Train Loss: 0.0007901101075253687, Test Loss: 0.00020318964130662968\n",
      "Epoch: 40, Iteration: 100, Loss: 0.018980023938638624\n",
      "Epoch: 40, Iteration: 200, Loss: 0.02195948538428638\n",
      "Epoch: 40, Iteration: 300, Loss: 0.019559924978238996\n",
      "Epoch: 40, Iteration: 400, Loss: 0.018731420022959355\n",
      "Epoch: 40, Iteration: 500, Loss: 0.020889502920908853\n",
      "Epoch: 40, Iteration: 600, Loss: 0.018467243244231213\n",
      "Epoch: 40, Iteration: 700, Loss: 0.020436353064724244\n",
      "Epoch: 40, Iteration: 800, Loss: 0.019531443089363165\n",
      "Epoch: 40, Train Loss: 0.0007865000922168013, Test Loss: 0.00018354283121348568\n",
      "Epoch: 41, Iteration: 100, Loss: 0.017828022544563282\n",
      "Epoch: 41, Iteration: 200, Loss: 0.019397327603655867\n",
      "Epoch: 41, Iteration: 300, Loss: 0.01951212157291593\n",
      "Epoch: 41, Iteration: 400, Loss: 0.019693068308697548\n",
      "Epoch: 41, Iteration: 500, Loss: 0.01954945492616389\n",
      "Epoch: 41, Iteration: 600, Loss: 0.01872826253384119\n",
      "Epoch: 41, Iteration: 700, Loss: 0.020293669287639204\n",
      "Epoch: 41, Iteration: 800, Loss: 0.01921078209852567\n",
      "Epoch: 41, Train Loss: 0.000784667745915924, Test Loss: 0.00019504517153624156\n",
      "Epoch: 42, Iteration: 100, Loss: 0.020828920940402895\n",
      "Epoch: 42, Iteration: 200, Loss: 0.019010653217264917\n",
      "Epoch: 42, Iteration: 300, Loss: 0.0184384590756963\n",
      "Epoch: 42, Iteration: 400, Loss: 0.018393400649074465\n",
      "Epoch: 42, Iteration: 500, Loss: 0.021075568816740997\n",
      "Epoch: 42, Iteration: 600, Loss: 0.0191636604940868\n",
      "Epoch: 42, Iteration: 700, Loss: 0.020198827813146636\n",
      "Epoch: 42, Iteration: 800, Loss: 0.01925368920637993\n",
      "Epoch: 42, Train Loss: 0.0007788107556556182, Test Loss: 0.000185843882477602\n",
      "Epoch: 43, Iteration: 100, Loss: 0.019810767757007852\n",
      "Epoch: 43, Iteration: 200, Loss: 0.019964481129136402\n",
      "Epoch: 43, Iteration: 300, Loss: 0.019404183578444645\n",
      "Epoch: 43, Iteration: 400, Loss: 0.018111778255843092\n",
      "Epoch: 43, Iteration: 500, Loss: 0.020687318887212314\n",
      "Epoch: 43, Iteration: 600, Loss: 0.019053310075832997\n",
      "Epoch: 43, Iteration: 700, Loss: 0.017864387518784497\n",
      "Epoch: 43, Iteration: 800, Loss: 0.02135736693162471\n",
      "Epoch: 43, Train Loss: 0.0007753152553814454, Test Loss: 0.0001814121794546678\n",
      "Epoch: 44, Iteration: 100, Loss: 0.019976449715613853\n",
      "Epoch: 44, Iteration: 200, Loss: 0.01885198063973803\n",
      "Epoch: 44, Iteration: 300, Loss: 0.019529128505382687\n",
      "Epoch: 44, Iteration: 400, Loss: 0.018692201600060798\n",
      "Epoch: 44, Iteration: 500, Loss: 0.02066240585554624\n",
      "Epoch: 44, Iteration: 600, Loss: 0.019761323288548738\n",
      "Epoch: 44, Iteration: 700, Loss: 0.019440008036326617\n",
      "Epoch: 44, Iteration: 800, Loss: 0.01776143479946768\n",
      "Epoch: 44, Train Loss: 0.0007717111436652815, Test Loss: 0.0001888177320735899\n",
      "Epoch: 45, Iteration: 100, Loss: 0.018699967353313696\n",
      "Epoch: 45, Iteration: 200, Loss: 0.018965023453347385\n",
      "Epoch: 45, Iteration: 300, Loss: 0.019603169748734217\n",
      "Epoch: 45, Iteration: 400, Loss: 0.018216582880995702\n",
      "Epoch: 45, Iteration: 500, Loss: 0.017365518993756268\n",
      "Epoch: 45, Iteration: 600, Loss: 0.019937371092964895\n",
      "Epoch: 45, Iteration: 700, Loss: 0.019545427363482304\n",
      "Epoch: 45, Iteration: 800, Loss: 0.021194628716330044\n",
      "Epoch: 45, Train Loss: 0.0007672011132642295, Test Loss: 0.00018500240183287603\n",
      "Epoch: 46, Iteration: 100, Loss: 0.01799488080723677\n",
      "Epoch: 46, Iteration: 200, Loss: 0.018525490326283034\n",
      "Epoch: 46, Iteration: 300, Loss: 0.02057746474747546\n",
      "Epoch: 46, Iteration: 400, Loss: 0.019197947134671267\n",
      "Epoch: 46, Iteration: 500, Loss: 0.017713871980959084\n",
      "Epoch: 46, Iteration: 600, Loss: 0.018631095830642153\n",
      "Epoch: 46, Iteration: 700, Loss: 0.020203738509735558\n",
      "Epoch: 46, Iteration: 800, Loss: 0.019082457445620093\n",
      "Epoch: 46, Train Loss: 0.000756447441769885, Test Loss: 0.00017540165778413228\n",
      "Epoch: 47, Iteration: 100, Loss: 0.019555363091058098\n",
      "Epoch: 47, Iteration: 200, Loss: 0.017421177952201106\n",
      "Epoch: 47, Iteration: 300, Loss: 0.01767978129646508\n",
      "Epoch: 47, Iteration: 400, Loss: 0.019771729697822593\n",
      "Epoch: 47, Iteration: 500, Loss: 0.019388773915125057\n",
      "Epoch: 47, Iteration: 600, Loss: 0.020105887357203756\n",
      "Epoch: 47, Iteration: 700, Loss: 0.01914936615503393\n",
      "Epoch: 47, Iteration: 800, Loss: 0.018136401209631003\n",
      "Epoch: 47, Train Loss: 0.0007558867148470269, Test Loss: 0.00017969187047544094\n",
      "Epoch: 48, Iteration: 100, Loss: 0.019652461960504297\n",
      "Epoch: 48, Iteration: 200, Loss: 0.018400731365545653\n",
      "Epoch: 48, Iteration: 300, Loss: 0.01853895308158826\n",
      "Epoch: 48, Iteration: 400, Loss: 0.01854993059532717\n",
      "Epoch: 48, Iteration: 500, Loss: 0.018912536062998697\n",
      "Epoch: 48, Iteration: 600, Loss: 0.01870070862787543\n",
      "Epoch: 48, Iteration: 700, Loss: 0.017596800898900256\n",
      "Epoch: 48, Iteration: 800, Loss: 0.019644230218546\n",
      "Epoch: 48, Train Loss: 0.0007479801170618835, Test Loss: 0.00017470571084360519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Iteration: 100, Loss: 0.018628460325999185\n",
      "Epoch: 49, Iteration: 200, Loss: 0.01767322904197499\n",
      "Epoch: 49, Iteration: 300, Loss: 0.0185088714279118\n",
      "Epoch: 49, Iteration: 400, Loss: 0.018419019666907843\n",
      "Epoch: 49, Iteration: 500, Loss: 0.016159952552698087\n",
      "Epoch: 49, Iteration: 600, Loss: 0.02030187139462214\n",
      "Epoch: 49, Iteration: 700, Loss: 0.019512542436132208\n",
      "Epoch: 49, Iteration: 800, Loss: 0.02038514856394613\n",
      "Epoch: 49, Train Loss: 0.0007483908826443813, Test Loss: 0.00018072506084216368\n",
      "Epoch: 50, Iteration: 100, Loss: 0.020223633153364062\n",
      "Epoch: 50, Iteration: 200, Loss: 0.017343435596558265\n",
      "Epoch: 50, Iteration: 300, Loss: 0.01868723022926133\n",
      "Epoch: 50, Iteration: 400, Loss: 0.018490725829906296\n",
      "Epoch: 50, Iteration: 500, Loss: 0.020391230187669862\n",
      "Epoch: 50, Iteration: 600, Loss: 0.01670876242860686\n",
      "Epoch: 50, Iteration: 700, Loss: 0.018780280035571195\n",
      "Epoch: 50, Iteration: 800, Loss: 0.018485749707906507\n",
      "Epoch: 50, Train Loss: 0.0007433769343031218, Test Loss: 0.0001765159536158394\n",
      "Epoch: 51, Iteration: 100, Loss: 0.017077601471100934\n",
      "Epoch: 51, Iteration: 200, Loss: 0.01739871814061189\n",
      "Epoch: 51, Iteration: 300, Loss: 0.017972229448787402\n",
      "Epoch: 51, Iteration: 400, Loss: 0.018075811523885932\n",
      "Epoch: 51, Iteration: 500, Loss: 0.019104886530840304\n",
      "Epoch: 51, Iteration: 600, Loss: 0.0197629775502719\n",
      "Epoch: 51, Iteration: 700, Loss: 0.01827645904268138\n",
      "Epoch: 51, Iteration: 800, Loss: 0.01933281700621592\n",
      "Epoch: 51, Train Loss: 0.0007367388156059807, Test Loss: 0.0001755818668750251\n",
      "Epoch: 52, Iteration: 100, Loss: 0.019643515261122957\n",
      "Epoch: 52, Iteration: 200, Loss: 0.018768030611681752\n",
      "Epoch: 52, Iteration: 300, Loss: 0.017688167019514367\n",
      "Epoch: 52, Iteration: 400, Loss: 0.017424155797925778\n",
      "Epoch: 52, Iteration: 500, Loss: 0.019778947054874152\n",
      "Epoch: 52, Iteration: 600, Loss: 0.017964441889489535\n",
      "Epoch: 52, Iteration: 700, Loss: 0.017885609486256726\n",
      "Epoch: 52, Iteration: 800, Loss: 0.017593320764717646\n",
      "Epoch: 52, Train Loss: 0.000734405503410274, Test Loss: 0.00017589424808826467\n",
      "Epoch: 53, Iteration: 100, Loss: 0.01658042037888663\n",
      "Epoch: 53, Iteration: 200, Loss: 0.019767971360124648\n",
      "Epoch: 53, Iteration: 300, Loss: 0.021197958711127285\n",
      "Epoch: 53, Iteration: 400, Loss: 0.018556846465799026\n",
      "Epoch: 53, Iteration: 500, Loss: 0.018747746289591305\n",
      "Epoch: 53, Iteration: 600, Loss: 0.01859630930266576\n",
      "Epoch: 53, Iteration: 700, Loss: 0.01551490269775968\n",
      "Epoch: 53, Iteration: 800, Loss: 0.017211652018886525\n",
      "Epoch: 53, Train Loss: 0.0007291996220847566, Test Loss: 0.0001737214807279038\n",
      "Epoch: 54, Iteration: 100, Loss: 0.017731653304508654\n",
      "Epoch: 54, Iteration: 200, Loss: 0.01903769238560926\n",
      "Epoch: 54, Iteration: 300, Loss: 0.0176802511268761\n",
      "Epoch: 54, Iteration: 400, Loss: 0.017364324448863044\n",
      "Epoch: 54, Iteration: 500, Loss: 0.018022821888735052\n",
      "Epoch: 54, Iteration: 600, Loss: 0.016836417649756186\n",
      "Epoch: 54, Iteration: 700, Loss: 0.018608950274938252\n",
      "Epoch: 54, Iteration: 800, Loss: 0.017617730096390005\n",
      "Epoch: 54, Train Loss: 0.0007221111440538061, Test Loss: 0.00018023909425089713\n",
      "Epoch: 55, Iteration: 100, Loss: 0.017456409048463684\n",
      "Epoch: 55, Iteration: 200, Loss: 0.01871486413438106\n",
      "Epoch: 55, Iteration: 300, Loss: 0.01951091201044619\n",
      "Epoch: 55, Iteration: 400, Loss: 0.018620860697410535\n",
      "Epoch: 55, Iteration: 500, Loss: 0.018182917730882764\n",
      "Epoch: 55, Iteration: 600, Loss: 0.017035482298524585\n",
      "Epoch: 55, Iteration: 700, Loss: 0.01907830981508596\n",
      "Epoch: 55, Iteration: 800, Loss: 0.016717492369934916\n",
      "Epoch: 55, Train Loss: 0.0007214434667834353, Test Loss: 0.00020128293179526297\n",
      "Epoch: 56, Iteration: 100, Loss: 0.01666363752883626\n",
      "Epoch: 56, Iteration: 200, Loss: 0.01878608416154748\n",
      "Epoch: 56, Iteration: 300, Loss: 0.01654948264331324\n",
      "Epoch: 56, Iteration: 400, Loss: 0.01861942163668573\n",
      "Epoch: 56, Iteration: 500, Loss: 0.01808271263143979\n",
      "Epoch: 56, Iteration: 600, Loss: 0.018929576013761107\n",
      "Epoch: 56, Iteration: 700, Loss: 0.017576511760125868\n",
      "Epoch: 56, Iteration: 800, Loss: 0.01878018114803126\n",
      "Epoch: 56, Train Loss: 0.0007153217950926192, Test Loss: 0.00017289951084592043\n",
      "Epoch: 57, Iteration: 100, Loss: 0.016976188533590175\n",
      "Epoch: 57, Iteration: 200, Loss: 0.01669880644476507\n",
      "Epoch: 57, Iteration: 300, Loss: 0.020109811332076788\n",
      "Epoch: 57, Iteration: 400, Loss: 0.015997625683667138\n",
      "Epoch: 57, Iteration: 500, Loss: 0.01868658290914027\n",
      "Epoch: 57, Iteration: 600, Loss: 0.01800729702517856\n",
      "Epoch: 57, Iteration: 700, Loss: 0.016826233615574893\n",
      "Epoch: 57, Iteration: 800, Loss: 0.018536676885560155\n",
      "Epoch: 57, Train Loss: 0.000709667258784834, Test Loss: 0.00016761121245608218\n",
      "Epoch: 58, Iteration: 100, Loss: 0.017979857228056062\n",
      "Epoch: 58, Iteration: 200, Loss: 0.0186008281816612\n",
      "Epoch: 58, Iteration: 300, Loss: 0.017570217059983406\n",
      "Epoch: 58, Iteration: 400, Loss: 0.01683751847303938\n",
      "Epoch: 58, Iteration: 500, Loss: 0.017951694440853316\n",
      "Epoch: 58, Iteration: 600, Loss: 0.017478046291216742\n",
      "Epoch: 58, Iteration: 700, Loss: 0.016413742836448364\n",
      "Epoch: 58, Iteration: 800, Loss: 0.017984178142796736\n",
      "Epoch: 58, Train Loss: 0.0007061155278503589, Test Loss: 0.00016725983968500014\n",
      "Epoch: 59, Iteration: 100, Loss: 0.016591300634900108\n",
      "Epoch: 59, Iteration: 200, Loss: 0.016545088270504493\n",
      "Epoch: 59, Iteration: 300, Loss: 0.018217746022855863\n",
      "Epoch: 59, Iteration: 400, Loss: 0.01741458100877935\n",
      "Epoch: 59, Iteration: 500, Loss: 0.01828230459796032\n",
      "Epoch: 59, Iteration: 600, Loss: 0.018149089453800116\n",
      "Epoch: 59, Iteration: 700, Loss: 0.017448444712499622\n",
      "Epoch: 59, Iteration: 800, Loss: 0.017538888278068043\n",
      "Epoch: 59, Train Loss: 0.0006967674675434785, Test Loss: 0.00017402322282993393\n",
      "Epoch: 60, Iteration: 100, Loss: 0.018515632800699677\n",
      "Epoch: 60, Iteration: 200, Loss: 0.01616655980615178\n",
      "Epoch: 60, Iteration: 300, Loss: 0.017298663937253878\n",
      "Epoch: 60, Iteration: 400, Loss: 0.015909584690234624\n",
      "Epoch: 60, Iteration: 500, Loss: 0.018951856647618115\n",
      "Epoch: 60, Iteration: 600, Loss: 0.017855936952400953\n",
      "Epoch: 60, Iteration: 700, Loss: 0.01664503831852926\n",
      "Epoch: 60, Iteration: 800, Loss: 0.018560734679340385\n",
      "Epoch: 60, Train Loss: 0.0006957913322794702, Test Loss: 0.00016529167613990554\n",
      "Epoch: 61, Iteration: 100, Loss: 0.018367556622251868\n",
      "Epoch: 61, Iteration: 200, Loss: 0.01705057727667736\n",
      "Epoch: 61, Iteration: 300, Loss: 0.016981980996206403\n",
      "Epoch: 61, Iteration: 400, Loss: 0.016009762068279088\n",
      "Epoch: 61, Iteration: 500, Loss: 0.01676897088327678\n",
      "Epoch: 61, Iteration: 600, Loss: 0.01717746066424297\n",
      "Epoch: 61, Iteration: 700, Loss: 0.017304109707765747\n",
      "Epoch: 61, Iteration: 800, Loss: 0.01763405786914518\n",
      "Epoch: 61, Train Loss: 0.0006843548885107542, Test Loss: 0.00017285577881896282\n",
      "Epoch: 62, Iteration: 100, Loss: 0.01581395513494499\n",
      "Epoch: 62, Iteration: 200, Loss: 0.01639211396104656\n",
      "Epoch: 62, Iteration: 300, Loss: 0.016752865893067792\n",
      "Epoch: 62, Iteration: 400, Loss: 0.01778488259151345\n",
      "Epoch: 62, Iteration: 500, Loss: 0.017023976564814802\n",
      "Epoch: 62, Iteration: 600, Loss: 0.01656848005950451\n",
      "Epoch: 62, Iteration: 700, Loss: 0.017011879754136316\n",
      "Epoch: 62, Iteration: 800, Loss: 0.01727646515792003\n",
      "Epoch: 62, Train Loss: 0.0006808890992274282, Test Loss: 0.0001646026081355612\n",
      "Epoch: 63, Iteration: 100, Loss: 0.016441097541246563\n",
      "Epoch: 63, Iteration: 200, Loss: 0.01640006488742074\n",
      "Epoch: 63, Iteration: 300, Loss: 0.016896349588932935\n",
      "Epoch: 63, Iteration: 400, Loss: 0.019064091364271007\n",
      "Epoch: 63, Iteration: 500, Loss: 0.017832425655797124\n",
      "Epoch: 63, Iteration: 600, Loss: 0.01599183206417365\n",
      "Epoch: 63, Iteration: 700, Loss: 0.01627266426658025\n",
      "Epoch: 63, Iteration: 800, Loss: 0.016566241523833014\n",
      "Epoch: 63, Train Loss: 0.0006756466406662779, Test Loss: 0.00016034418247511491\n",
      "Epoch: 64, Iteration: 100, Loss: 0.016980802050966304\n",
      "Epoch: 64, Iteration: 200, Loss: 0.016438083730463404\n",
      "Epoch: 64, Iteration: 300, Loss: 0.016047313205490354\n",
      "Epoch: 64, Iteration: 400, Loss: 0.017544886395626236\n",
      "Epoch: 64, Iteration: 500, Loss: 0.01686172869085567\n",
      "Epoch: 64, Iteration: 600, Loss: 0.016253471541858744\n",
      "Epoch: 64, Iteration: 700, Loss: 0.016479443234857172\n",
      "Epoch: 64, Iteration: 800, Loss: 0.01652263379219221\n",
      "Epoch: 64, Train Loss: 0.0006650534464638004, Test Loss: 0.00015877288271573086\n",
      "Epoch: 65, Iteration: 100, Loss: 0.016340535396011546\n",
      "Epoch: 65, Iteration: 200, Loss: 0.017721003059705254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Iteration: 300, Loss: 0.017165501005365513\n",
      "Epoch: 65, Iteration: 400, Loss: 0.015072130532644223\n",
      "Epoch: 65, Iteration: 500, Loss: 0.016628758690785617\n",
      "Epoch: 65, Iteration: 600, Loss: 0.01749813689093571\n",
      "Epoch: 65, Iteration: 700, Loss: 0.016196038341149688\n",
      "Epoch: 65, Iteration: 800, Loss: 0.01685696837375872\n",
      "Epoch: 65, Train Loss: 0.00066649339658306, Test Loss: 0.00016035198934507007\n",
      "Epoch: 66, Iteration: 100, Loss: 0.017576710219145752\n",
      "Epoch: 66, Iteration: 200, Loss: 0.015363815771706868\n",
      "Epoch: 66, Iteration: 300, Loss: 0.015159894872340374\n",
      "Epoch: 66, Iteration: 400, Loss: 0.016038665802625474\n",
      "Epoch: 66, Iteration: 500, Loss: 0.01768802206061082\n",
      "Epoch: 66, Iteration: 600, Loss: 0.01721964144235244\n",
      "Epoch: 66, Iteration: 700, Loss: 0.017881407591630705\n",
      "Epoch: 66, Iteration: 800, Loss: 0.015834810750675388\n",
      "Epoch: 66, Train Loss: 0.0006631053846619691, Test Loss: 0.00015522531957602655\n",
      "Epoch: 67, Iteration: 100, Loss: 0.017456575609685387\n",
      "Epoch: 67, Iteration: 200, Loss: 0.014578718633856624\n",
      "Epoch: 67, Iteration: 300, Loss: 0.017639240417338442\n",
      "Epoch: 67, Iteration: 400, Loss: 0.018385261319053825\n",
      "Epoch: 67, Iteration: 500, Loss: 0.01497868265141733\n",
      "Epoch: 67, Iteration: 600, Loss: 0.01751065027929144\n",
      "Epoch: 67, Iteration: 700, Loss: 0.01547525379282888\n",
      "Epoch: 67, Iteration: 800, Loss: 0.015313791285734624\n",
      "Epoch: 67, Train Loss: 0.0006566910897370972, Test Loss: 0.0001554922545267605\n",
      "Epoch: 68, Iteration: 100, Loss: 0.015623913910530973\n",
      "Epoch: 68, Iteration: 200, Loss: 0.016074846687843092\n",
      "Epoch: 68, Iteration: 300, Loss: 0.01611976637650514\n",
      "Epoch: 68, Iteration: 400, Loss: 0.01748896686331136\n",
      "Epoch: 68, Iteration: 500, Loss: 0.01564516184225795\n",
      "Epoch: 68, Iteration: 600, Loss: 0.01596133827115409\n",
      "Epoch: 68, Iteration: 700, Loss: 0.017218622590007726\n",
      "Epoch: 68, Iteration: 800, Loss: 0.015919427263725083\n",
      "Epoch: 68, Train Loss: 0.0006562424368977485, Test Loss: 0.00016727314053420147\n",
      "Epoch: 69, Iteration: 100, Loss: 0.017457284309784882\n",
      "Epoch: 69, Iteration: 200, Loss: 0.0176769574827631\n",
      "Epoch: 69, Iteration: 300, Loss: 0.015635813106200658\n",
      "Epoch: 69, Iteration: 400, Loss: 0.016480495512951165\n",
      "Epoch: 69, Iteration: 500, Loss: 0.01580164671759121\n",
      "Epoch: 69, Iteration: 600, Loss: 0.016040871298173442\n",
      "Epoch: 69, Iteration: 700, Loss: 0.01541688652287121\n",
      "Epoch: 69, Iteration: 800, Loss: 0.015292268246412277\n",
      "Epoch: 69, Train Loss: 0.0006446457650470197, Test Loss: 0.0001763362461341158\n",
      "Epoch: 70, Iteration: 100, Loss: 0.016033024920034222\n",
      "Epoch: 70, Iteration: 200, Loss: 0.016501529804372694\n",
      "Epoch: 70, Iteration: 300, Loss: 0.016408830902946647\n",
      "Epoch: 70, Iteration: 400, Loss: 0.015224783281155396\n",
      "Epoch: 70, Iteration: 500, Loss: 0.017136512185970787\n",
      "Epoch: 70, Iteration: 600, Loss: 0.01638895364885684\n",
      "Epoch: 70, Iteration: 700, Loss: 0.01620298164198175\n",
      "Epoch: 70, Iteration: 800, Loss: 0.016191358263313305\n",
      "Epoch: 70, Train Loss: 0.0006550672512180406, Test Loss: 0.00015674861718514577\n",
      "Epoch: 71, Iteration: 100, Loss: 0.015422884200233966\n",
      "Epoch: 71, Iteration: 200, Loss: 0.014612284809118137\n",
      "Epoch: 71, Iteration: 300, Loss: 0.014945687180443201\n",
      "Epoch: 71, Iteration: 400, Loss: 0.016870447114342824\n",
      "Epoch: 71, Iteration: 500, Loss: 0.01749827114690561\n",
      "Epoch: 71, Iteration: 600, Loss: 0.01700593739951728\n",
      "Epoch: 71, Iteration: 700, Loss: 0.01708137670357246\n",
      "Epoch: 71, Iteration: 800, Loss: 0.01613913163600955\n",
      "Epoch: 71, Train Loss: 0.0006473522202271785, Test Loss: 0.0001474327748518046\n",
      "Epoch: 72, Iteration: 100, Loss: 0.015959885146003217\n",
      "Epoch: 72, Iteration: 200, Loss: 0.016378689142584335\n",
      "Epoch: 72, Iteration: 300, Loss: 0.015625309439201374\n",
      "Epoch: 72, Iteration: 400, Loss: 0.014928904158296064\n",
      "Epoch: 72, Iteration: 500, Loss: 0.016495725416461937\n",
      "Epoch: 72, Iteration: 600, Loss: 0.017226483265403658\n",
      "Epoch: 72, Iteration: 700, Loss: 0.016001360294467304\n",
      "Epoch: 72, Iteration: 800, Loss: 0.016116638806124683\n",
      "Epoch: 72, Train Loss: 0.0006412444543531994, Test Loss: 0.0001554238130819567\n",
      "Epoch: 73, Iteration: 100, Loss: 0.01547720616508741\n",
      "Epoch: 73, Iteration: 200, Loss: 0.01865595435083378\n",
      "Epoch: 73, Iteration: 300, Loss: 0.015093772795808036\n",
      "Epoch: 73, Iteration: 400, Loss: 0.01587986632512184\n",
      "Epoch: 73, Iteration: 500, Loss: 0.015891815350187244\n",
      "Epoch: 73, Iteration: 600, Loss: 0.016546889310120605\n",
      "Epoch: 73, Iteration: 700, Loss: 0.015963077050400898\n",
      "Epoch: 73, Iteration: 800, Loss: 0.014945477145374753\n",
      "Epoch: 73, Train Loss: 0.0006368972015596095, Test Loss: 0.00014863956112841985\n",
      "Epoch: 74, Iteration: 100, Loss: 0.015550147138128523\n",
      "Epoch: 74, Iteration: 200, Loss: 0.015597690267895814\n",
      "Epoch: 74, Iteration: 300, Loss: 0.014431451796554029\n",
      "Epoch: 74, Iteration: 400, Loss: 0.01633414464595262\n",
      "Epoch: 74, Iteration: 500, Loss: 0.014807690502493642\n",
      "Epoch: 74, Iteration: 600, Loss: 0.01480141126376111\n",
      "Epoch: 74, Iteration: 700, Loss: 0.01711559972318355\n",
      "Epoch: 74, Iteration: 800, Loss: 0.018278931151144207\n",
      "Epoch: 74, Train Loss: 0.0006327303460517156, Test Loss: 0.00014599702844197423\n",
      "Epoch: 75, Iteration: 100, Loss: 0.01499674854130717\n",
      "Epoch: 75, Iteration: 200, Loss: 0.016925655472732615\n",
      "Epoch: 75, Iteration: 300, Loss: 0.017048674846591894\n",
      "Epoch: 75, Iteration: 400, Loss: 0.01445068064640509\n",
      "Epoch: 75, Iteration: 500, Loss: 0.014965118083637208\n",
      "Epoch: 75, Iteration: 600, Loss: 0.016426835776655935\n",
      "Epoch: 75, Iteration: 700, Loss: 0.015327128661738243\n",
      "Epoch: 75, Iteration: 800, Loss: 0.015489723424252588\n",
      "Epoch: 75, Train Loss: 0.0006259402256396803, Test Loss: 0.00014618101744268293\n",
      "Epoch: 76, Iteration: 100, Loss: 0.015109492571355077\n",
      "Epoch: 76, Iteration: 200, Loss: 0.016021647621528246\n",
      "Epoch: 76, Iteration: 300, Loss: 0.015028248220914975\n",
      "Epoch: 76, Iteration: 400, Loss: 0.014741061473614536\n",
      "Epoch: 76, Iteration: 500, Loss: 0.016052246603067033\n",
      "Epoch: 76, Iteration: 600, Loss: 0.015445842858753167\n",
      "Epoch: 76, Iteration: 700, Loss: 0.014986766618676484\n",
      "Epoch: 76, Iteration: 800, Loss: 0.017078620447136927\n",
      "Epoch: 76, Train Loss: 0.0006251451620662247, Test Loss: 0.0001524376454226222\n",
      "Epoch: 77, Iteration: 100, Loss: 0.016558773269935045\n",
      "Epoch: 77, Iteration: 200, Loss: 0.015074524970259517\n",
      "Epoch: 77, Iteration: 300, Loss: 0.016660052780935075\n",
      "Epoch: 77, Iteration: 400, Loss: 0.01632362733653281\n",
      "Epoch: 77, Iteration: 500, Loss: 0.014629134522692766\n",
      "Epoch: 77, Iteration: 600, Loss: 0.014686759241158143\n",
      "Epoch: 77, Iteration: 700, Loss: 0.016277327624266036\n",
      "Epoch: 77, Iteration: 800, Loss: 0.014918867498636246\n",
      "Epoch: 77, Train Loss: 0.0006212370578009852, Test Loss: 0.00014234709556875793\n",
      "Epoch: 78, Iteration: 100, Loss: 0.015517550491495058\n",
      "Epoch: 78, Iteration: 200, Loss: 0.015620154503267258\n",
      "Epoch: 78, Iteration: 300, Loss: 0.01632851958129322\n",
      "Epoch: 78, Iteration: 400, Loss: 0.015726324960269267\n",
      "Epoch: 78, Iteration: 500, Loss: 0.01622051637241384\n",
      "Epoch: 78, Iteration: 600, Loss: 0.01623136179841822\n",
      "Epoch: 78, Iteration: 700, Loss: 0.015067595137224998\n",
      "Epoch: 78, Iteration: 800, Loss: 0.013876343240553979\n",
      "Epoch: 78, Train Loss: 0.000616429560255237, Test Loss: 0.00014376716275505942\n",
      "Epoch: 79, Iteration: 100, Loss: 0.014455807824560907\n",
      "Epoch: 79, Iteration: 200, Loss: 0.015764497704367386\n",
      "Epoch: 79, Iteration: 300, Loss: 0.015541574422968552\n",
      "Epoch: 79, Iteration: 400, Loss: 0.015600323495164048\n",
      "Epoch: 79, Iteration: 500, Loss: 0.015966103514074348\n",
      "Epoch: 79, Iteration: 600, Loss: 0.017586246802238747\n",
      "Epoch: 79, Iteration: 700, Loss: 0.014705303379741963\n",
      "Epoch: 79, Iteration: 800, Loss: 0.015417199712828733\n",
      "Epoch: 79, Train Loss: 0.0006167139687005815, Test Loss: 0.00016404637049516284\n",
      "Epoch: 80, Iteration: 100, Loss: 0.014898060988343786\n",
      "Epoch: 80, Iteration: 200, Loss: 0.0147242872917559\n",
      "Epoch: 80, Iteration: 300, Loss: 0.01593897359998664\n",
      "Epoch: 80, Iteration: 400, Loss: 0.01607868570863502\n",
      "Epoch: 80, Iteration: 500, Loss: 0.015682993238442577\n",
      "Epoch: 80, Iteration: 600, Loss: 0.015491317717533093\n",
      "Epoch: 80, Iteration: 700, Loss: 0.015826826114789583\n",
      "Epoch: 80, Iteration: 800, Loss: 0.015182000679487828\n",
      "Epoch: 80, Train Loss: 0.0006168301610757712, Test Loss: 0.00015830473766899263\n",
      "Epoch: 81, Iteration: 100, Loss: 0.014404878871573601\n",
      "Epoch: 81, Iteration: 200, Loss: 0.016235001014138106\n",
      "Epoch: 81, Iteration: 300, Loss: 0.01632140095171053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Iteration: 400, Loss: 0.015270852491084952\n",
      "Epoch: 81, Iteration: 500, Loss: 0.016256662616797257\n",
      "Epoch: 81, Iteration: 600, Loss: 0.014161326449539047\n",
      "Epoch: 81, Iteration: 700, Loss: 0.014758786724996753\n",
      "Epoch: 81, Iteration: 800, Loss: 0.014727625824889401\n",
      "Epoch: 81, Train Loss: 0.0006150761918056082, Test Loss: 0.00016939030755146463\n",
      "Epoch: 82, Iteration: 100, Loss: 0.015031199596705846\n",
      "Epoch: 82, Iteration: 200, Loss: 0.01690816315385746\n",
      "Epoch: 82, Iteration: 300, Loss: 0.014378939977177652\n",
      "Epoch: 82, Iteration: 400, Loss: 0.014575467292161193\n",
      "Epoch: 82, Iteration: 500, Loss: 0.015761073354951805\n",
      "Epoch: 82, Iteration: 600, Loss: 0.0155514779035002\n",
      "Epoch: 82, Iteration: 700, Loss: 0.013730639093409991\n",
      "Epoch: 82, Iteration: 800, Loss: 0.016495707390276948\n",
      "Epoch: 82, Train Loss: 0.0006075214504433868, Test Loss: 0.00014772468907867567\n",
      "Epoch: 83, Iteration: 100, Loss: 0.01394737531154533\n",
      "Epoch: 83, Iteration: 200, Loss: 0.016522937970876228\n",
      "Epoch: 83, Iteration: 300, Loss: 0.015422339783981442\n",
      "Epoch: 83, Iteration: 400, Loss: 0.014636190651799552\n",
      "Epoch: 83, Iteration: 500, Loss: 0.01405525879090419\n",
      "Epoch: 83, Iteration: 600, Loss: 0.01520366725162603\n",
      "Epoch: 83, Iteration: 700, Loss: 0.01485882319684606\n",
      "Epoch: 83, Iteration: 800, Loss: 0.014983522432885366\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 83, Train Loss: 0.000602177147246944, Test Loss: 0.00014842786986596636\n",
      "Epoch: 84, Iteration: 100, Loss: 0.013728098801948363\n",
      "Epoch: 84, Iteration: 200, Loss: 0.013890059311961522\n",
      "Epoch: 84, Iteration: 300, Loss: 0.013944218429969624\n",
      "Epoch: 84, Iteration: 400, Loss: 0.014517218449327629\n",
      "Epoch: 84, Iteration: 500, Loss: 0.013304432337463368\n",
      "Epoch: 84, Iteration: 600, Loss: 0.013715347922698129\n",
      "Epoch: 84, Iteration: 700, Loss: 0.013070073324342957\n",
      "Epoch: 84, Iteration: 800, Loss: 0.013915153642301448\n",
      "Epoch: 84, Train Loss: 0.0005489254528841325, Test Loss: 0.00013336765894639204\n",
      "Epoch: 85, Iteration: 100, Loss: 0.014875465614750283\n",
      "Epoch: 85, Iteration: 200, Loss: 0.015205892254016362\n",
      "Epoch: 85, Iteration: 300, Loss: 0.01433676494343672\n",
      "Epoch: 85, Iteration: 400, Loss: 0.012792941775842337\n",
      "Epoch: 85, Iteration: 500, Loss: 0.013928321644925745\n",
      "Epoch: 85, Iteration: 600, Loss: 0.013094240879581776\n",
      "Epoch: 85, Iteration: 700, Loss: 0.013417966412816895\n",
      "Epoch: 85, Iteration: 800, Loss: 0.012432349165464984\n",
      "Epoch: 85, Train Loss: 0.0005472997995452279, Test Loss: 0.0001333535857659182\n",
      "Epoch: 86, Iteration: 100, Loss: 0.012247960017703008\n",
      "Epoch: 86, Iteration: 200, Loss: 0.013743550851359032\n",
      "Epoch: 86, Iteration: 300, Loss: 0.014096086106292205\n",
      "Epoch: 86, Iteration: 400, Loss: 0.015368779517302755\n",
      "Epoch: 86, Iteration: 500, Loss: 0.013772572492598556\n",
      "Epoch: 86, Iteration: 600, Loss: 0.01325060413000756\n",
      "Epoch: 86, Iteration: 700, Loss: 0.012991530627914472\n",
      "Epoch: 86, Iteration: 800, Loss: 0.013489663826476317\n",
      "Epoch: 86, Train Loss: 0.0005471348425558955, Test Loss: 0.0001342652697335678\n",
      "Epoch: 87, Iteration: 100, Loss: 0.012763486949552316\n",
      "Epoch: 87, Iteration: 200, Loss: 0.013210121735028224\n",
      "Epoch: 87, Iteration: 300, Loss: 0.015380608372652205\n",
      "Epoch: 87, Iteration: 400, Loss: 0.013805531449179398\n",
      "Epoch: 87, Iteration: 500, Loss: 0.01278101449861424\n",
      "Epoch: 87, Iteration: 600, Loss: 0.013597478326119017\n",
      "Epoch: 87, Iteration: 700, Loss: 0.013525414869945962\n",
      "Epoch: 87, Iteration: 800, Loss: 0.013946036768174963\n",
      "Epoch: 87, Train Loss: 0.000547125771629505, Test Loss: 0.00013391861791276483\n",
      "Epoch: 88, Iteration: 100, Loss: 0.015227036317810416\n",
      "Epoch: 88, Iteration: 200, Loss: 0.012360626034933375\n",
      "Epoch: 88, Iteration: 300, Loss: 0.012763579274178483\n",
      "Epoch: 88, Iteration: 400, Loss: 0.014133939977909904\n",
      "Epoch: 88, Iteration: 500, Loss: 0.014613349245337304\n",
      "Epoch: 88, Iteration: 600, Loss: 0.014699648450914538\n",
      "Epoch: 88, Iteration: 700, Loss: 0.01308267977583455\n",
      "Epoch: 88, Iteration: 800, Loss: 0.012153845393186202\n",
      "Epoch: 88, Train Loss: 0.0005462000456284604, Test Loss: 0.00013584611897136736\n",
      "Epoch: 89, Iteration: 100, Loss: 0.012898081840830855\n",
      "Epoch: 89, Iteration: 200, Loss: 0.013339416265807813\n",
      "Epoch: 89, Iteration: 300, Loss: 0.013186981668695807\n",
      "Epoch: 89, Iteration: 400, Loss: 0.014267129914514953\n",
      "Epoch: 89, Iteration: 500, Loss: 0.013486087249475531\n",
      "Epoch: 89, Iteration: 600, Loss: 0.013337169599253684\n",
      "Epoch: 89, Iteration: 700, Loss: 0.013732708583120257\n",
      "Epoch: 89, Iteration: 800, Loss: 0.01497957111132564\n",
      "Epoch: 89, Train Loss: 0.0005467545910439131, Test Loss: 0.00013313810294879425\n",
      "Epoch: 90, Iteration: 100, Loss: 0.013111424927046755\n",
      "Epoch: 90, Iteration: 200, Loss: 0.014229245480237296\n",
      "Epoch: 90, Iteration: 300, Loss: 0.013038168279308593\n",
      "Epoch: 90, Iteration: 400, Loss: 0.012509528671216685\n",
      "Epoch: 90, Iteration: 500, Loss: 0.014495024541247403\n",
      "Epoch: 90, Iteration: 600, Loss: 0.01503792401854298\n",
      "Epoch: 90, Iteration: 700, Loss: 0.013164219097234309\n",
      "Epoch: 90, Iteration: 800, Loss: 0.013289755945152137\n",
      "Epoch: 90, Train Loss: 0.0005461539974548858, Test Loss: 0.000134692714867484\n",
      "Epoch: 91, Iteration: 100, Loss: 0.014755894815607462\n",
      "Epoch: 91, Iteration: 200, Loss: 0.013259134011605056\n",
      "Epoch: 91, Iteration: 300, Loss: 0.014351992947922554\n",
      "Epoch: 91, Iteration: 400, Loss: 0.013817632363497978\n",
      "Epoch: 91, Iteration: 500, Loss: 0.012453853341867216\n",
      "Epoch: 91, Iteration: 600, Loss: 0.013170693538995693\n",
      "Epoch: 91, Iteration: 700, Loss: 0.01421364139241632\n",
      "Epoch: 91, Iteration: 800, Loss: 0.013329914436326362\n",
      "Epoch: 91, Train Loss: 0.000544409949899801, Test Loss: 0.00013222418762102924\n",
      "Epoch: 92, Iteration: 100, Loss: 0.013869587713998044\n",
      "Epoch: 92, Iteration: 200, Loss: 0.015081152207130799\n",
      "Epoch: 92, Iteration: 300, Loss: 0.013454245061438996\n",
      "Epoch: 92, Iteration: 400, Loss: 0.014071457750105765\n",
      "Epoch: 92, Iteration: 500, Loss: 0.01319385629176395\n",
      "Epoch: 92, Iteration: 600, Loss: 0.012530011234048288\n",
      "Epoch: 92, Iteration: 700, Loss: 0.01258489651081618\n",
      "Epoch: 92, Iteration: 800, Loss: 0.014370563520060387\n",
      "Epoch: 92, Train Loss: 0.0005445395458015756, Test Loss: 0.00013401049119699427\n",
      "Epoch: 93, Iteration: 100, Loss: 0.014349716995639028\n",
      "Epoch: 93, Iteration: 200, Loss: 0.012198808100947645\n",
      "Epoch: 93, Iteration: 300, Loss: 0.015726489687949652\n",
      "Epoch: 93, Iteration: 400, Loss: 0.012910057728731772\n",
      "Epoch: 93, Iteration: 500, Loss: 0.014825835092779016\n",
      "Epoch: 93, Iteration: 600, Loss: 0.013775748913758434\n",
      "Epoch: 93, Iteration: 700, Loss: 0.011944950834731571\n",
      "Epoch: 93, Iteration: 800, Loss: 0.013545538924518041\n",
      "Epoch: 93, Train Loss: 0.0005441503199772881, Test Loss: 0.00013223694337143342\n",
      "Epoch: 94, Iteration: 100, Loss: 0.01598919885873329\n",
      "Epoch: 94, Iteration: 200, Loss: 0.011937543040403398\n",
      "Epoch: 94, Iteration: 300, Loss: 0.013479484874551417\n",
      "Epoch: 94, Iteration: 400, Loss: 0.014336699063278502\n",
      "Epoch: 94, Iteration: 500, Loss: 0.013242828725196887\n",
      "Epoch: 94, Iteration: 600, Loss: 0.013046471278357785\n",
      "Epoch: 94, Iteration: 700, Loss: 0.012892557355371537\n",
      "Epoch: 94, Iteration: 800, Loss: 0.013181561207602499\n",
      "Epoch: 94, Train Loss: 0.0005424485345063816, Test Loss: 0.00013254903999173728\n",
      "Epoch: 95, Iteration: 100, Loss: 0.012665182792261476\n",
      "Epoch: 95, Iteration: 200, Loss: 0.015319174384785583\n",
      "Epoch: 95, Iteration: 300, Loss: 0.012658938947424758\n",
      "Epoch: 95, Iteration: 400, Loss: 0.013483339702361263\n",
      "Epoch: 95, Iteration: 500, Loss: 0.012439590922440402\n",
      "Epoch: 95, Iteration: 600, Loss: 0.014705913668876747\n",
      "Epoch: 95, Iteration: 700, Loss: 0.012835271063522669\n",
      "Epoch: 95, Iteration: 800, Loss: 0.014013843512657331\n",
      "Epoch: 95, Train Loss: 0.0005424122209826827, Test Loss: 0.00013366050447922596\n",
      "Epoch: 96, Iteration: 100, Loss: 0.013162860621378059\n",
      "Epoch: 96, Iteration: 200, Loss: 0.013632123103889171\n",
      "Epoch: 96, Iteration: 300, Loss: 0.014130477189610247\n",
      "Epoch: 96, Iteration: 400, Loss: 0.013349998855119338\n",
      "Epoch: 96, Iteration: 500, Loss: 0.012825234793126583\n",
      "Epoch: 96, Iteration: 600, Loss: 0.013413736334769055\n",
      "Epoch: 96, Iteration: 700, Loss: 0.013956566101114731\n",
      "Epoch: 96, Iteration: 800, Loss: 0.01393311918945983\n",
      "Epoch: 96, Train Loss: 0.0005422459430914195, Test Loss: 0.00013249318381068701\n",
      "Epoch: 97, Iteration: 100, Loss: 0.013849426592059899\n",
      "Epoch: 97, Iteration: 200, Loss: 0.013722636191232596\n",
      "Epoch: 97, Iteration: 300, Loss: 0.012993723303225124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97, Iteration: 400, Loss: 0.014482011039945064\n",
      "Epoch: 97, Iteration: 500, Loss: 0.013140515147824772\n",
      "Epoch: 97, Iteration: 600, Loss: 0.013397999075095868\n",
      "Epoch: 97, Iteration: 700, Loss: 0.012265705361642176\n",
      "Epoch: 97, Iteration: 800, Loss: 0.013985218192829052\n",
      "Epoch: 97, Train Loss: 0.00054068210474351, Test Loss: 0.00013186830355688508\n",
      "Epoch: 98, Iteration: 100, Loss: 0.01314552612893749\n",
      "Epoch: 98, Iteration: 200, Loss: 0.015044760308228433\n",
      "Epoch: 98, Iteration: 300, Loss: 0.012541441174107604\n",
      "Epoch: 98, Iteration: 400, Loss: 0.012157595709140878\n",
      "Epoch: 98, Iteration: 500, Loss: 0.012428040430677356\n",
      "Epoch: 98, Iteration: 600, Loss: 0.01518566283994005\n",
      "Epoch: 98, Iteration: 700, Loss: 0.014100105043326039\n",
      "Epoch: 98, Iteration: 800, Loss: 0.012863520467362832\n",
      "Epoch: 98, Train Loss: 0.000541578999197708, Test Loss: 0.00013202746717294372\n",
      "Epoch: 99, Iteration: 100, Loss: 0.01389289947837824\n",
      "Epoch: 99, Iteration: 200, Loss: 0.013453738974931184\n",
      "Epoch: 99, Iteration: 300, Loss: 0.013549003957450623\n",
      "Epoch: 99, Iteration: 400, Loss: 0.013836557820468443\n",
      "Epoch: 99, Iteration: 500, Loss: 0.013990325529448455\n",
      "Epoch: 99, Iteration: 600, Loss: 0.014304837444797158\n",
      "Epoch: 99, Iteration: 700, Loss: 0.01269940604470321\n",
      "Epoch: 99, Iteration: 800, Loss: 0.013498270556738134\n",
      "Epoch: 99, Train Loss: 0.0005411383605266593, Test Loss: 0.0001313381992791063\n",
      "Epoch: 100, Iteration: 100, Loss: 0.01267568847106304\n",
      "Epoch: 100, Iteration: 200, Loss: 0.013386413287662435\n",
      "Epoch: 100, Iteration: 300, Loss: 0.013756295025814325\n",
      "Epoch: 100, Iteration: 400, Loss: 0.014542000004439615\n",
      "Epoch: 100, Iteration: 500, Loss: 0.014037464370630914\n",
      "Epoch: 100, Iteration: 600, Loss: 0.012358187053905567\n",
      "Epoch: 100, Iteration: 700, Loss: 0.014263867818954168\n",
      "Epoch: 100, Iteration: 800, Loss: 0.01301719996263273\n",
      "Epoch: 100, Train Loss: 0.0005390440370760206, Test Loss: 0.0001319138651217236\n",
      "Epoch: 101, Iteration: 100, Loss: 0.013521272365323966\n",
      "Epoch: 101, Iteration: 200, Loss: 0.013862076844816329\n",
      "Epoch: 101, Iteration: 300, Loss: 0.012709449852991384\n",
      "Epoch: 101, Iteration: 400, Loss: 0.014167690205795225\n",
      "Epoch: 101, Iteration: 500, Loss: 0.014506918807455804\n",
      "Epoch: 101, Iteration: 600, Loss: 0.013207258223701501\n",
      "Epoch: 101, Iteration: 700, Loss: 0.01312703243092983\n",
      "Epoch: 101, Iteration: 800, Loss: 0.013089490807033144\n",
      "Epoch: 101, Train Loss: 0.0005381836077279717, Test Loss: 0.00013055067750150615\n",
      "Epoch: 102, Iteration: 100, Loss: 0.013706071142223664\n",
      "Epoch: 102, Iteration: 200, Loss: 0.013765392282948596\n",
      "Epoch: 102, Iteration: 300, Loss: 0.013904835264838766\n",
      "Epoch: 102, Iteration: 400, Loss: 0.014095381106017157\n",
      "Epoch: 102, Iteration: 500, Loss: 0.01235422308309353\n",
      "Epoch: 102, Iteration: 600, Loss: 0.013102217104460578\n",
      "Epoch: 102, Iteration: 700, Loss: 0.013099176714604255\n",
      "Epoch: 102, Iteration: 800, Loss: 0.013293885356688406\n",
      "Epoch: 102, Train Loss: 0.0005387749622960535, Test Loss: 0.00013056706581527865\n",
      "Epoch: 103, Iteration: 100, Loss: 0.013870247206796193\n",
      "Epoch: 103, Iteration: 200, Loss: 0.014289445149188396\n",
      "Epoch: 103, Iteration: 300, Loss: 0.012725831129500875\n",
      "Epoch: 103, Iteration: 400, Loss: 0.01334631994177471\n",
      "Epoch: 103, Iteration: 500, Loss: 0.013513205129129346\n",
      "Epoch: 103, Iteration: 600, Loss: 0.01446399807537091\n",
      "Epoch: 103, Iteration: 700, Loss: 0.013307006240211194\n",
      "Epoch: 103, Iteration: 800, Loss: 0.01181667688433663\n",
      "Epoch: 103, Train Loss: 0.0005370329762789044, Test Loss: 0.00013031511737704042\n",
      "Epoch: 104, Iteration: 100, Loss: 0.013602988885395462\n",
      "Epoch: 104, Iteration: 200, Loss: 0.013092350156512111\n",
      "Epoch: 104, Iteration: 300, Loss: 0.014268029382947134\n",
      "Epoch: 104, Iteration: 400, Loss: 0.013566356559749693\n",
      "Epoch: 104, Iteration: 500, Loss: 0.014038989549590042\n",
      "Epoch: 104, Iteration: 600, Loss: 0.012012476967356633\n",
      "Epoch: 104, Iteration: 700, Loss: 0.013579793649114436\n",
      "Epoch: 104, Iteration: 800, Loss: 0.012685288325883448\n",
      "Epoch: 104, Train Loss: 0.0005367684713817313, Test Loss: 0.00012999446931378554\n",
      "Epoch: 105, Iteration: 100, Loss: 0.013268821428937372\n",
      "Epoch: 105, Iteration: 200, Loss: 0.012673279346927302\n",
      "Epoch: 105, Iteration: 300, Loss: 0.013504680780897615\n",
      "Epoch: 105, Iteration: 400, Loss: 0.015011171453807037\n",
      "Epoch: 105, Iteration: 500, Loss: 0.013133244701748481\n",
      "Epoch: 105, Iteration: 600, Loss: 0.013377739804127486\n",
      "Epoch: 105, Iteration: 700, Loss: 0.0135910492426774\n",
      "Epoch: 105, Iteration: 800, Loss: 0.014051905385713326\n",
      "Epoch: 105, Train Loss: 0.0005364145570256286, Test Loss: 0.00013259238787138937\n",
      "Epoch: 106, Iteration: 100, Loss: 0.013470399717334658\n",
      "Epoch: 106, Iteration: 200, Loss: 0.012591588998475345\n",
      "Epoch: 106, Iteration: 300, Loss: 0.01360954474148457\n",
      "Epoch: 106, Iteration: 400, Loss: 0.01387422087645973\n",
      "Epoch: 106, Iteration: 500, Loss: 0.012731150633044308\n",
      "Epoch: 106, Iteration: 600, Loss: 0.014053347902517999\n",
      "Epoch: 106, Iteration: 700, Loss: 0.013817931310768472\n",
      "Epoch: 106, Iteration: 800, Loss: 0.012416003279213328\n",
      "Epoch: 106, Train Loss: 0.0005363701147456031, Test Loss: 0.00013081733722834568\n",
      "Epoch: 107, Iteration: 100, Loss: 0.012383691620925674\n",
      "Epoch: 107, Iteration: 200, Loss: 0.013254768589831656\n",
      "Epoch: 107, Iteration: 300, Loss: 0.014791337533097249\n",
      "Epoch: 107, Iteration: 400, Loss: 0.012859478450991446\n",
      "Epoch: 107, Iteration: 500, Loss: 0.013224533708125819\n",
      "Epoch: 107, Iteration: 600, Loss: 0.013903484745242167\n",
      "Epoch: 107, Iteration: 700, Loss: 0.012872703806351637\n",
      "Epoch: 107, Iteration: 800, Loss: 0.01385212039531325\n",
      "Epoch: 107, Train Loss: 0.0005353872830091527, Test Loss: 0.00013111363766161029\n",
      "Epoch: 108, Iteration: 100, Loss: 0.012592027786013205\n",
      "Epoch: 108, Iteration: 200, Loss: 0.012464913983421866\n",
      "Epoch: 108, Iteration: 300, Loss: 0.013916124476963887\n",
      "Epoch: 108, Iteration: 400, Loss: 0.012958612147485837\n",
      "Epoch: 108, Iteration: 500, Loss: 0.013848697239154717\n",
      "Epoch: 108, Iteration: 600, Loss: 0.01392166694131447\n",
      "Epoch: 108, Iteration: 700, Loss: 0.012362409608613234\n",
      "Epoch: 108, Iteration: 800, Loss: 0.013767435164481867\n",
      "Epoch: 108, Train Loss: 0.0005349317924806595, Test Loss: 0.0001298344608560549\n",
      "Epoch: 109, Iteration: 100, Loss: 0.013401822598098079\n",
      "Epoch: 109, Iteration: 200, Loss: 0.013047713862761157\n",
      "Epoch: 109, Iteration: 300, Loss: 0.013431617244350491\n",
      "Epoch: 109, Iteration: 400, Loss: 0.012884921990917064\n",
      "Epoch: 109, Iteration: 500, Loss: 0.01265972987312125\n",
      "Epoch: 109, Iteration: 600, Loss: 0.01428727782331407\n",
      "Epoch: 109, Iteration: 700, Loss: 0.014368159285368165\n",
      "Epoch: 109, Iteration: 800, Loss: 0.013239181509561604\n",
      "Epoch: 109, Train Loss: 0.0005338953638476687, Test Loss: 0.00013507010427024003\n",
      "Epoch: 110, Iteration: 100, Loss: 0.01377644364038133\n",
      "Epoch: 110, Iteration: 200, Loss: 0.012971830517926719\n",
      "Epoch: 110, Iteration: 300, Loss: 0.012728167217574082\n",
      "Epoch: 110, Iteration: 400, Loss: 0.014132897595118266\n",
      "Epoch: 110, Iteration: 500, Loss: 0.01268084012917825\n",
      "Epoch: 110, Iteration: 600, Loss: 0.014698560895340052\n",
      "Epoch: 110, Iteration: 700, Loss: 0.01309745207981905\n",
      "Epoch: 110, Iteration: 800, Loss: 0.012970835825399263\n",
      "Epoch: 110, Train Loss: 0.0005346907605038324, Test Loss: 0.0001309145753020442\n",
      "Epoch: 111, Iteration: 100, Loss: 0.012748919471050613\n",
      "Epoch: 111, Iteration: 200, Loss: 0.013592308834631694\n",
      "Epoch: 111, Iteration: 300, Loss: 0.013462264294503257\n",
      "Epoch: 111, Iteration: 400, Loss: 0.014608811594371218\n",
      "Epoch: 111, Iteration: 500, Loss: 0.012324965369771235\n",
      "Epoch: 111, Iteration: 600, Loss: 0.013106737671478186\n",
      "Epoch: 111, Iteration: 700, Loss: 0.012212203146191314\n",
      "Epoch: 111, Iteration: 800, Loss: 0.01404504211677704\n",
      "Epoch: 111, Train Loss: 0.0005337996284917824, Test Loss: 0.00012982817507653548\n",
      "Epoch: 112, Iteration: 100, Loss: 0.01284603417298058\n",
      "Epoch: 112, Iteration: 200, Loss: 0.014093741821852745\n",
      "Epoch: 112, Iteration: 300, Loss: 0.013793500082101673\n",
      "Epoch: 112, Iteration: 400, Loss: 0.0132317709394556\n",
      "Epoch: 112, Iteration: 500, Loss: 0.012946965911396546\n",
      "Epoch: 112, Iteration: 600, Loss: 0.013484258626704104\n",
      "Epoch: 112, Iteration: 700, Loss: 0.013958055693365168\n",
      "Epoch: 112, Iteration: 800, Loss: 0.012855388602474704\n",
      "Epoch: 112, Train Loss: 0.000533070566757799, Test Loss: 0.00012933778751042957\n",
      "Epoch: 113, Iteration: 100, Loss: 0.013876777135010343\n",
      "Epoch: 113, Iteration: 200, Loss: 0.013485921968822367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113, Iteration: 300, Loss: 0.011656517599476501\n",
      "Epoch: 113, Iteration: 400, Loss: 0.012983818360225996\n",
      "Epoch: 113, Iteration: 500, Loss: 0.012796100621926598\n",
      "Epoch: 113, Iteration: 600, Loss: 0.013786638028250309\n",
      "Epoch: 113, Iteration: 700, Loss: 0.013536218899389496\n",
      "Epoch: 113, Iteration: 800, Loss: 0.012916660161863547\n",
      "Epoch: 113, Train Loss: 0.000532154703051512, Test Loss: 0.00012969320418566177\n",
      "Epoch: 114, Iteration: 100, Loss: 0.013883559968235204\n",
      "Epoch: 114, Iteration: 200, Loss: 0.01382082091004122\n",
      "Epoch: 114, Iteration: 300, Loss: 0.013501982197340112\n",
      "Epoch: 114, Iteration: 400, Loss: 0.013649825177708408\n",
      "Epoch: 114, Iteration: 500, Loss: 0.012269269082025858\n",
      "Epoch: 114, Iteration: 600, Loss: 0.01220192057735403\n",
      "Epoch: 114, Iteration: 700, Loss: 0.014578625046851812\n",
      "Epoch: 114, Iteration: 800, Loss: 0.012862928881077096\n",
      "Epoch: 114, Train Loss: 0.00053205822976734, Test Loss: 0.00013046542337851613\n",
      "Epoch: 115, Iteration: 100, Loss: 0.01411118379837717\n",
      "Epoch: 115, Iteration: 200, Loss: 0.012663215562497498\n",
      "Epoch: 115, Iteration: 300, Loss: 0.014974704954511253\n",
      "Epoch: 115, Iteration: 400, Loss: 0.013375892474869033\n",
      "Epoch: 115, Iteration: 500, Loss: 0.014215907005564077\n",
      "Epoch: 115, Iteration: 600, Loss: 0.01235973587972694\n",
      "Epoch: 115, Iteration: 700, Loss: 0.012206963019707473\n",
      "Epoch: 115, Iteration: 800, Loss: 0.013286987126775784\n",
      "Epoch: 115, Train Loss: 0.0005331168658190171, Test Loss: 0.00012949968570711507\n",
      "Epoch: 116, Iteration: 100, Loss: 0.013161563627363648\n",
      "Epoch: 116, Iteration: 200, Loss: 0.012396678102959413\n",
      "Epoch: 116, Iteration: 300, Loss: 0.014287109228462214\n",
      "Epoch: 116, Iteration: 400, Loss: 0.013292815569002414\n",
      "Epoch: 116, Iteration: 500, Loss: 0.012892514168925118\n",
      "Epoch: 116, Iteration: 600, Loss: 0.013360946788452566\n",
      "Epoch: 116, Iteration: 700, Loss: 0.0137656168481044\n",
      "Epoch: 116, Iteration: 800, Loss: 0.013244255675090244\n",
      "Epoch: 116, Train Loss: 0.0005319603125876524, Test Loss: 0.0001306443049726034\n",
      "Epoch: 117, Iteration: 100, Loss: 0.012599988804140594\n",
      "Epoch: 117, Iteration: 200, Loss: 0.013948735500889597\n",
      "Epoch: 117, Iteration: 300, Loss: 0.013190623907576082\n",
      "Epoch: 117, Iteration: 400, Loss: 0.013930432221968658\n",
      "Epoch: 117, Iteration: 500, Loss: 0.014011060862685554\n",
      "Epoch: 117, Iteration: 600, Loss: 0.012393589608109323\n",
      "Epoch: 117, Iteration: 700, Loss: 0.013081209795927862\n",
      "Epoch: 117, Iteration: 800, Loss: 0.012455382300686324\n",
      "Epoch: 117, Train Loss: 0.000530171622863049, Test Loss: 0.00013011757541021553\n",
      "Epoch: 118, Iteration: 100, Loss: 0.012963374298124108\n",
      "Epoch: 118, Iteration: 200, Loss: 0.013198356122302357\n",
      "Epoch: 118, Iteration: 300, Loss: 0.013053190981736407\n",
      "Epoch: 118, Iteration: 400, Loss: 0.012461956146580633\n",
      "Epoch: 118, Iteration: 500, Loss: 0.013662559416843578\n",
      "Epoch: 118, Iteration: 600, Loss: 0.012752434216963593\n",
      "Epoch: 118, Iteration: 700, Loss: 0.013721538878598949\n",
      "Epoch: 118, Iteration: 800, Loss: 0.01442820349620888\n",
      "Epoch   119: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 118, Train Loss: 0.0005300533256430251, Test Loss: 0.00013023255969404101\n",
      "Epoch: 119, Iteration: 100, Loss: 0.012721925268124323\n",
      "Epoch: 119, Iteration: 200, Loss: 0.0133311071076605\n",
      "Epoch: 119, Iteration: 300, Loss: 0.011976973477430874\n",
      "Epoch: 119, Iteration: 400, Loss: 0.012323753602686338\n",
      "Epoch: 119, Iteration: 500, Loss: 0.013134221684595104\n",
      "Epoch: 119, Iteration: 600, Loss: 0.014275947676651413\n",
      "Epoch: 119, Iteration: 700, Loss: 0.013974053963465849\n",
      "Epoch: 119, Iteration: 800, Loss: 0.013394835961662466\n",
      "Epoch: 119, Train Loss: 0.0005230232859441009, Test Loss: 0.00012785460581055726\n",
      "Epoch: 120, Iteration: 100, Loss: 0.013814615351293469\n",
      "Epoch: 120, Iteration: 200, Loss: 0.012241474836628186\n",
      "Epoch: 120, Iteration: 300, Loss: 0.013259427156299353\n",
      "Epoch: 120, Iteration: 400, Loss: 0.012528711347840726\n",
      "Epoch: 120, Iteration: 500, Loss: 0.01335073860900593\n",
      "Epoch: 120, Iteration: 600, Loss: 0.014142289281153353\n",
      "Epoch: 120, Iteration: 700, Loss: 0.013140611619746778\n",
      "Epoch: 120, Iteration: 800, Loss: 0.012804587262507994\n",
      "Epoch: 120, Train Loss: 0.0005228819566968612, Test Loss: 0.00012778564637737374\n",
      "Epoch: 121, Iteration: 100, Loss: 0.011170763551490381\n",
      "Epoch: 121, Iteration: 200, Loss: 0.013974217628856422\n",
      "Epoch: 121, Iteration: 300, Loss: 0.013229181902715936\n",
      "Epoch: 121, Iteration: 400, Loss: 0.012953742883837549\n",
      "Epoch: 121, Iteration: 500, Loss: 0.013780472505459329\n",
      "Epoch: 121, Iteration: 600, Loss: 0.012110243515053298\n",
      "Epoch: 121, Iteration: 700, Loss: 0.013775669867754914\n",
      "Epoch: 121, Iteration: 800, Loss: 0.01375072780501796\n",
      "Epoch: 121, Train Loss: 0.000522775973470299, Test Loss: 0.00012771642786694147\n",
      "Epoch: 122, Iteration: 100, Loss: 0.012420464834576705\n",
      "Epoch: 122, Iteration: 200, Loss: 0.01357840874698013\n",
      "Epoch: 122, Iteration: 300, Loss: 0.01234384148119716\n",
      "Epoch: 122, Iteration: 400, Loss: 0.013101454605930485\n",
      "Epoch: 122, Iteration: 500, Loss: 0.013224613681813935\n",
      "Epoch: 122, Iteration: 600, Loss: 0.013044499199168058\n",
      "Epoch: 122, Iteration: 700, Loss: 0.013110068979585776\n",
      "Epoch: 122, Iteration: 800, Loss: 0.013358778618567158\n",
      "Epoch: 122, Train Loss: 0.0005227516011552105, Test Loss: 0.00012772700470060506\n",
      "Epoch: 123, Iteration: 100, Loss: 0.011753502440114971\n",
      "Epoch: 123, Iteration: 200, Loss: 0.013233438010502141\n",
      "Epoch: 123, Iteration: 300, Loss: 0.013026111948420294\n",
      "Epoch: 123, Iteration: 400, Loss: 0.012837820238928543\n",
      "Epoch: 123, Iteration: 500, Loss: 0.013370407938054996\n",
      "Epoch: 123, Iteration: 600, Loss: 0.013738722522248281\n",
      "Epoch: 123, Iteration: 700, Loss: 0.012796158534911228\n",
      "Epoch: 123, Iteration: 800, Loss: 0.014197067426721333\n",
      "Epoch: 123, Train Loss: 0.0005226033596817902, Test Loss: 0.00012784771012970498\n",
      "Epoch: 124, Iteration: 100, Loss: 0.013035717460297747\n",
      "Epoch: 124, Iteration: 200, Loss: 0.013071446224785177\n",
      "Epoch: 124, Iteration: 300, Loss: 0.01262204376689624\n",
      "Epoch: 124, Iteration: 400, Loss: 0.013274422683025477\n",
      "Epoch: 124, Iteration: 500, Loss: 0.014000382016092772\n",
      "Epoch: 124, Iteration: 600, Loss: 0.01261178888307768\n",
      "Epoch: 124, Iteration: 700, Loss: 0.012258835009561153\n",
      "Epoch: 124, Iteration: 800, Loss: 0.014070559456740739\n",
      "Epoch: 124, Train Loss: 0.0005226039617257532, Test Loss: 0.00012759315379223888\n",
      "Epoch: 125, Iteration: 100, Loss: 0.012835283454478486\n",
      "Epoch: 125, Iteration: 200, Loss: 0.012213921010697959\n",
      "Epoch: 125, Iteration: 300, Loss: 0.012418860376783414\n",
      "Epoch: 125, Iteration: 400, Loss: 0.012147989684308413\n",
      "Epoch: 125, Iteration: 500, Loss: 0.013546525176934665\n",
      "Epoch: 125, Iteration: 600, Loss: 0.013046562169620302\n",
      "Epoch: 125, Iteration: 700, Loss: 0.013749313657172024\n",
      "Epoch: 125, Iteration: 800, Loss: 0.014843764478428056\n",
      "Epoch: 125, Train Loss: 0.0005223579968157816, Test Loss: 0.00012781494039604675\n",
      "Epoch: 126, Iteration: 100, Loss: 0.013111128162563546\n",
      "Epoch: 126, Iteration: 200, Loss: 0.013174192663427675\n",
      "Epoch: 126, Iteration: 300, Loss: 0.01263210734759923\n",
      "Epoch: 126, Iteration: 400, Loss: 0.013409716466412647\n",
      "Epoch: 126, Iteration: 500, Loss: 0.012163629329734249\n",
      "Epoch: 126, Iteration: 600, Loss: 0.013643952741404064\n",
      "Epoch: 126, Iteration: 700, Loss: 0.012710210612567607\n",
      "Epoch: 126, Iteration: 800, Loss: 0.013808755214995472\n",
      "Epoch: 126, Train Loss: 0.000522436825919351, Test Loss: 0.0001278850660423087\n",
      "Epoch: 127, Iteration: 100, Loss: 0.012488337226386648\n",
      "Epoch: 127, Iteration: 200, Loss: 0.013089236825180706\n",
      "Epoch: 127, Iteration: 300, Loss: 0.01312807286012685\n",
      "Epoch: 127, Iteration: 400, Loss: 0.012849286045820918\n",
      "Epoch: 127, Iteration: 500, Loss: 0.013399684616160812\n",
      "Epoch: 127, Iteration: 600, Loss: 0.013506672956282273\n",
      "Epoch: 127, Iteration: 700, Loss: 0.012304883985052584\n",
      "Epoch: 127, Iteration: 800, Loss: 0.012956411643244792\n",
      "Epoch   128: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 127, Train Loss: 0.0005221504630487982, Test Loss: 0.00012796274732203503\n",
      "Epoch: 128, Iteration: 100, Loss: 0.013031904563831631\n",
      "Epoch: 128, Iteration: 200, Loss: 0.01236122772024828\n",
      "Epoch: 128, Iteration: 300, Loss: 0.013785153092612745\n",
      "Epoch: 128, Iteration: 400, Loss: 0.013140710470906924\n",
      "Epoch: 128, Iteration: 500, Loss: 0.013200440509535838\n",
      "Epoch: 128, Iteration: 600, Loss: 0.012239973540999927\n",
      "Epoch: 128, Iteration: 700, Loss: 0.014552308981365059\n",
      "Epoch: 128, Iteration: 800, Loss: 0.01148281915084226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 128, Train Loss: 0.0005216788880026805, Test Loss: 0.00012757266848308213\n",
      "Epoch: 129, Iteration: 100, Loss: 0.01274483920133207\n",
      "Epoch: 129, Iteration: 200, Loss: 0.013479872803145554\n",
      "Epoch: 129, Iteration: 300, Loss: 0.012609029705345165\n",
      "Epoch: 129, Iteration: 400, Loss: 0.012302871087740641\n",
      "Epoch: 129, Iteration: 500, Loss: 0.012751831327477703\n",
      "Epoch: 129, Iteration: 600, Loss: 0.011854612159368116\n",
      "Epoch: 129, Iteration: 700, Loss: 0.014327201421110658\n",
      "Epoch: 129, Iteration: 800, Loss: 0.01310827118504676\n",
      "Epoch: 129, Train Loss: 0.0005214557714103009, Test Loss: 0.0001279646643209133\n",
      "Epoch: 130, Iteration: 100, Loss: 0.0136176140185853\n",
      "Epoch: 130, Iteration: 200, Loss: 0.01323226081149187\n",
      "Epoch: 130, Iteration: 300, Loss: 0.013143218420736957\n",
      "Epoch: 130, Iteration: 400, Loss: 0.013117450427671429\n",
      "Epoch: 130, Iteration: 500, Loss: 0.01336491726760869\n",
      "Epoch: 130, Iteration: 600, Loss: 0.012966617148777004\n",
      "Epoch: 130, Iteration: 700, Loss: 0.01223997716078884\n",
      "Epoch: 130, Iteration: 800, Loss: 0.012393857821734855\n",
      "Epoch: 130, Train Loss: 0.0005214320402266829, Test Loss: 0.00012764463979725887\n",
      "Epoch: 131, Iteration: 100, Loss: 0.012891512025817065\n",
      "Epoch: 131, Iteration: 200, Loss: 0.011764650083932793\n",
      "Epoch: 131, Iteration: 300, Loss: 0.014476230167929316\n",
      "Epoch: 131, Iteration: 400, Loss: 0.013468626177200349\n",
      "Epoch: 131, Iteration: 500, Loss: 0.012244995145010762\n",
      "Epoch: 131, Iteration: 600, Loss: 0.01358087349581183\n",
      "Epoch: 131, Iteration: 700, Loss: 0.012819996387406718\n",
      "Epoch: 131, Iteration: 800, Loss: 0.01305321112886304\n",
      "Epoch: 131, Train Loss: 0.0005214391080055282, Test Loss: 0.00012757920985137747\n",
      "Epoch: 132, Iteration: 100, Loss: 0.012270159655599855\n",
      "Epoch: 132, Iteration: 200, Loss: 0.01370579783906578\n",
      "Epoch: 132, Iteration: 300, Loss: 0.012894362662336789\n",
      "Epoch: 132, Iteration: 400, Loss: 0.012884988740552217\n",
      "Epoch: 132, Iteration: 500, Loss: 0.0140187901124591\n",
      "Epoch: 132, Iteration: 600, Loss: 0.01385007854696596\n",
      "Epoch: 132, Iteration: 700, Loss: 0.012988419763132697\n",
      "Epoch: 132, Iteration: 800, Loss: 0.01216784019197803\n",
      "Epoch: 132, Train Loss: 0.0005214180060008224, Test Loss: 0.00012758755278332397\n",
      "Epoch: 133, Iteration: 100, Loss: 0.012639479857170954\n",
      "Epoch: 133, Iteration: 200, Loss: 0.01399157786363503\n",
      "Epoch: 133, Iteration: 300, Loss: 0.012497360854467843\n",
      "Epoch: 133, Iteration: 400, Loss: 0.011708154728694353\n",
      "Epoch: 133, Iteration: 500, Loss: 0.01361554866161896\n",
      "Epoch: 133, Iteration: 600, Loss: 0.013044119299593149\n",
      "Epoch: 133, Iteration: 700, Loss: 0.0125702929108229\n",
      "Epoch: 133, Iteration: 800, Loss: 0.013428479567664908\n",
      "Epoch: 133, Train Loss: 0.00052142234546168, Test Loss: 0.00012754855971380995\n",
      "Epoch: 134, Iteration: 100, Loss: 0.0132143350529077\n",
      "Epoch: 134, Iteration: 200, Loss: 0.011931515349715482\n",
      "Epoch: 134, Iteration: 300, Loss: 0.013727229386859108\n",
      "Epoch: 134, Iteration: 400, Loss: 0.013499431770469528\n",
      "Epoch: 134, Iteration: 500, Loss: 0.011742043745471165\n",
      "Epoch: 134, Iteration: 600, Loss: 0.01318172195169609\n",
      "Epoch: 134, Iteration: 700, Loss: 0.013918468157498864\n",
      "Epoch: 134, Iteration: 800, Loss: 0.012639363456401043\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 134, Train Loss: 0.0005214202329255938, Test Loss: 0.00012751782088597373\n",
      "Epoch: 135, Iteration: 100, Loss: 0.012871078728494467\n",
      "Epoch: 135, Iteration: 200, Loss: 0.014072149380808696\n",
      "Epoch: 135, Iteration: 300, Loss: 0.01222741266246885\n",
      "Epoch: 135, Iteration: 400, Loss: 0.013334135772311129\n",
      "Epoch: 135, Iteration: 500, Loss: 0.012607692689925898\n",
      "Epoch: 135, Iteration: 600, Loss: 0.013699816492589889\n",
      "Epoch: 135, Iteration: 700, Loss: 0.013491836933098966\n",
      "Epoch: 135, Iteration: 800, Loss: 0.012558667785924627\n",
      "Epoch: 135, Train Loss: 0.0005213199910442249, Test Loss: 0.0001275155833130543\n",
      "Epoch: 136, Iteration: 100, Loss: 0.013069494838418905\n",
      "Epoch: 136, Iteration: 200, Loss: 0.012377652819850482\n",
      "Epoch: 136, Iteration: 300, Loss: 0.012044939590850845\n",
      "Epoch: 136, Iteration: 400, Loss: 0.013767154745437438\n",
      "Epoch: 136, Iteration: 500, Loss: 0.012464862393244402\n",
      "Epoch: 136, Iteration: 600, Loss: 0.012346533439995255\n",
      "Epoch: 136, Iteration: 700, Loss: 0.013608796129119582\n",
      "Epoch: 136, Iteration: 800, Loss: 0.013997731133713387\n",
      "Epoch: 136, Train Loss: 0.0005213084179691686, Test Loss: 0.0001275154234413281\n",
      "Epoch: 137, Iteration: 100, Loss: 0.013192932998208562\n",
      "Epoch: 137, Iteration: 200, Loss: 0.014207561067451024\n",
      "Epoch: 137, Iteration: 300, Loss: 0.014512683443172136\n",
      "Epoch: 137, Iteration: 400, Loss: 0.012829254395910539\n",
      "Epoch: 137, Iteration: 500, Loss: 0.012286795430554776\n",
      "Epoch: 137, Iteration: 600, Loss: 0.01194516645045951\n",
      "Epoch: 137, Iteration: 700, Loss: 0.012767378171702148\n",
      "Epoch: 137, Iteration: 800, Loss: 0.013077870498818811\n",
      "Epoch: 137, Train Loss: 0.0005212975394158288, Test Loss: 0.00012752441554484447\n",
      "Epoch: 138, Iteration: 100, Loss: 0.013079051532258745\n",
      "Epoch: 138, Iteration: 200, Loss: 0.013058879452728434\n",
      "Epoch: 138, Iteration: 300, Loss: 0.012135588280216325\n",
      "Epoch: 138, Iteration: 400, Loss: 0.014611034483095864\n",
      "Epoch: 138, Iteration: 500, Loss: 0.012204563237901311\n",
      "Epoch: 138, Iteration: 600, Loss: 0.012077306782884989\n",
      "Epoch: 138, Iteration: 700, Loss: 0.014590133585443255\n",
      "Epoch: 138, Iteration: 800, Loss: 0.011657787337753689\n",
      "Epoch: 138, Train Loss: 0.0005212974802779085, Test Loss: 0.0001277065823999123\n",
      "Epoch: 139, Iteration: 100, Loss: 0.012980391831661109\n",
      "Epoch: 139, Iteration: 200, Loss: 0.013303663850820158\n",
      "Epoch: 139, Iteration: 300, Loss: 0.011864988220622763\n",
      "Epoch: 139, Iteration: 400, Loss: 0.012504840848123422\n",
      "Epoch: 139, Iteration: 500, Loss: 0.012727748217002954\n",
      "Epoch: 139, Iteration: 600, Loss: 0.013314406201970996\n",
      "Epoch: 139, Iteration: 700, Loss: 0.014066281331906794\n",
      "Epoch: 139, Iteration: 800, Loss: 0.014104019574006088\n",
      "Epoch: 139, Train Loss: 0.0005212959861483842, Test Loss: 0.00012752752271098124\n",
      "Epoch: 140, Iteration: 100, Loss: 0.013255415633466328\n",
      "Epoch: 140, Iteration: 200, Loss: 0.012221316639625002\n",
      "Epoch: 140, Iteration: 300, Loss: 0.012864305466791848\n",
      "Epoch: 140, Iteration: 400, Loss: 0.013122372438374441\n",
      "Epoch: 140, Iteration: 500, Loss: 0.0125743581847928\n",
      "Epoch: 140, Iteration: 600, Loss: 0.01325763981367345\n",
      "Epoch: 140, Iteration: 700, Loss: 0.012973702119779773\n",
      "Epoch: 140, Iteration: 800, Loss: 0.014031731301656691\n",
      "Epoch   141: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 140, Train Loss: 0.0005212937536088327, Test Loss: 0.00012795262595002868\n",
      "Epoch: 141, Iteration: 100, Loss: 0.013279987681016792\n",
      "Epoch: 141, Iteration: 200, Loss: 0.013768132266704924\n",
      "Epoch: 141, Iteration: 300, Loss: 0.013186461335862987\n",
      "Epoch: 141, Iteration: 400, Loss: 0.012672457571170526\n",
      "Epoch: 141, Iteration: 500, Loss: 0.012868931924458593\n",
      "Epoch: 141, Iteration: 600, Loss: 0.013807408126012888\n",
      "Epoch: 141, Iteration: 700, Loss: 0.012961268330400344\n",
      "Epoch: 141, Iteration: 800, Loss: 0.01284343210500083\n",
      "Epoch: 141, Train Loss: 0.0005212783111529382, Test Loss: 0.00012770772567212893\n",
      "Epoch: 142, Iteration: 100, Loss: 0.01272780062208767\n",
      "Epoch: 142, Iteration: 200, Loss: 0.012882601946330396\n",
      "Epoch: 142, Iteration: 300, Loss: 0.013064316412055632\n",
      "Epoch: 142, Iteration: 400, Loss: 0.01260303537856089\n",
      "Epoch: 142, Iteration: 500, Loss: 0.01348122891431558\n",
      "Epoch: 142, Iteration: 600, Loss: 0.011818139329989208\n",
      "Epoch: 142, Iteration: 700, Loss: 0.014709676288475748\n",
      "Epoch: 142, Iteration: 800, Loss: 0.013634028702654177\n",
      "Epoch: 142, Train Loss: 0.0005212778714060205, Test Loss: 0.0001275573542036465\n",
      "Epoch: 143, Iteration: 100, Loss: 0.013958236009784741\n",
      "Epoch: 143, Iteration: 200, Loss: 0.012809739797376096\n",
      "Epoch: 143, Iteration: 300, Loss: 0.013521298718842445\n",
      "Epoch: 143, Iteration: 400, Loss: 0.011898916760401335\n",
      "Epoch: 143, Iteration: 500, Loss: 0.014068328426219523\n",
      "Epoch: 143, Iteration: 600, Loss: 0.012808216797566274\n",
      "Epoch: 143, Iteration: 700, Loss: 0.013014866421144689\n",
      "Epoch: 143, Iteration: 800, Loss: 0.013682151038665324\n",
      "Epoch: 143, Train Loss: 0.0005212778080487092, Test Loss: 0.0001275078949847282\n",
      "Epoch: 144, Iteration: 100, Loss: 0.01189610549408826\n",
      "Epoch: 144, Iteration: 200, Loss: 0.014568049347872147\n",
      "Epoch: 144, Iteration: 300, Loss: 0.01376998940759222\n",
      "Epoch: 144, Iteration: 400, Loss: 0.013188609653298045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 144, Iteration: 500, Loss: 0.013304330153914634\n",
      "Epoch: 144, Iteration: 600, Loss: 0.013254876685095951\n",
      "Epoch: 144, Iteration: 700, Loss: 0.012844438144384185\n",
      "Epoch: 144, Iteration: 800, Loss: 0.012244326080690371\n",
      "Epoch: 144, Train Loss: 0.0005212777171490744, Test Loss: 0.00012753724388870705\n",
      "Epoch: 145, Iteration: 100, Loss: 0.01290389370842604\n",
      "Epoch: 145, Iteration: 200, Loss: 0.012256680543941911\n",
      "Epoch: 145, Iteration: 300, Loss: 0.011319936824293109\n",
      "Epoch: 145, Iteration: 400, Loss: 0.012522560489742318\n",
      "Epoch: 145, Iteration: 500, Loss: 0.013148885722330306\n",
      "Epoch: 145, Iteration: 600, Loss: 0.013671619613887742\n",
      "Epoch: 145, Iteration: 700, Loss: 0.015110422951693181\n",
      "Epoch: 145, Iteration: 800, Loss: 0.013583983323769644\n",
      "Epoch: 145, Train Loss: 0.0005212776542901163, Test Loss: 0.0001275697008724703\n",
      "Epoch: 146, Iteration: 100, Loss: 0.014437292909860844\n",
      "Epoch: 146, Iteration: 200, Loss: 0.013505887221981538\n",
      "Epoch: 146, Iteration: 300, Loss: 0.01262833871805924\n",
      "Epoch: 146, Iteration: 400, Loss: 0.01248304817272583\n",
      "Epoch: 146, Iteration: 500, Loss: 0.013347909862204688\n",
      "Epoch: 146, Iteration: 600, Loss: 0.012449684083549073\n",
      "Epoch: 146, Iteration: 700, Loss: 0.01269586803027778\n",
      "Epoch: 146, Iteration: 800, Loss: 0.013007729998207651\n",
      "Epoch: 146, Train Loss: 0.0005212776228606373, Test Loss: 0.00012752354204785993\n",
      "Epoch: 147, Iteration: 100, Loss: 0.013079707645374583\n",
      "Epoch: 147, Iteration: 200, Loss: 0.012641466350032715\n",
      "Epoch: 147, Iteration: 300, Loss: 0.013514418238628423\n",
      "Epoch: 147, Iteration: 400, Loss: 0.012216520106449025\n",
      "Epoch: 147, Iteration: 500, Loss: 0.014212002828571713\n",
      "Epoch: 147, Iteration: 600, Loss: 0.012786025879904628\n",
      "Epoch: 147, Iteration: 700, Loss: 0.013401946034719003\n",
      "Epoch: 147, Iteration: 800, Loss: 0.01220689715773915\n",
      "Epoch: 147, Train Loss: 0.0005212777942609357, Test Loss: 0.00012759336562559846\n",
      "Epoch: 148, Iteration: 100, Loss: 0.012712329458736349\n",
      "Epoch: 148, Iteration: 200, Loss: 0.01289868553794804\n",
      "Epoch: 148, Iteration: 300, Loss: 0.013051436872046907\n",
      "Epoch: 148, Iteration: 400, Loss: 0.014913866962160682\n",
      "Epoch: 148, Iteration: 500, Loss: 0.01354126575824921\n",
      "Epoch: 148, Iteration: 600, Loss: 0.012917929034301778\n",
      "Epoch: 148, Iteration: 700, Loss: 0.01222769796368084\n",
      "Epoch: 148, Iteration: 800, Loss: 0.01208895481977379\n",
      "Epoch: 148, Train Loss: 0.0005212774591183675, Test Loss: 0.00012751930802194062\n",
      "Epoch: 149, Iteration: 100, Loss: 0.013528540399420308\n",
      "Epoch: 149, Iteration: 200, Loss: 0.012399518171150703\n",
      "Epoch: 149, Iteration: 300, Loss: 0.013429157072096132\n",
      "Epoch: 149, Iteration: 400, Loss: 0.012619042932783486\n",
      "Epoch: 149, Iteration: 500, Loss: 0.014265769503253978\n",
      "Epoch: 149, Iteration: 600, Loss: 0.011028265722416108\n",
      "Epoch: 149, Iteration: 700, Loss: 0.012964889345312258\n",
      "Epoch: 149, Iteration: 800, Loss: 0.013896435659262352\n",
      "Epoch: 149, Train Loss: 0.0005212775138375555, Test Loss: 0.000127966901578209\n",
      "Epoch: 150, Iteration: 100, Loss: 0.012353405541944085\n",
      "Epoch: 150, Iteration: 200, Loss: 0.012869013906311011\n",
      "Epoch: 150, Iteration: 300, Loss: 0.013600390557257924\n",
      "Epoch: 150, Iteration: 400, Loss: 0.012948343821335584\n",
      "Epoch: 150, Iteration: 500, Loss: 0.014560569394234335\n",
      "Epoch: 150, Iteration: 600, Loss: 0.013429978418571409\n",
      "Epoch: 150, Iteration: 700, Loss: 0.012938688454596559\n",
      "Epoch: 150, Iteration: 800, Loss: 0.013218627016613027\n",
      "Epoch: 150, Train Loss: 0.0005212776346549978, Test Loss: 0.0001275503733708336\n",
      "Epoch: 151, Iteration: 100, Loss: 0.013318041696038563\n",
      "Epoch: 151, Iteration: 200, Loss: 0.012263001059181988\n",
      "Epoch: 151, Iteration: 300, Loss: 0.013273427332023857\n",
      "Epoch: 151, Iteration: 400, Loss: 0.013301633000082802\n",
      "Epoch: 151, Iteration: 500, Loss: 0.013445402059005573\n",
      "Epoch: 151, Iteration: 600, Loss: 0.01321235422801692\n",
      "Epoch: 151, Iteration: 700, Loss: 0.012896089709101943\n",
      "Epoch: 151, Iteration: 800, Loss: 0.013831428124831291\n",
      "Epoch: 151, Train Loss: 0.0005212775215620311, Test Loss: 0.0001277400325352781\n",
      "Epoch: 152, Iteration: 100, Loss: 0.013217481842730194\n",
      "Epoch: 152, Iteration: 200, Loss: 0.01212337121978635\n",
      "Epoch: 152, Iteration: 300, Loss: 0.013723439140449045\n",
      "Epoch: 152, Iteration: 400, Loss: 0.012231226541189244\n",
      "Epoch: 152, Iteration: 500, Loss: 0.013910315898101544\n",
      "Epoch: 152, Iteration: 600, Loss: 0.01374476370983757\n",
      "Epoch: 152, Iteration: 700, Loss: 0.01293208834977122\n",
      "Epoch: 152, Iteration: 800, Loss: 0.013042494414548855\n",
      "Epoch: 152, Train Loss: 0.0005212775688557556, Test Loss: 0.00012751525493147868\n",
      "Epoch: 153, Iteration: 100, Loss: 0.014380940123373875\n",
      "Epoch: 153, Iteration: 200, Loss: 0.013896301996283\n",
      "Epoch: 153, Iteration: 300, Loss: 0.013946989449323155\n",
      "Epoch: 153, Iteration: 400, Loss: 0.011885425581567688\n",
      "Epoch: 153, Iteration: 500, Loss: 0.012915417730255285\n",
      "Epoch: 153, Iteration: 600, Loss: 0.012237604172696592\n",
      "Epoch: 153, Iteration: 700, Loss: 0.013292875471961452\n",
      "Epoch: 153, Iteration: 800, Loss: 0.012904314211482415\n",
      "Epoch: 153, Train Loss: 0.0005212776151029382, Test Loss: 0.0001275763380408742\n",
      "Epoch: 154, Iteration: 100, Loss: 0.013192952104873257\n",
      "Epoch: 154, Iteration: 200, Loss: 0.012786498107743682\n",
      "Epoch: 154, Iteration: 300, Loss: 0.011564855758479098\n",
      "Epoch: 154, Iteration: 400, Loss: 0.013238780178653542\n",
      "Epoch: 154, Iteration: 500, Loss: 0.012797443181625567\n",
      "Epoch: 154, Iteration: 600, Loss: 0.013648742700752337\n",
      "Epoch: 154, Iteration: 700, Loss: 0.01257238708785735\n",
      "Epoch: 154, Iteration: 800, Loss: 0.015040068989037536\n",
      "Epoch: 154, Train Loss: 0.0005212774045154618, Test Loss: 0.000127537036822927\n",
      "Epoch: 155, Iteration: 100, Loss: 0.013103688379487721\n",
      "Epoch: 155, Iteration: 200, Loss: 0.011847091489471495\n",
      "Epoch: 155, Iteration: 300, Loss: 0.012921759214805206\n",
      "Epoch: 155, Iteration: 400, Loss: 0.013773905666312203\n",
      "Epoch: 155, Iteration: 500, Loss: 0.013036234518949641\n",
      "Epoch: 155, Iteration: 600, Loss: 0.012568825648486381\n",
      "Epoch: 155, Iteration: 700, Loss: 0.012598889628861798\n",
      "Epoch: 155, Iteration: 800, Loss: 0.013891357470129151\n",
      "Epoch: 155, Train Loss: 0.0005212775243361976, Test Loss: 0.0001277112532158533\n",
      "Epoch: 156, Iteration: 100, Loss: 0.013325209369213553\n",
      "Epoch: 156, Iteration: 200, Loss: 0.012773489987011999\n",
      "Epoch: 156, Iteration: 300, Loss: 0.013295052107423544\n",
      "Epoch: 156, Iteration: 400, Loss: 0.013185514621000038\n",
      "Epoch: 156, Iteration: 500, Loss: 0.012541654057713458\n",
      "Epoch: 156, Iteration: 600, Loss: 0.013048014752712334\n",
      "Epoch: 156, Iteration: 700, Loss: 0.013672079894604394\n",
      "Epoch: 156, Iteration: 800, Loss: 0.012980372299352894\n",
      "Epoch: 156, Train Loss: 0.0005212776222792252, Test Loss: 0.00012770547786635585\n",
      "Epoch: 157, Iteration: 100, Loss: 0.014180066755216103\n",
      "Epoch: 157, Iteration: 200, Loss: 0.012372313944069901\n",
      "Epoch: 157, Iteration: 300, Loss: 0.013329391142178793\n",
      "Epoch: 157, Iteration: 400, Loss: 0.012287095425563166\n",
      "Epoch: 157, Iteration: 500, Loss: 0.012657764433242846\n",
      "Epoch: 157, Iteration: 600, Loss: 0.013085116916045081\n",
      "Epoch: 157, Iteration: 700, Loss: 0.012553306012705434\n",
      "Epoch: 157, Iteration: 800, Loss: 0.013939500586275244\n",
      "Epoch: 157, Train Loss: 0.0005212775597192792, Test Loss: 0.0001277895047778283\n",
      "Epoch: 158, Iteration: 100, Loss: 0.011773350051953457\n",
      "Epoch: 158, Iteration: 200, Loss: 0.011934931273572147\n",
      "Epoch: 158, Iteration: 300, Loss: 0.012685386933299014\n",
      "Epoch: 158, Iteration: 400, Loss: 0.013048678491031751\n",
      "Epoch: 158, Iteration: 500, Loss: 0.013270352083054604\n",
      "Epoch: 158, Iteration: 600, Loss: 0.013896542306611082\n",
      "Epoch: 158, Iteration: 700, Loss: 0.01419883234120789\n",
      "Epoch: 158, Iteration: 800, Loss: 0.01254292918019928\n",
      "Epoch: 158, Train Loss: 0.0005212776182757873, Test Loss: 0.00012751845841268907\n",
      "Epoch: 159, Iteration: 100, Loss: 0.013913350197981345\n",
      "Epoch: 159, Iteration: 200, Loss: 0.01300643694048631\n",
      "Epoch: 159, Iteration: 300, Loss: 0.01306937069966807\n",
      "Epoch: 159, Iteration: 400, Loss: 0.012008111840259517\n",
      "Epoch: 159, Iteration: 500, Loss: 0.012173130235169083\n",
      "Epoch: 159, Iteration: 600, Loss: 0.013099778192554368\n",
      "Epoch: 159, Iteration: 700, Loss: 0.01339493472551112\n",
      "Epoch: 159, Iteration: 800, Loss: 0.013043988743447699\n",
      "Epoch: 159, Train Loss: 0.0005212773644810831, Test Loss: 0.00012758206034897292\n",
      "Epoch: 160, Iteration: 100, Loss: 0.013067665335256606\n",
      "Epoch: 160, Iteration: 200, Loss: 0.011268789683526848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 160, Iteration: 300, Loss: 0.01521415208117105\n",
      "Epoch: 160, Iteration: 400, Loss: 0.013676128728548065\n",
      "Epoch: 160, Iteration: 500, Loss: 0.012042864229442785\n",
      "Epoch: 160, Iteration: 600, Loss: 0.014157570836687228\n",
      "Epoch: 160, Iteration: 700, Loss: 0.013043976156041026\n",
      "Epoch: 160, Iteration: 800, Loss: 0.012492770347307669\n",
      "Epoch: 160, Train Loss: 0.0005212774281041829, Test Loss: 0.0001276905796786698\n",
      "Epoch: 161, Iteration: 100, Loss: 0.013407214239123277\n",
      "Epoch: 161, Iteration: 200, Loss: 0.013465807580359979\n",
      "Epoch: 161, Iteration: 300, Loss: 0.012664420806686394\n",
      "Epoch: 161, Iteration: 400, Loss: 0.012152014754974516\n",
      "Epoch: 161, Iteration: 500, Loss: 0.012048799053445691\n",
      "Epoch: 161, Iteration: 600, Loss: 0.014236483926652\n",
      "Epoch: 161, Iteration: 700, Loss: 0.013938435760792345\n",
      "Epoch: 161, Iteration: 800, Loss: 0.011977619342360413\n",
      "Epoch: 161, Train Loss: 0.000521277618009999, Test Loss: 0.0001275863120498212\n",
      "Epoch: 162, Iteration: 100, Loss: 0.011755124342016643\n",
      "Epoch: 162, Iteration: 200, Loss: 0.011663076009426732\n",
      "Epoch: 162, Iteration: 300, Loss: 0.013660436219652183\n",
      "Epoch: 162, Iteration: 400, Loss: 0.013483438699040562\n",
      "Epoch: 162, Iteration: 500, Loss: 0.012965001056727488\n",
      "Epoch: 162, Iteration: 600, Loss: 0.01351112277188804\n",
      "Epoch: 162, Iteration: 700, Loss: 0.014926615891454276\n",
      "Epoch: 162, Iteration: 800, Loss: 0.012327598480624147\n",
      "Epoch: 162, Train Loss: 0.0005212775837399064, Test Loss: 0.00012756957447347146\n",
      "Epoch: 163, Iteration: 100, Loss: 0.013207118026912212\n",
      "Epoch: 163, Iteration: 200, Loss: 0.012632991500140633\n",
      "Epoch: 163, Iteration: 300, Loss: 0.013330647561815567\n",
      "Epoch: 163, Iteration: 400, Loss: 0.013762301514361752\n",
      "Epoch: 163, Iteration: 500, Loss: 0.012549612147267908\n",
      "Epoch: 163, Iteration: 600, Loss: 0.012832060056098271\n",
      "Epoch: 163, Iteration: 700, Loss: 0.012703478420007741\n",
      "Epoch: 163, Iteration: 800, Loss: 0.01277624834983726\n",
      "Epoch: 163, Train Loss: 0.0005212773748800545, Test Loss: 0.0001277912166046689\n",
      "Epoch: 164, Iteration: 100, Loss: 0.012591735128808068\n",
      "Epoch: 164, Iteration: 200, Loss: 0.013258486898848787\n",
      "Epoch: 164, Iteration: 300, Loss: 0.014627080472564558\n",
      "Epoch: 164, Iteration: 400, Loss: 0.01292467880921322\n",
      "Epoch: 164, Iteration: 500, Loss: 0.012662760102102766\n",
      "Epoch: 164, Iteration: 600, Loss: 0.012560808489070041\n",
      "Epoch: 164, Iteration: 700, Loss: 0.012670970980252605\n",
      "Epoch: 164, Iteration: 800, Loss: 0.013953428457170958\n",
      "Epoch: 164, Train Loss: 0.0005212772442284412, Test Loss: 0.00012750383908687624\n",
      "Epoch: 165, Iteration: 100, Loss: 0.013532826898881467\n",
      "Epoch: 165, Iteration: 200, Loss: 0.01325549385001068\n",
      "Epoch: 165, Iteration: 300, Loss: 0.013565495712100528\n",
      "Epoch: 165, Iteration: 400, Loss: 0.012864040232670959\n",
      "Epoch: 165, Iteration: 500, Loss: 0.011797059698437806\n",
      "Epoch: 165, Iteration: 600, Loss: 0.012521309778094292\n",
      "Epoch: 165, Iteration: 700, Loss: 0.013122657292115036\n",
      "Epoch: 165, Iteration: 800, Loss: 0.012853369160438888\n",
      "Epoch: 165, Train Loss: 0.0005212775238710678, Test Loss: 0.00012776020584206977\n",
      "Epoch: 166, Iteration: 100, Loss: 0.012959614057763247\n",
      "Epoch: 166, Iteration: 200, Loss: 0.012856773428211454\n",
      "Epoch: 166, Iteration: 300, Loss: 0.012593573599588126\n",
      "Epoch: 166, Iteration: 400, Loss: 0.01348408328340156\n",
      "Epoch: 166, Iteration: 500, Loss: 0.012668028266489273\n",
      "Epoch: 166, Iteration: 600, Loss: 0.01263155139531591\n",
      "Epoch: 166, Iteration: 700, Loss: 0.013628403925395105\n",
      "Epoch: 166, Iteration: 800, Loss: 0.013613769118819619\n",
      "Epoch: 166, Train Loss: 0.0005212775217945959, Test Loss: 0.00012753930371562998\n",
      "Epoch: 167, Iteration: 100, Loss: 0.013564508950366871\n",
      "Epoch: 167, Iteration: 200, Loss: 0.0133772947556281\n",
      "Epoch: 167, Iteration: 300, Loss: 0.014483219649264356\n",
      "Epoch: 167, Iteration: 400, Loss: 0.0120379945874447\n",
      "Epoch: 167, Iteration: 500, Loss: 0.013254966786917066\n",
      "Epoch: 167, Iteration: 600, Loss: 0.013422761650872417\n",
      "Epoch: 167, Iteration: 700, Loss: 0.013173840878152987\n",
      "Epoch: 167, Iteration: 800, Loss: 0.012104160334274638\n",
      "Epoch: 167, Train Loss: 0.0005212774847337251, Test Loss: 0.00012758277985479971\n",
      "Epoch: 168, Iteration: 100, Loss: 0.012500147036917042\n",
      "Epoch: 168, Iteration: 200, Loss: 0.012759047465806361\n",
      "Epoch: 168, Iteration: 300, Loss: 0.011997928522760049\n",
      "Epoch: 168, Iteration: 400, Loss: 0.013384147452597972\n",
      "Epoch: 168, Iteration: 500, Loss: 0.013055232335318578\n",
      "Epoch: 168, Iteration: 600, Loss: 0.012224470407090848\n",
      "Epoch: 168, Iteration: 700, Loss: 0.013300062415510183\n",
      "Epoch: 168, Iteration: 800, Loss: 0.014195443280186737\n",
      "Epoch: 168, Train Loss: 0.0005212776142889613, Test Loss: 0.0001276142461122796\n",
      "Epoch: 169, Iteration: 100, Loss: 0.013945683283964172\n",
      "Epoch: 169, Iteration: 200, Loss: 0.013168567704269662\n",
      "Epoch: 169, Iteration: 300, Loss: 0.012111754094803473\n",
      "Epoch: 169, Iteration: 400, Loss: 0.012628705055249156\n",
      "Epoch: 169, Iteration: 500, Loss: 0.012881323193141725\n",
      "Epoch: 169, Iteration: 600, Loss: 0.012246729344042251\n",
      "Epoch: 169, Iteration: 700, Loss: 0.01231306980116642\n",
      "Epoch: 169, Iteration: 800, Loss: 0.01452686147968052\n",
      "Epoch: 169, Train Loss: 0.0005212773281677422, Test Loss: 0.00012751568519307266\n",
      "Epoch: 170, Iteration: 100, Loss: 0.014678885916509898\n",
      "Epoch: 170, Iteration: 200, Loss: 0.012255632114829496\n",
      "Epoch: 170, Iteration: 300, Loss: 0.012826190901250811\n",
      "Epoch: 170, Iteration: 400, Loss: 0.012817052662285278\n",
      "Epoch: 170, Iteration: 500, Loss: 0.014486382784525631\n",
      "Epoch: 170, Iteration: 600, Loss: 0.012976101890671998\n",
      "Epoch: 170, Iteration: 700, Loss: 0.013788298900180962\n",
      "Epoch: 170, Iteration: 800, Loss: 0.011008823948941426\n",
      "Epoch: 170, Train Loss: 0.0005212777642268458, Test Loss: 0.00012751214369545696\n",
      "Epoch: 171, Iteration: 100, Loss: 0.013779634460661327\n",
      "Epoch: 171, Iteration: 200, Loss: 0.011741756658011582\n",
      "Epoch: 171, Iteration: 300, Loss: 0.012556538531498518\n",
      "Epoch: 171, Iteration: 400, Loss: 0.01409971358225448\n",
      "Epoch: 171, Iteration: 500, Loss: 0.013846376023138873\n",
      "Epoch: 171, Iteration: 600, Loss: 0.013015360171266366\n",
      "Epoch: 171, Iteration: 700, Loss: 0.013307820951013127\n",
      "Epoch: 171, Iteration: 800, Loss: 0.012091358130419394\n",
      "Epoch: 171, Train Loss: 0.0005212774267254054, Test Loss: 0.00012752563406840355\n",
      "Epoch: 172, Iteration: 100, Loss: 0.014015621032740455\n",
      "Epoch: 172, Iteration: 200, Loss: 0.012652380908548366\n",
      "Epoch: 172, Iteration: 300, Loss: 0.013838462764397264\n",
      "Epoch: 172, Iteration: 400, Loss: 0.01308282033278374\n",
      "Epoch: 172, Iteration: 500, Loss: 0.012853162552346475\n",
      "Epoch: 172, Iteration: 600, Loss: 0.01281947750976542\n",
      "Epoch: 172, Iteration: 700, Loss: 0.013254478944872972\n",
      "Epoch: 172, Iteration: 800, Loss: 0.013197829601267586\n",
      "Epoch: 172, Train Loss: 0.0005212776222958369, Test Loss: 0.0001276225032610181\n",
      "Epoch: 173, Iteration: 100, Loss: 0.014202026370185195\n",
      "Epoch: 173, Iteration: 200, Loss: 0.01316138305264758\n",
      "Epoch: 173, Iteration: 300, Loss: 0.011862696774187498\n",
      "Epoch: 173, Iteration: 400, Loss: 0.012722939667582978\n",
      "Epoch: 173, Iteration: 500, Loss: 0.012898038225102937\n",
      "Epoch: 173, Iteration: 600, Loss: 0.013676773491170024\n",
      "Epoch: 173, Iteration: 700, Loss: 0.012976676425751066\n",
      "Epoch: 173, Iteration: 800, Loss: 0.013785830597043969\n",
      "Epoch: 173, Train Loss: 0.0005212775277582233, Test Loss: 0.00012751418553182714\n",
      "Epoch: 174, Iteration: 100, Loss: 0.013208107910031686\n",
      "Epoch: 174, Iteration: 200, Loss: 0.013057114352704957\n",
      "Epoch: 174, Iteration: 300, Loss: 0.013839711657055886\n",
      "Epoch: 174, Iteration: 400, Loss: 0.012377514987747418\n",
      "Epoch: 174, Iteration: 500, Loss: 0.012063443264196394\n",
      "Epoch: 174, Iteration: 600, Loss: 0.013758565117313992\n",
      "Epoch: 174, Iteration: 700, Loss: 0.012525549118436174\n",
      "Epoch: 174, Iteration: 800, Loss: 0.013124556291586487\n"
     ]
    }
   ],
   "source": [
    "train_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "test_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "\n",
    "patience = config[\"patience\"]\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"])):\n",
    "    loss_ae1_train = 0\n",
    "    loss_ae2_train = 0\n",
    "    loss_dyn_train = 0\n",
    "\n",
    "    current_train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    warmup = 1 if epoch <= config[\"warmup\"] else 0\n",
    "\n",
    "    encoder.train()\n",
    "    dynamics.train()\n",
    "    decoder.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i, (x_t, x_tau) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z_t = encoder(x_t)\n",
    "        x_t_pred = decoder(z_t)\n",
    "\n",
    "        z_tau_pred = dynamics(z_t)\n",
    "        z_tau = encoder(x_tau)\n",
    "        x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_ae1 = criterion(x_t, x_t_pred)\n",
    "        loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "        loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "        loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "        # Backward pass\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += loss_total.item()\n",
    "        epoch_train_loss += loss_total.item()\n",
    "\n",
    "        loss_ae1_train += loss_ae1.item()\n",
    "        loss_ae2_train += loss_ae2.item()\n",
    "        loss_dyn_train += loss_dyn.item() * warmup\n",
    "        counter += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Epoch: {}, Iteration: {}, Loss: {}\".format(epoch, i+1, current_train_loss))\n",
    "            current_train_loss = 0\n",
    "        \n",
    "    train_losses['loss_ae1'].append(loss_ae1_train / counter)\n",
    "    train_losses['loss_ae2'].append(loss_ae2_train / counter)\n",
    "    train_losses['loss_dyn'].append(loss_dyn_train / counter)\n",
    "    train_losses['loss_total'].append(epoch_train_loss / counter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_ae1_test = 0\n",
    "        loss_ae2_test = 0\n",
    "        loss_dyn_test = 0\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        encoder.eval()\n",
    "        dynamics.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        counter = 0\n",
    "        for i, (x_t, x_tau) in enumerate(test_loader,0):\n",
    "            # Forward pass\n",
    "            z_t = encoder(x_t)\n",
    "            x_t_pred = decoder(z_t)\n",
    "\n",
    "            z_tau_pred = dynamics(z_t)\n",
    "            z_tau = encoder(x_tau)\n",
    "            x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_ae1 = criterion(x_t, x_t_pred)\n",
    "            loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "            loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "            loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "            epoch_test_loss += loss_total.item()\n",
    "\n",
    "            loss_ae1_test += loss_ae1.item()\n",
    "            loss_ae2_test += loss_ae2.item()\n",
    "            loss_dyn_test += loss_dyn.item() * warmup\n",
    "            counter += 1\n",
    "\n",
    "        test_losses['loss_ae1'].append(loss_ae1_test / counter)\n",
    "        test_losses['loss_ae2'].append(loss_ae2_test / counter)\n",
    "        test_losses['loss_dyn'].append(loss_dyn_test / counter)\n",
    "        test_losses['loss_total'].append(epoch_test_loss / counter)\n",
    "\n",
    "        if epoch >= patience and np.mean(test_losses['loss_total'][-patience:]) >= np.mean(test_losses['loss_total'][-patience:-1]):\n",
    "            break\n",
    "\n",
    "        if epoch >= config[\"warmup\"]:\n",
    "            scheduler.step(epoch_test_loss / counter)\n",
    "        \n",
    "    print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch, epoch_train_loss / counter, epoch_test_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(encoder, os.path.join(config[\"model_dir\"], \"encoder.pt\"))\n",
    "torch.save(dynamics, os.path.join(config[\"model_dir\"], \"dynamics.pt\"))\n",
    "torch.save(decoder, os.path.join(config[\"model_dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_dir doesn't exist, create it\n",
    "if not os.path.exists(config[\"log_dir\"]):\n",
    "    os.makedirs(config[\"log_dir\"])\n",
    "\n",
    "# Save the losses as a pickle file\n",
    "with open(os.path.join(config[\"log_dir\"], \"losses.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": train_losses, \"test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
