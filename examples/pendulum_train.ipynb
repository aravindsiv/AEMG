{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from AEMG.data_utils import DynamicsDataset\n",
    "from AEMG.systems.utils import get_system\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from AEMG.models import *\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting data for:  physics_pendulum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:45<00:00, 22.22it/s]\n"
     ]
    }
   ],
   "source": [
    "system = get_system(\"pendulum\")\n",
    "# config_fname = \"config/pendulum_lqr_1K.txt\"\n",
    "config_fname = \"config/physics_pendulum.txt\"\n",
    "\n",
    "with open(config_fname, 'r') as f:\n",
    "    config = eval(f.read())\n",
    "\n",
    "dataset = DynamicsDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = Encoder(config[\"high_dims\"],config[\"low_dims\"])\n",
    "dynamics = LatentDynamics(config[\"low_dims\"])\n",
    "decoder  = Decoder(config[\"low_dims\"],config[\"high_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(set(list(encoder.parameters()) + list(dynamics.parameters()) + list(decoder.parameters())), \n",
    "    lr=config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.001, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b33e5dca1c149cf9d6ca75fbb6e714e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 7.273164948448539\n",
      "Epoch: 0, Iteration: 200, Loss: 2.9534839782863855\n",
      "Epoch: 0, Iteration: 300, Loss: 2.3458500783890486\n",
      "Epoch: 0, Iteration: 400, Loss: 1.713400412350893\n",
      "Epoch: 0, Iteration: 500, Loss: 0.9405544474720955\n",
      "Epoch: 0, Iteration: 600, Loss: 0.6114334119483829\n",
      "Epoch: 0, Iteration: 700, Loss: 0.5618428718298674\n",
      "Epoch: 0, Iteration: 800, Loss: 0.538589644478634\n",
      "Epoch: 0, Train Loss: 0.07971856984189682, Test Loss: 0.005303990248213029\n",
      "Epoch: 1, Iteration: 100, Loss: 0.4962015822529793\n",
      "Epoch: 1, Iteration: 200, Loss: 0.4459876911714673\n",
      "Epoch: 1, Iteration: 300, Loss: 0.35431585228070617\n",
      "Epoch: 1, Iteration: 400, Loss: 0.2798199006356299\n",
      "Epoch: 1, Iteration: 500, Loss: 0.2338814929826185\n",
      "Epoch: 1, Iteration: 600, Loss: 0.2113340785726905\n",
      "Epoch: 1, Iteration: 700, Loss: 0.197131528344471\n",
      "Epoch: 1, Iteration: 800, Loss: 0.1902435659430921\n",
      "Epoch: 1, Train Loss: 0.01168235778150874, Test Loss: 0.0017935771217209198\n",
      "Epoch: 2, Iteration: 100, Loss: 0.17465478228405118\n",
      "Epoch: 2, Iteration: 200, Loss: 0.16618490952532738\n",
      "Epoch: 2, Iteration: 300, Loss: 0.15683774690842256\n",
      "Epoch: 2, Iteration: 400, Loss: 0.1487850412959233\n",
      "Epoch: 2, Iteration: 500, Loss: 0.1440838286653161\n",
      "Epoch: 2, Iteration: 600, Loss: 0.13438386376947165\n",
      "Epoch: 2, Iteration: 700, Loss: 0.1286787151475437\n",
      "Epoch: 2, Iteration: 800, Loss: 0.12237242463743314\n",
      "Epoch: 2, Train Loss: 0.0057974175307556365, Test Loss: 0.0011597115335629797\n",
      "Epoch: 3, Iteration: 100, Loss: 0.11189957649912685\n",
      "Epoch: 3, Iteration: 200, Loss: 0.10848537989659235\n",
      "Epoch: 3, Iteration: 300, Loss: 0.10659639514051378\n",
      "Epoch: 3, Iteration: 400, Loss: 0.10065401677275077\n",
      "Epoch: 3, Iteration: 500, Loss: 0.09727781038964167\n",
      "Epoch: 3, Iteration: 600, Loss: 0.09188161906786263\n",
      "Epoch: 3, Iteration: 700, Loss: 0.08892412128625438\n",
      "Epoch: 3, Iteration: 800, Loss: 0.08802511054091156\n",
      "Epoch: 3, Train Loss: 0.003917766771169834, Test Loss: 0.0008019091156933646\n",
      "Epoch: 4, Iteration: 100, Loss: 0.07964071375317872\n",
      "Epoch: 4, Iteration: 200, Loss: 0.07815078645944595\n",
      "Epoch: 4, Iteration: 300, Loss: 0.07873551722150296\n",
      "Epoch: 4, Iteration: 400, Loss: 0.07538418908370659\n",
      "Epoch: 4, Iteration: 500, Loss: 0.07524800731334835\n",
      "Epoch: 4, Iteration: 600, Loss: 0.07080455278628506\n",
      "Epoch: 4, Iteration: 700, Loss: 0.07085389632266015\n",
      "Epoch: 4, Iteration: 800, Loss: 0.073189905640902\n",
      "Epoch: 4, Train Loss: 0.002991839917430285, Test Loss: 0.0007041287619174213\n",
      "Epoch: 5, Iteration: 100, Loss: 0.06995532620931044\n",
      "Epoch: 5, Iteration: 200, Loss: 0.06901834439486265\n",
      "Epoch: 5, Iteration: 300, Loss: 0.07003677979810163\n",
      "Epoch: 5, Iteration: 400, Loss: 0.06780922884354368\n",
      "Epoch: 5, Iteration: 500, Loss: 0.06906196405179799\n",
      "Epoch: 5, Iteration: 600, Loss: 0.06650949758477509\n",
      "Epoch: 5, Iteration: 700, Loss: 0.06519124953774735\n",
      "Epoch: 5, Iteration: 800, Loss: 0.06384583414182998\n",
      "Epoch: 5, Train Loss: 0.0027025310975265547, Test Loss: 0.0006690472401978989\n",
      "Epoch: 6, Iteration: 100, Loss: 0.06498840419226326\n",
      "Epoch: 6, Iteration: 200, Loss: 0.06424833222990856\n",
      "Epoch: 6, Iteration: 300, Loss: 0.0652289651625324\n",
      "Epoch: 6, Iteration: 400, Loss: 0.06381472173961811\n",
      "Epoch: 6, Iteration: 500, Loss: 0.06294494666508399\n",
      "Epoch: 6, Iteration: 600, Loss: 0.06251329006045125\n",
      "Epoch: 6, Iteration: 700, Loss: 0.06321241130353883\n",
      "Epoch: 6, Iteration: 800, Loss: 0.06306442190543748\n",
      "Epoch: 6, Train Loss: 0.002544911362902446, Test Loss: 0.0006212699705839998\n",
      "Epoch: 7, Iteration: 100, Loss: 0.0637028819473926\n",
      "Epoch: 7, Iteration: 200, Loss: 0.059720860910601914\n",
      "Epoch: 7, Iteration: 300, Loss: 0.061373096774332225\n",
      "Epoch: 7, Iteration: 400, Loss: 0.06216506514465436\n",
      "Epoch: 7, Iteration: 500, Loss: 0.0601351564982906\n",
      "Epoch: 7, Iteration: 600, Loss: 0.05998048186302185\n",
      "Epoch: 7, Iteration: 700, Loss: 0.06084245242527686\n",
      "Epoch: 7, Iteration: 800, Loss: 0.05933695848216303\n",
      "Epoch: 7, Train Loss: 0.002439874203308069, Test Loss: 0.0005824747267350863\n",
      "Epoch: 8, Iteration: 100, Loss: 0.058932264684699476\n",
      "Epoch: 8, Iteration: 200, Loss: 0.06057413507369347\n",
      "Epoch: 8, Iteration: 300, Loss: 0.05925837202812545\n",
      "Epoch: 8, Iteration: 400, Loss: 0.06048431360977702\n",
      "Epoch: 8, Iteration: 500, Loss: 0.057557682652259246\n",
      "Epoch: 8, Iteration: 600, Loss: 0.0597953183169011\n",
      "Epoch: 8, Iteration: 700, Loss: 0.05813716133707203\n",
      "Epoch: 8, Iteration: 800, Loss: 0.05704077839618549\n",
      "Epoch: 8, Train Loss: 0.0023489769454026156, Test Loss: 0.0005801640932978246\n",
      "Epoch: 9, Iteration: 100, Loss: 0.05902705332846381\n",
      "Epoch: 9, Iteration: 200, Loss: 0.057615311874542385\n",
      "Epoch: 9, Iteration: 300, Loss: 0.05788297459366731\n",
      "Epoch: 9, Iteration: 400, Loss: 0.05692826517042704\n",
      "Epoch: 9, Iteration: 500, Loss: 0.05523054156219587\n",
      "Epoch: 9, Iteration: 600, Loss: 0.05688564063166268\n",
      "Epoch: 9, Iteration: 700, Loss: 0.05673197234864347\n",
      "Epoch: 9, Iteration: 800, Loss: 0.0558639184164349\n",
      "Epoch: 9, Train Loss: 0.0022789449491355707, Test Loss: 0.0005457930537509358\n",
      "Epoch: 10, Iteration: 100, Loss: 0.05558930244296789\n",
      "Epoch: 10, Iteration: 200, Loss: 0.05471899840631522\n",
      "Epoch: 10, Iteration: 300, Loss: 0.05435475963167846\n",
      "Epoch: 10, Iteration: 400, Loss: 0.055082301259972155\n",
      "Epoch: 10, Iteration: 500, Loss: 0.05512331647332758\n",
      "Epoch: 10, Iteration: 600, Loss: 0.05722453159978613\n",
      "Epoch: 10, Iteration: 700, Loss: 0.05501947828452103\n",
      "Epoch: 10, Iteration: 800, Loss: 0.05454958777409047\n",
      "Epoch: 10, Train Loss: 0.0022048164261794707, Test Loss: 0.0005412708850167606\n",
      "Epoch: 11, Iteration: 100, Loss: 0.0537146812712308\n",
      "Epoch: 11, Iteration: 200, Loss: 0.05351562693249434\n",
      "Epoch: 11, Iteration: 300, Loss: 0.05411405870108865\n",
      "Epoch: 11, Iteration: 400, Loss: 0.05322680019889958\n",
      "Epoch: 11, Iteration: 500, Loss: 0.05391158355632797\n",
      "Epoch: 11, Iteration: 600, Loss: 0.05342379043577239\n",
      "Epoch: 11, Iteration: 700, Loss: 0.05301483630319126\n",
      "Epoch: 11, Iteration: 800, Loss: 0.0535087080206722\n",
      "Epoch: 11, Train Loss: 0.0021464262607782084, Test Loss: 0.0005244711700457788\n",
      "Epoch: 12, Iteration: 100, Loss: 0.05294565681833774\n",
      "Epoch: 12, Iteration: 200, Loss: 0.05451088905101642\n",
      "Epoch: 12, Iteration: 300, Loss: 0.053991838765796274\n",
      "Epoch: 12, Iteration: 400, Loss: 0.051943140977527946\n",
      "Epoch: 12, Iteration: 500, Loss: 0.05135675048222765\n",
      "Epoch: 12, Iteration: 600, Loss: 0.052662799396784976\n",
      "Epoch: 12, Iteration: 700, Loss: 0.05176576672238298\n",
      "Epoch: 12, Iteration: 800, Loss: 0.0509547519905027\n",
      "Epoch: 12, Train Loss: 0.00209413293007684, Test Loss: 0.0004973106977072746\n",
      "Epoch: 13, Iteration: 100, Loss: 0.050603477109689265\n",
      "Epoch: 13, Iteration: 200, Loss: 0.0503544605744537\n",
      "Epoch: 13, Iteration: 300, Loss: 0.050141689367592335\n",
      "Epoch: 13, Iteration: 400, Loss: 0.05080712286871858\n",
      "Epoch: 13, Iteration: 500, Loss: 0.05071549490094185\n",
      "Epoch: 13, Iteration: 600, Loss: 0.04913959192344919\n",
      "Epoch: 13, Iteration: 700, Loss: 0.04971464065602049\n",
      "Epoch: 13, Iteration: 800, Loss: 0.05027291062287986\n",
      "Epoch: 13, Train Loss: 0.0020058175102562947, Test Loss: 0.0005034125013832724\n",
      "Epoch: 14, Iteration: 100, Loss: 0.04689316911390051\n",
      "Epoch: 14, Iteration: 200, Loss: 0.04991607737611048\n",
      "Epoch: 14, Iteration: 300, Loss: 0.04849884411669336\n",
      "Epoch: 14, Iteration: 400, Loss: 0.04756521401577629\n",
      "Epoch: 14, Iteration: 500, Loss: 0.04836788825923577\n",
      "Epoch: 14, Iteration: 600, Loss: 0.048102060274686664\n",
      "Epoch: 14, Iteration: 700, Loss: 0.04521975337411277\n",
      "Epoch: 14, Iteration: 800, Loss: 0.04647026746533811\n",
      "Epoch: 14, Train Loss: 0.0018950593372118072, Test Loss: 0.0004541085369633897\n",
      "Epoch: 15, Iteration: 100, Loss: 0.044577873952221125\n",
      "Epoch: 15, Iteration: 200, Loss: 0.04558782631647773\n",
      "Epoch: 15, Iteration: 300, Loss: 0.04282852218602784\n",
      "Epoch: 15, Iteration: 400, Loss: 0.04401331761619076\n",
      "Epoch: 15, Iteration: 500, Loss: 0.04659807446296327\n",
      "Epoch: 15, Iteration: 600, Loss: 0.043586576677626\n",
      "Epoch: 15, Iteration: 700, Loss: 0.046817906899377704\n",
      "Epoch: 15, Iteration: 800, Loss: 0.04263285960769281\n",
      "Epoch: 15, Train Loss: 0.0017814213166727946, Test Loss: 0.0004349946626998518\n",
      "Epoch: 16, Iteration: 100, Loss: 0.043401383503805846\n",
      "Epoch: 16, Iteration: 200, Loss: 0.042129778856178746\n",
      "Epoch: 16, Iteration: 300, Loss: 0.04247274462250061\n",
      "Epoch: 16, Iteration: 400, Loss: 0.042927758157020435\n",
      "Epoch: 16, Iteration: 500, Loss: 0.04093036224367097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Iteration: 600, Loss: 0.04144890184397809\n",
      "Epoch: 16, Iteration: 700, Loss: 0.04374198050936684\n",
      "Epoch: 16, Iteration: 800, Loss: 0.04256481758784503\n",
      "Epoch: 16, Train Loss: 0.0016968153421938728, Test Loss: 0.00041014975709905034\n",
      "Epoch: 17, Iteration: 100, Loss: 0.04186595280771144\n",
      "Epoch: 17, Iteration: 200, Loss: 0.04186607027077116\n",
      "Epoch: 17, Iteration: 300, Loss: 0.04057490234845318\n",
      "Epoch: 17, Iteration: 400, Loss: 0.04273464865400456\n",
      "Epoch: 17, Iteration: 500, Loss: 0.0411005723872222\n",
      "Epoch: 17, Iteration: 600, Loss: 0.042252741259289905\n",
      "Epoch: 17, Iteration: 700, Loss: 0.04097498196642846\n",
      "Epoch: 17, Iteration: 800, Loss: 0.03951478152885102\n",
      "Epoch: 17, Train Loss: 0.001656765508125343, Test Loss: 0.00041173019863274547\n",
      "Epoch: 18, Iteration: 100, Loss: 0.04020504659274593\n",
      "Epoch: 18, Iteration: 200, Loss: 0.040955312084406614\n",
      "Epoch: 18, Iteration: 300, Loss: 0.0400980809063185\n",
      "Epoch: 18, Iteration: 400, Loss: 0.0394077833625488\n",
      "Epoch: 18, Iteration: 500, Loss: 0.04045046941610053\n",
      "Epoch: 18, Iteration: 600, Loss: 0.0409372752183117\n",
      "Epoch: 18, Iteration: 700, Loss: 0.041946227895095944\n",
      "Epoch: 18, Iteration: 800, Loss: 0.04023846794734709\n",
      "Epoch: 18, Train Loss: 0.0016204324376035465, Test Loss: 0.0003916645109544032\n",
      "Epoch: 19, Iteration: 100, Loss: 0.04187954741064459\n",
      "Epoch: 19, Iteration: 200, Loss: 0.03963831558940001\n",
      "Epoch: 19, Iteration: 300, Loss: 0.041069993952987716\n",
      "Epoch: 19, Iteration: 400, Loss: 0.04036274057580158\n",
      "Epoch: 19, Iteration: 500, Loss: 0.04058418778004125\n",
      "Epoch: 19, Iteration: 600, Loss: 0.03914220858132467\n",
      "Epoch: 19, Iteration: 700, Loss: 0.038998477830318734\n",
      "Epoch: 19, Iteration: 800, Loss: 0.03837692705565132\n",
      "Epoch: 19, Train Loss: 0.0015996642954473723, Test Loss: 0.00038616870522439\n",
      "Epoch: 20, Iteration: 100, Loss: 0.03949301311513409\n",
      "Epoch: 20, Iteration: 200, Loss: 0.03923103175475262\n",
      "Epoch: 20, Iteration: 300, Loss: 0.03910163085674867\n",
      "Epoch: 20, Iteration: 400, Loss: 0.03860751652973704\n",
      "Epoch: 20, Iteration: 500, Loss: 0.037957961845677346\n",
      "Epoch: 20, Iteration: 600, Loss: 0.03872565977508202\n",
      "Epoch: 20, Iteration: 700, Loss: 0.038854913582326844\n",
      "Epoch: 20, Iteration: 800, Loss: 0.0410155865829438\n",
      "Epoch: 20, Train Loss: 0.0015646555746126542, Test Loss: 0.00041462774393856867\n",
      "Epoch: 21, Iteration: 100, Loss: 0.0389335440704599\n",
      "Epoch: 21, Iteration: 200, Loss: 0.039902525139041245\n",
      "Epoch: 21, Iteration: 300, Loss: 0.04017198388464749\n",
      "Epoch: 21, Iteration: 400, Loss: 0.03856466125580482\n",
      "Epoch: 21, Iteration: 500, Loss: 0.039270046167075634\n",
      "Epoch: 21, Iteration: 600, Loss: 0.03906544437631965\n",
      "Epoch: 21, Iteration: 700, Loss: 0.03771082001912873\n",
      "Epoch: 21, Iteration: 800, Loss: 0.039529894856968895\n",
      "Epoch: 21, Train Loss: 0.0015653237878974777, Test Loss: 0.00037746691179337314\n",
      "Epoch: 22, Iteration: 100, Loss: 0.039074547588825226\n",
      "Epoch: 22, Iteration: 200, Loss: 0.03752625739434734\n",
      "Epoch: 22, Iteration: 300, Loss: 0.039291505701839924\n",
      "Epoch: 22, Iteration: 400, Loss: 0.03924092229863163\n",
      "Epoch: 22, Iteration: 500, Loss: 0.036938503821147606\n",
      "Epoch: 22, Iteration: 600, Loss: 0.03904095193138346\n",
      "Epoch: 22, Iteration: 700, Loss: 0.03745642339345068\n",
      "Epoch: 22, Iteration: 800, Loss: 0.03709624594193883\n",
      "Epoch: 22, Train Loss: 0.0015365741290618807, Test Loss: 0.0003705877752508968\n",
      "Epoch: 23, Iteration: 100, Loss: 0.040487646707333624\n",
      "Epoch: 23, Iteration: 200, Loss: 0.04050303026451729\n",
      "Epoch: 23, Iteration: 300, Loss: 0.03655947381048463\n",
      "Epoch: 23, Iteration: 400, Loss: 0.03828815635642968\n",
      "Epoch: 23, Iteration: 500, Loss: 0.03796703640546184\n",
      "Epoch: 23, Iteration: 600, Loss: 0.037367576209362596\n",
      "Epoch: 23, Iteration: 700, Loss: 0.03942760042264126\n",
      "Epoch: 23, Iteration: 800, Loss: 0.03672538294631522\n",
      "Epoch: 23, Train Loss: 0.001533522039264535, Test Loss: 0.0003666024982220008\n",
      "Epoch: 24, Iteration: 100, Loss: 0.03719601521152072\n",
      "Epoch: 24, Iteration: 200, Loss: 0.03900876158149913\n",
      "Epoch: 24, Iteration: 300, Loss: 0.037970839359331876\n",
      "Epoch: 24, Iteration: 400, Loss: 0.035792837908957154\n",
      "Epoch: 24, Iteration: 500, Loss: 0.03747891841339879\n",
      "Epoch: 24, Iteration: 600, Loss: 0.03809526245458983\n",
      "Epoch: 24, Iteration: 700, Loss: 0.03724166320171207\n",
      "Epoch: 24, Iteration: 800, Loss: 0.03600921281031333\n",
      "Epoch: 24, Train Loss: 0.0014974564522577767, Test Loss: 0.0003760343505105021\n",
      "Epoch: 25, Iteration: 100, Loss: 0.03707818774273619\n",
      "Epoch: 25, Iteration: 200, Loss: 0.036722171265864745\n",
      "Epoch: 25, Iteration: 300, Loss: 0.03591026953654364\n",
      "Epoch: 25, Iteration: 400, Loss: 0.03717825055355206\n",
      "Epoch: 25, Iteration: 500, Loss: 0.036555185331963\n",
      "Epoch: 25, Iteration: 600, Loss: 0.03687306062784046\n",
      "Epoch: 25, Iteration: 700, Loss: 0.03809770167572424\n",
      "Epoch: 25, Iteration: 800, Loss: 0.03649927170772571\n",
      "Epoch: 25, Train Loss: 0.001470455391456433, Test Loss: 0.0003745195222213813\n",
      "Epoch: 26, Iteration: 100, Loss: 0.035099121334496886\n",
      "Epoch: 26, Iteration: 200, Loss: 0.03742352550034411\n",
      "Epoch: 26, Iteration: 300, Loss: 0.03831529621675145\n",
      "Epoch: 26, Iteration: 400, Loss: 0.037614881235640496\n",
      "Epoch: 26, Iteration: 500, Loss: 0.035064530558884144\n",
      "Epoch: 26, Iteration: 600, Loss: 0.038553023056010716\n",
      "Epoch: 26, Iteration: 700, Loss: 0.03552501025842503\n",
      "Epoch: 26, Iteration: 800, Loss: 0.03663511674676556\n",
      "Epoch: 26, Train Loss: 0.0014659705371858791, Test Loss: 0.0003724567266771235\n",
      "Epoch: 27, Iteration: 100, Loss: 0.03592110247700475\n",
      "Epoch: 27, Iteration: 200, Loss: 0.03618013407685794\n",
      "Epoch: 27, Iteration: 300, Loss: 0.03807144775055349\n",
      "Epoch: 27, Iteration: 400, Loss: 0.03619053040165454\n",
      "Epoch: 27, Iteration: 500, Loss: 0.03865310247056186\n",
      "Epoch: 27, Iteration: 600, Loss: 0.03877868605195545\n",
      "Epoch: 27, Iteration: 700, Loss: 0.03496079525211826\n",
      "Epoch: 27, Iteration: 800, Loss: 0.03386418477748521\n",
      "Epoch: 27, Train Loss: 0.0014634500319365635, Test Loss: 0.00034263918191314703\n",
      "Epoch: 28, Iteration: 100, Loss: 0.0353474928997457\n",
      "Epoch: 28, Iteration: 200, Loss: 0.03647185937734321\n",
      "Epoch: 28, Iteration: 300, Loss: 0.03598307687207125\n",
      "Epoch: 28, Iteration: 400, Loss: 0.039795553690055385\n",
      "Epoch: 28, Iteration: 500, Loss: 0.03520020988071337\n",
      "Epoch: 28, Iteration: 600, Loss: 0.036611685500247404\n",
      "Epoch: 28, Iteration: 700, Loss: 0.03531165233289357\n",
      "Epoch: 28, Iteration: 800, Loss: 0.035341634444193915\n",
      "Epoch: 28, Train Loss: 0.0014575243640389543, Test Loss: 0.0003566340583431307\n",
      "Epoch: 29, Iteration: 100, Loss: 0.03593666121014394\n",
      "Epoch: 29, Iteration: 200, Loss: 0.03531549125909805\n",
      "Epoch: 29, Iteration: 300, Loss: 0.03740117041161284\n",
      "Epoch: 29, Iteration: 400, Loss: 0.034247167466674\n",
      "Epoch: 29, Iteration: 500, Loss: 0.03415780958312098\n",
      "Epoch: 29, Iteration: 600, Loss: 0.03627311528543942\n",
      "Epoch: 29, Iteration: 700, Loss: 0.03665090554568451\n",
      "Epoch: 29, Iteration: 800, Loss: 0.03641226270701736\n",
      "Epoch: 29, Train Loss: 0.0014362652184686318, Test Loss: 0.00037924124720326117\n",
      "Epoch: 30, Iteration: 100, Loss: 0.03654097429534886\n",
      "Epoch: 30, Iteration: 200, Loss: 0.03654914168873802\n",
      "Epoch: 30, Iteration: 300, Loss: 0.03692746604792774\n",
      "Epoch: 30, Iteration: 400, Loss: 0.03548777010291815\n",
      "Epoch: 30, Iteration: 500, Loss: 0.036008834722451866\n",
      "Epoch: 30, Iteration: 600, Loss: 0.03413634224853013\n",
      "Epoch: 30, Iteration: 700, Loss: 0.0359112280420959\n",
      "Epoch: 30, Iteration: 800, Loss: 0.03433988326287363\n",
      "Epoch: 30, Train Loss: 0.0014272293636268477, Test Loss: 0.0003656705436784954\n",
      "Epoch: 31, Iteration: 100, Loss: 0.036143160265055485\n",
      "Epoch: 31, Iteration: 200, Loss: 0.033883218551636674\n",
      "Epoch: 31, Iteration: 300, Loss: 0.03803010896081105\n",
      "Epoch: 31, Iteration: 400, Loss: 0.03497014689492062\n",
      "Epoch: 31, Iteration: 500, Loss: 0.03456037488649599\n",
      "Epoch: 31, Iteration: 600, Loss: 0.036300282983575016\n",
      "Epoch: 31, Iteration: 700, Loss: 0.034870433271862566\n",
      "Epoch: 31, Iteration: 800, Loss: 0.03402982928673737\n",
      "Epoch: 31, Train Loss: 0.001421513939721613, Test Loss: 0.00033682766118331507\n",
      "Epoch: 32, Iteration: 100, Loss: 0.03419693882460706\n",
      "Epoch: 32, Iteration: 200, Loss: 0.03404765228333417\n",
      "Epoch: 32, Iteration: 300, Loss: 0.03499916031432804\n",
      "Epoch: 32, Iteration: 400, Loss: 0.035884290700778365\n",
      "Epoch: 32, Iteration: 500, Loss: 0.03444603946991265\n",
      "Epoch: 32, Iteration: 600, Loss: 0.035486493608914316\n",
      "Epoch: 32, Iteration: 700, Loss: 0.03552805905928835\n",
      "Epoch: 32, Iteration: 800, Loss: 0.03533722407883033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Train Loss: 0.0013995157147828523, Test Loss: 0.00035551542921575467\n",
      "Epoch: 33, Iteration: 100, Loss: 0.03388402357813902\n",
      "Epoch: 33, Iteration: 200, Loss: 0.03590705612441525\n",
      "Epoch: 33, Iteration: 300, Loss: 0.03410803625592962\n",
      "Epoch: 33, Iteration: 400, Loss: 0.035110166456433944\n",
      "Epoch: 33, Iteration: 500, Loss: 0.03457008187251631\n",
      "Epoch: 33, Iteration: 600, Loss: 0.03354499945999123\n",
      "Epoch: 33, Iteration: 700, Loss: 0.03441454845597036\n",
      "Epoch: 33, Iteration: 800, Loss: 0.03452479632687755\n",
      "Epoch: 33, Train Loss: 0.0013802588690922076, Test Loss: 0.0003785430853392431\n",
      "Epoch: 34, Iteration: 100, Loss: 0.03483213212166447\n",
      "Epoch: 34, Iteration: 200, Loss: 0.03726212809851859\n",
      "Epoch: 34, Iteration: 300, Loss: 0.034021387866232544\n",
      "Epoch: 34, Iteration: 400, Loss: 0.03650591065525077\n",
      "Epoch: 34, Iteration: 500, Loss: 0.03429651670739986\n",
      "Epoch: 34, Iteration: 600, Loss: 0.03474533182452433\n",
      "Epoch: 34, Iteration: 700, Loss: 0.0343802310526371\n",
      "Epoch: 34, Iteration: 800, Loss: 0.03396822861395776\n",
      "Epoch: 34, Train Loss: 0.0014022702427257995, Test Loss: 0.0003249723098642077\n",
      "Epoch: 35, Iteration: 100, Loss: 0.033307721561868675\n",
      "Epoch: 35, Iteration: 200, Loss: 0.034427868129569106\n",
      "Epoch: 35, Iteration: 300, Loss: 0.03569206752581522\n",
      "Epoch: 35, Iteration: 400, Loss: 0.03369492225465365\n",
      "Epoch: 35, Iteration: 500, Loss: 0.03498710262647364\n",
      "Epoch: 35, Iteration: 600, Loss: 0.03412799761281349\n",
      "Epoch: 35, Iteration: 700, Loss: 0.03386452060658485\n",
      "Epoch: 35, Iteration: 800, Loss: 0.03469334324472584\n",
      "Epoch: 35, Train Loss: 0.0013695530390741373, Test Loss: 0.00032288103132650325\n",
      "Epoch: 36, Iteration: 100, Loss: 0.03436479163065087\n",
      "Epoch: 36, Iteration: 200, Loss: 0.03572379170509521\n",
      "Epoch: 36, Iteration: 300, Loss: 0.034118046576622874\n",
      "Epoch: 36, Iteration: 400, Loss: 0.03326954517979175\n",
      "Epoch: 36, Iteration: 500, Loss: 0.034679189819144085\n",
      "Epoch: 36, Iteration: 600, Loss: 0.03656595229404047\n",
      "Epoch: 36, Iteration: 700, Loss: 0.0329025170794921\n",
      "Epoch: 36, Iteration: 800, Loss: 0.03317346205585636\n",
      "Epoch: 36, Train Loss: 0.0013666063175490554, Test Loss: 0.0003423118469072506\n",
      "Epoch: 37, Iteration: 100, Loss: 0.033724475200870074\n",
      "Epoch: 37, Iteration: 200, Loss: 0.034338196375756525\n",
      "Epoch: 37, Iteration: 300, Loss: 0.034672639856580645\n",
      "Epoch: 37, Iteration: 400, Loss: 0.03346662022522651\n",
      "Epoch: 37, Iteration: 500, Loss: 0.033786607134970836\n",
      "Epoch: 37, Iteration: 600, Loss: 0.03433983166178223\n",
      "Epoch: 37, Iteration: 700, Loss: 0.034654273389605805\n",
      "Epoch: 37, Iteration: 800, Loss: 0.03415917725942563\n",
      "Epoch: 37, Train Loss: 0.0013712373378950552, Test Loss: 0.0003552758796108363\n",
      "Epoch: 38, Iteration: 100, Loss: 0.034162837808253244\n",
      "Epoch: 38, Iteration: 200, Loss: 0.03419036937702913\n",
      "Epoch: 38, Iteration: 300, Loss: 0.03456598418415524\n",
      "Epoch: 38, Iteration: 400, Loss: 0.03246830996067729\n",
      "Epoch: 38, Iteration: 500, Loss: 0.034112953711883165\n",
      "Epoch: 38, Iteration: 600, Loss: 0.03284992575936485\n",
      "Epoch: 38, Iteration: 700, Loss: 0.03318718739319593\n",
      "Epoch: 38, Iteration: 800, Loss: 0.03382669640996028\n",
      "Epoch: 38, Train Loss: 0.001348759238605499, Test Loss: 0.0003321633655695923\n",
      "Epoch: 39, Iteration: 100, Loss: 0.0344287615880603\n",
      "Epoch: 39, Iteration: 200, Loss: 0.03453588325646706\n",
      "Epoch: 39, Iteration: 300, Loss: 0.033080190667533316\n",
      "Epoch: 39, Iteration: 400, Loss: 0.033181416656589136\n",
      "Epoch: 39, Iteration: 500, Loss: 0.03473280192702077\n",
      "Epoch: 39, Iteration: 600, Loss: 0.03473942227719817\n",
      "Epoch: 39, Iteration: 700, Loss: 0.03300766668689903\n",
      "Epoch: 39, Iteration: 800, Loss: 0.03339640224294271\n",
      "Epoch: 39, Train Loss: 0.0013508414977722618, Test Loss: 0.00030670582708711673\n",
      "Epoch: 40, Iteration: 100, Loss: 0.03391663293587044\n",
      "Epoch: 40, Iteration: 200, Loss: 0.03588316179229878\n",
      "Epoch: 40, Iteration: 300, Loss: 0.03285024702199735\n",
      "Epoch: 40, Iteration: 400, Loss: 0.032392237859312445\n",
      "Epoch: 40, Iteration: 500, Loss: 0.03160663948801812\n",
      "Epoch: 40, Iteration: 600, Loss: 0.034745726734399796\n",
      "Epoch: 40, Iteration: 700, Loss: 0.03411555527418386\n",
      "Epoch: 40, Iteration: 800, Loss: 0.032220388427958824\n",
      "Epoch: 40, Train Loss: 0.0013426847249341373, Test Loss: 0.0003537175069618884\n",
      "Epoch: 41, Iteration: 100, Loss: 0.03251745586749166\n",
      "Epoch: 41, Iteration: 200, Loss: 0.033954423444811255\n",
      "Epoch: 41, Iteration: 300, Loss: 0.033523644582601264\n",
      "Epoch: 41, Iteration: 400, Loss: 0.03278841220890172\n",
      "Epoch: 41, Iteration: 500, Loss: 0.03353216123650782\n",
      "Epoch: 41, Iteration: 600, Loss: 0.034071987436618656\n",
      "Epoch: 41, Iteration: 700, Loss: 0.03185857759672217\n",
      "Epoch: 41, Iteration: 800, Loss: 0.03287246695253998\n",
      "Epoch: 41, Train Loss: 0.0013178188271561963, Test Loss: 0.00035127141819526504\n",
      "Epoch: 42, Iteration: 100, Loss: 0.03195152604894247\n",
      "Epoch: 42, Iteration: 200, Loss: 0.03303946448431816\n",
      "Epoch: 42, Iteration: 300, Loss: 0.032806562070618384\n",
      "Epoch: 42, Iteration: 400, Loss: 0.033541215234436095\n",
      "Epoch: 42, Iteration: 500, Loss: 0.03310701478039846\n",
      "Epoch: 42, Iteration: 600, Loss: 0.03165515710134059\n",
      "Epoch: 42, Iteration: 700, Loss: 0.03338236748822965\n",
      "Epoch: 42, Iteration: 800, Loss: 0.03242650291940663\n",
      "Epoch: 42, Train Loss: 0.0013009651896945084, Test Loss: 0.0003113362139218577\n",
      "Epoch: 43, Iteration: 100, Loss: 0.03270772359974217\n",
      "Epoch: 43, Iteration: 200, Loss: 0.031978167418856174\n",
      "Epoch: 43, Iteration: 300, Loss: 0.031332306694821455\n",
      "Epoch: 43, Iteration: 400, Loss: 0.03280522055865731\n",
      "Epoch: 43, Iteration: 500, Loss: 0.03413375368108973\n",
      "Epoch: 43, Iteration: 600, Loss: 0.03316430056293029\n",
      "Epoch: 43, Iteration: 700, Loss: 0.03166571687324904\n",
      "Epoch: 43, Iteration: 800, Loss: 0.03408438510086853\n",
      "Epoch: 43, Train Loss: 0.0013130728730342827, Test Loss: 0.0003129715368302951\n",
      "Epoch: 44, Iteration: 100, Loss: 0.03150239917158615\n",
      "Epoch: 44, Iteration: 200, Loss: 0.033025259952410124\n",
      "Epoch: 44, Iteration: 300, Loss: 0.032367871899623424\n",
      "Epoch: 44, Iteration: 400, Loss: 0.0341286523907911\n",
      "Epoch: 44, Iteration: 500, Loss: 0.03191131319908891\n",
      "Epoch: 44, Iteration: 600, Loss: 0.03117252528318204\n",
      "Epoch: 44, Iteration: 700, Loss: 0.030929555839975365\n",
      "Epoch: 44, Iteration: 800, Loss: 0.034798022170434706\n",
      "Epoch: 44, Train Loss: 0.0013067201299626113, Test Loss: 0.00031365055507982434\n",
      "Epoch: 45, Iteration: 100, Loss: 0.030190101693733595\n",
      "Epoch: 45, Iteration: 200, Loss: 0.030765980773139745\n",
      "Epoch: 45, Iteration: 300, Loss: 0.03125741932308301\n",
      "Epoch: 45, Iteration: 400, Loss: 0.03265519009437412\n",
      "Epoch: 45, Iteration: 500, Loss: 0.03246164861775469\n",
      "Epoch: 45, Iteration: 600, Loss: 0.03267864341614768\n",
      "Epoch: 45, Iteration: 700, Loss: 0.031192523616482504\n",
      "Epoch: 45, Iteration: 800, Loss: 0.03356604489090387\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 45, Train Loss: 0.0012805852611857136, Test Loss: 0.0003392039065910203\n",
      "Epoch: 46, Iteration: 100, Loss: 0.029205263359472156\n",
      "Epoch: 46, Iteration: 200, Loss: 0.02893826790386811\n",
      "Epoch: 46, Iteration: 300, Loss: 0.02946487336885184\n",
      "Epoch: 46, Iteration: 400, Loss: 0.029469778513885103\n",
      "Epoch: 46, Iteration: 500, Loss: 0.028469947952544317\n",
      "Epoch: 46, Iteration: 600, Loss: 0.028905062121339142\n",
      "Epoch: 46, Iteration: 700, Loss: 0.02970271182130091\n",
      "Epoch: 46, Iteration: 800, Loss: 0.029288581587024964\n",
      "Epoch: 46, Train Loss: 0.0011625579345868342, Test Loss: 0.0002878252993560984\n",
      "Epoch: 47, Iteration: 100, Loss: 0.028527165908599272\n",
      "Epoch: 47, Iteration: 200, Loss: 0.02930315131379757\n",
      "Epoch: 47, Iteration: 300, Loss: 0.028680659903329797\n",
      "Epoch: 47, Iteration: 400, Loss: 0.028796819839044474\n",
      "Epoch: 47, Iteration: 500, Loss: 0.028413219217327423\n",
      "Epoch: 47, Iteration: 600, Loss: 0.028989050435484387\n",
      "Epoch: 47, Iteration: 700, Loss: 0.02864865136507433\n",
      "Epoch: 47, Iteration: 800, Loss: 0.029867436358472332\n",
      "Epoch: 47, Train Loss: 0.0011550503224188092, Test Loss: 0.0002876321444087254\n",
      "Epoch: 48, Iteration: 100, Loss: 0.029490559987607412\n",
      "Epoch: 48, Iteration: 200, Loss: 0.029279448004672304\n",
      "Epoch: 48, Iteration: 300, Loss: 0.02818986766214948\n",
      "Epoch: 48, Iteration: 400, Loss: 0.028853080206317827\n",
      "Epoch: 48, Iteration: 500, Loss: 0.02837321064725984\n",
      "Epoch: 48, Iteration: 600, Loss: 0.0284715507295914\n",
      "Epoch: 48, Iteration: 700, Loss: 0.028962353710085154\n",
      "Epoch: 48, Iteration: 800, Loss: 0.028665559613727964\n",
      "Epoch: 48, Train Loss: 0.0011490486200264772, Test Loss: 0.00028439826611804985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 49, Iteration: 100, Loss: 0.028112271073041484\n",
      "Epoch: 49, Iteration: 200, Loss: 0.028034971823217347\n",
      "Epoch: 49, Iteration: 300, Loss: 0.0288456350681372\n",
      "Epoch: 49, Iteration: 400, Loss: 0.028379967421642505\n",
      "Epoch: 49, Iteration: 500, Loss: 0.0287107680196641\n",
      "Epoch: 49, Iteration: 600, Loss: 0.028640187927521765\n",
      "Epoch: 49, Iteration: 700, Loss: 0.029007053453824483\n",
      "Epoch: 49, Iteration: 800, Loss: 0.0285148919938365\n",
      "Epoch: 49, Train Loss: 0.001141996280285358, Test Loss: 0.0002880741789516422\n",
      "Epoch: 50, Iteration: 100, Loss: 0.0285873246320989\n",
      "Epoch: 50, Iteration: 200, Loss: 0.029067060924717225\n",
      "Epoch: 50, Iteration: 300, Loss: 0.02839294400473591\n",
      "Epoch: 50, Iteration: 400, Loss: 0.02857309217506554\n",
      "Epoch: 50, Iteration: 500, Loss: 0.027671404081047513\n",
      "Epoch: 50, Iteration: 600, Loss: 0.028435753454687074\n",
      "Epoch: 50, Iteration: 700, Loss: 0.028575116797583178\n",
      "Epoch: 50, Iteration: 800, Loss: 0.028241214648005553\n",
      "Epoch: 50, Train Loss: 0.0011384704229792082, Test Loss: 0.00028407757583853233\n",
      "Epoch: 51, Iteration: 100, Loss: 0.028299601501203142\n",
      "Epoch: 51, Iteration: 200, Loss: 0.02811756613664329\n",
      "Epoch: 51, Iteration: 300, Loss: 0.027717662451323122\n",
      "Epoch: 51, Iteration: 400, Loss: 0.029113565062289126\n",
      "Epoch: 51, Iteration: 500, Loss: 0.02834922884358093\n",
      "Epoch: 51, Iteration: 600, Loss: 0.028544001310365275\n",
      "Epoch: 51, Iteration: 700, Loss: 0.027127066874527372\n",
      "Epoch: 51, Iteration: 800, Loss: 0.02871362422592938\n",
      "Epoch: 51, Train Loss: 0.0011303209867875611, Test Loss: 0.00028459292829513105\n",
      "Epoch: 52, Iteration: 100, Loss: 0.027775402588304132\n",
      "Epoch: 52, Iteration: 200, Loss: 0.028057939212885685\n",
      "Epoch: 52, Iteration: 300, Loss: 0.027444687686511315\n",
      "Epoch: 52, Iteration: 400, Loss: 0.028478152278694324\n",
      "Epoch: 52, Iteration: 500, Loss: 0.027678094236762263\n",
      "Epoch: 52, Iteration: 600, Loss: 0.02794346427253913\n",
      "Epoch: 52, Iteration: 700, Loss: 0.027892684112885036\n",
      "Epoch: 52, Iteration: 800, Loss: 0.028708897065371275\n",
      "Epoch: 52, Train Loss: 0.0011223710955776196, Test Loss: 0.00027999589324302214\n",
      "Epoch: 53, Iteration: 100, Loss: 0.027923508241656236\n",
      "Epoch: 53, Iteration: 200, Loss: 0.028301211234065704\n",
      "Epoch: 53, Iteration: 300, Loss: 0.028128980702604167\n",
      "Epoch: 53, Iteration: 400, Loss: 0.027731938040233217\n",
      "Epoch: 53, Iteration: 500, Loss: 0.0282937940210104\n",
      "Epoch: 53, Iteration: 600, Loss: 0.027170733446837403\n",
      "Epoch: 53, Iteration: 700, Loss: 0.028464642236940563\n",
      "Epoch: 53, Iteration: 800, Loss: 0.027447243555798195\n",
      "Epoch: 53, Train Loss: 0.0011165228198617182, Test Loss: 0.00027631750966857876\n",
      "Epoch: 54, Iteration: 100, Loss: 0.027154204493854195\n",
      "Epoch: 54, Iteration: 200, Loss: 0.028041566532920115\n",
      "Epoch: 54, Iteration: 300, Loss: 0.02748679550131783\n",
      "Epoch: 54, Iteration: 400, Loss: 0.027489921398228034\n",
      "Epoch: 54, Iteration: 500, Loss: 0.027993504583719186\n",
      "Epoch: 54, Iteration: 600, Loss: 0.027819222726975568\n",
      "Epoch: 54, Iteration: 700, Loss: 0.02756470991880633\n",
      "Epoch: 54, Iteration: 800, Loss: 0.027063779431045987\n",
      "Epoch: 54, Train Loss: 0.0011055748918429576, Test Loss: 0.000277982969056978\n",
      "Epoch: 55, Iteration: 100, Loss: 0.026846967986784875\n",
      "Epoch: 55, Iteration: 200, Loss: 0.02811029605800286\n",
      "Epoch: 55, Iteration: 300, Loss: 0.027667982503771782\n",
      "Epoch: 55, Iteration: 400, Loss: 0.026890771128819324\n",
      "Epoch: 55, Iteration: 500, Loss: 0.026930682972306386\n",
      "Epoch: 55, Iteration: 600, Loss: 0.027506028782227077\n",
      "Epoch: 55, Iteration: 700, Loss: 0.027617868065135553\n",
      "Epoch: 55, Iteration: 800, Loss: 0.027954001488978975\n",
      "Epoch: 55, Train Loss: 0.0010982844092714717, Test Loss: 0.00027349551696778803\n",
      "Epoch: 56, Iteration: 100, Loss: 0.027174400878720917\n",
      "Epoch: 56, Iteration: 200, Loss: 0.026824519300134853\n",
      "Epoch: 56, Iteration: 300, Loss: 0.026864675091928802\n",
      "Epoch: 56, Iteration: 400, Loss: 0.027597584907198325\n",
      "Epoch: 56, Iteration: 500, Loss: 0.02734409153345041\n",
      "Epoch: 56, Iteration: 600, Loss: 0.027370462034014054\n",
      "Epoch: 56, Iteration: 700, Loss: 0.02773808184429072\n",
      "Epoch: 56, Iteration: 800, Loss: 0.02727118559414521\n",
      "Epoch: 56, Train Loss: 0.0010905201543628964, Test Loss: 0.0002692290516715059\n",
      "Epoch: 57, Iteration: 100, Loss: 0.027260481918347068\n",
      "Epoch: 57, Iteration: 200, Loss: 0.02762629376957193\n",
      "Epoch: 57, Iteration: 300, Loss: 0.026845176209462807\n",
      "Epoch: 57, Iteration: 400, Loss: 0.02724436976131983\n",
      "Epoch: 57, Iteration: 500, Loss: 0.026772656259709038\n",
      "Epoch: 57, Iteration: 600, Loss: 0.025982608800404705\n",
      "Epoch: 57, Iteration: 700, Loss: 0.027008680932340212\n",
      "Epoch: 57, Iteration: 800, Loss: 0.027754066497436725\n",
      "Epoch: 57, Train Loss: 0.001080194116725097, Test Loss: 0.0002686305533266253\n",
      "Epoch: 58, Iteration: 100, Loss: 0.026672501262510195\n",
      "Epoch: 58, Iteration: 200, Loss: 0.026821346051292494\n",
      "Epoch: 58, Iteration: 300, Loss: 0.027094552031485364\n",
      "Epoch: 58, Iteration: 400, Loss: 0.027006547883502208\n",
      "Epoch: 58, Iteration: 500, Loss: 0.026749179800390266\n",
      "Epoch: 58, Iteration: 600, Loss: 0.026648038692655973\n",
      "Epoch: 58, Iteration: 700, Loss: 0.02678625489352271\n",
      "Epoch: 58, Iteration: 800, Loss: 0.02632547789835371\n",
      "Epoch: 58, Train Loss: 0.001070558185225636, Test Loss: 0.00026473813340999186\n",
      "Epoch: 59, Iteration: 100, Loss: 0.026650456871720962\n",
      "Epoch: 59, Iteration: 200, Loss: 0.02638084246427752\n",
      "Epoch: 59, Iteration: 300, Loss: 0.025948321883333847\n",
      "Epoch: 59, Iteration: 400, Loss: 0.026496129808947444\n",
      "Epoch: 59, Iteration: 500, Loss: 0.02648207404126879\n",
      "Epoch: 59, Iteration: 600, Loss: 0.027225868267123587\n",
      "Epoch: 59, Iteration: 700, Loss: 0.026603341073496267\n",
      "Epoch: 59, Iteration: 800, Loss: 0.02581999934045598\n",
      "Epoch: 59, Train Loss: 0.0010593688070681805, Test Loss: 0.0002609849753220939\n",
      "Epoch: 60, Iteration: 100, Loss: 0.027212078857701272\n",
      "Epoch: 60, Iteration: 200, Loss: 0.026342988436226733\n",
      "Epoch: 60, Iteration: 300, Loss: 0.026600433135172352\n",
      "Epoch: 60, Iteration: 400, Loss: 0.025755271402886137\n",
      "Epoch: 60, Iteration: 500, Loss: 0.02615858289937023\n",
      "Epoch: 60, Iteration: 600, Loss: 0.026445037889061496\n",
      "Epoch: 60, Iteration: 700, Loss: 0.025938742008293048\n",
      "Epoch: 60, Iteration: 800, Loss: 0.02545761961664539\n",
      "Epoch: 60, Train Loss: 0.0010500863054445366, Test Loss: 0.0002669871394192019\n",
      "Epoch: 61, Iteration: 100, Loss: 0.026863213963224553\n",
      "Epoch: 61, Iteration: 200, Loss: 0.02631070361530874\n",
      "Epoch: 61, Iteration: 300, Loss: 0.0260679779748898\n",
      "Epoch: 61, Iteration: 400, Loss: 0.026103041498572566\n",
      "Epoch: 61, Iteration: 500, Loss: 0.025343778033857234\n",
      "Epoch: 61, Iteration: 600, Loss: 0.02576857253734488\n",
      "Epoch: 61, Iteration: 700, Loss: 0.02622408558090683\n",
      "Epoch: 61, Iteration: 800, Loss: 0.02559523595846258\n",
      "Epoch: 61, Train Loss: 0.001041611302874699, Test Loss: 0.00025886338346848084\n",
      "Epoch: 62, Iteration: 100, Loss: 0.025676034754724242\n",
      "Epoch: 62, Iteration: 200, Loss: 0.025968213638407178\n",
      "Epoch: 62, Iteration: 300, Loss: 0.025967183173634112\n",
      "Epoch: 62, Iteration: 400, Loss: 0.02617149567231536\n",
      "Epoch: 62, Iteration: 500, Loss: 0.025584190123481676\n",
      "Epoch: 62, Iteration: 600, Loss: 0.026230994146317244\n",
      "Epoch: 62, Iteration: 700, Loss: 0.02514746057568118\n",
      "Epoch: 62, Iteration: 800, Loss: 0.025313110323622823\n",
      "Epoch: 62, Train Loss: 0.0010302030052351315, Test Loss: 0.0002541624702736392\n",
      "Epoch: 63, Iteration: 100, Loss: 0.02529975185461808\n",
      "Epoch: 63, Iteration: 200, Loss: 0.025112546296440996\n",
      "Epoch: 63, Iteration: 300, Loss: 0.024774430741672404\n",
      "Epoch: 63, Iteration: 400, Loss: 0.024974409927381203\n",
      "Epoch: 63, Iteration: 500, Loss: 0.025582369838957675\n",
      "Epoch: 63, Iteration: 600, Loss: 0.02602533678873442\n",
      "Epoch: 63, Iteration: 700, Loss: 0.02617575225303881\n",
      "Epoch: 63, Iteration: 800, Loss: 0.0254949928203132\n",
      "Epoch: 63, Train Loss: 0.0010170384431831873, Test Loss: 0.0002543699970143774\n",
      "Epoch: 64, Iteration: 100, Loss: 0.025450383429415524\n",
      "Epoch: 64, Iteration: 200, Loss: 0.02623056643642485\n",
      "Epoch: 64, Iteration: 300, Loss: 0.024600485179689713\n",
      "Epoch: 64, Iteration: 400, Loss: 0.024980851987493224\n",
      "Epoch: 64, Iteration: 500, Loss: 0.025141249381704256\n",
      "Epoch: 64, Iteration: 600, Loss: 0.025956500918255188\n",
      "Epoch: 64, Iteration: 700, Loss: 0.025273930121329613\n",
      "Epoch: 64, Iteration: 800, Loss: 0.025331277312943712\n",
      "Epoch: 64, Train Loss: 0.0010114920362191756, Test Loss: 0.0002600564976133758\n",
      "Epoch: 65, Iteration: 100, Loss: 0.024989507990540005\n",
      "Epoch: 65, Iteration: 200, Loss: 0.024346061720279977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 65, Iteration: 300, Loss: 0.024621151242172346\n",
      "Epoch: 65, Iteration: 400, Loss: 0.025211987071088515\n",
      "Epoch: 65, Iteration: 500, Loss: 0.02539442510169465\n",
      "Epoch: 65, Iteration: 600, Loss: 0.024914497189456597\n",
      "Epoch: 65, Iteration: 700, Loss: 0.02537338639376685\n",
      "Epoch: 65, Iteration: 800, Loss: 0.024731724566663615\n",
      "Epoch: 65, Train Loss: 0.0009987398180418567, Test Loss: 0.00024626171772962666\n",
      "Epoch: 66, Iteration: 100, Loss: 0.024338172355783172\n",
      "Epoch: 66, Iteration: 200, Loss: 0.025850593781797215\n",
      "Epoch: 66, Iteration: 300, Loss: 0.025177846764563583\n",
      "Epoch: 66, Iteration: 400, Loss: 0.0250244585913606\n",
      "Epoch: 66, Iteration: 500, Loss: 0.02402762095152866\n",
      "Epoch: 66, Iteration: 600, Loss: 0.024454357975628227\n",
      "Epoch: 66, Iteration: 700, Loss: 0.024304263584781438\n",
      "Epoch: 66, Iteration: 800, Loss: 0.024599743279395625\n",
      "Epoch: 66, Train Loss: 0.0009883789294388593, Test Loss: 0.00024361256476489542\n",
      "Epoch: 67, Iteration: 100, Loss: 0.02412051825376693\n",
      "Epoch: 67, Iteration: 200, Loss: 0.024531422954169102\n",
      "Epoch: 67, Iteration: 300, Loss: 0.024862072808900848\n",
      "Epoch: 67, Iteration: 400, Loss: 0.02374950028024614\n",
      "Epoch: 67, Iteration: 500, Loss: 0.024793131597107276\n",
      "Epoch: 67, Iteration: 600, Loss: 0.024574282753746957\n",
      "Epoch: 67, Iteration: 700, Loss: 0.02387305034790188\n",
      "Epoch: 67, Iteration: 800, Loss: 0.024825985645293258\n",
      "Epoch: 67, Train Loss: 0.0009758090519229345, Test Loss: 0.00024282529092853038\n",
      "Epoch: 68, Iteration: 100, Loss: 0.024541900522308424\n",
      "Epoch: 68, Iteration: 200, Loss: 0.024960036273114383\n",
      "Epoch: 68, Iteration: 300, Loss: 0.024476856109686196\n",
      "Epoch: 68, Iteration: 400, Loss: 0.024806828718283214\n",
      "Epoch: 68, Iteration: 500, Loss: 0.024350642881472595\n",
      "Epoch: 68, Iteration: 600, Loss: 0.023652672811294906\n",
      "Epoch: 68, Iteration: 700, Loss: 0.023750919674057513\n",
      "Epoch: 68, Iteration: 800, Loss: 0.024363946344237775\n",
      "Epoch: 68, Train Loss: 0.0009737251639475591, Test Loss: 0.00023845005898483202\n",
      "Epoch: 69, Iteration: 100, Loss: 0.0243784096383024\n",
      "Epoch: 69, Iteration: 200, Loss: 0.024266065622214228\n",
      "Epoch: 69, Iteration: 300, Loss: 0.02411910131922923\n",
      "Epoch: 69, Iteration: 400, Loss: 0.02416543553408701\n",
      "Epoch: 69, Iteration: 500, Loss: 0.023480522926547565\n",
      "Epoch: 69, Iteration: 600, Loss: 0.024124765128362924\n",
      "Epoch: 69, Iteration: 700, Loss: 0.023698664663243107\n",
      "Epoch: 69, Iteration: 800, Loss: 0.024148997705196962\n",
      "Epoch: 69, Train Loss: 0.0009577328681624988, Test Loss: 0.00024022628996609217\n",
      "Epoch: 70, Iteration: 100, Loss: 0.024559833880630322\n",
      "Epoch: 70, Iteration: 200, Loss: 0.023482806151150726\n",
      "Epoch: 70, Iteration: 300, Loss: 0.02307278871012386\n",
      "Epoch: 70, Iteration: 400, Loss: 0.02379211706283968\n",
      "Epoch: 70, Iteration: 500, Loss: 0.02474471874302253\n",
      "Epoch: 70, Iteration: 600, Loss: 0.02375791108352132\n",
      "Epoch: 70, Iteration: 700, Loss: 0.023102125560399145\n",
      "Epoch: 70, Iteration: 800, Loss: 0.022984360868576914\n",
      "Epoch: 70, Train Loss: 0.0009485159895070592, Test Loss: 0.0002360577229142172\n",
      "Epoch: 71, Iteration: 100, Loss: 0.02366140973754227\n",
      "Epoch: 71, Iteration: 200, Loss: 0.023450325097655877\n",
      "Epoch: 71, Iteration: 300, Loss: 0.02352852538751904\n",
      "Epoch: 71, Iteration: 400, Loss: 0.02308273561357055\n",
      "Epoch: 71, Iteration: 500, Loss: 0.02434072039613966\n",
      "Epoch: 71, Iteration: 600, Loss: 0.023118128738133237\n",
      "Epoch: 71, Iteration: 700, Loss: 0.023459674819605425\n",
      "Epoch: 71, Iteration: 800, Loss: 0.02328987688815687\n",
      "Epoch: 71, Train Loss: 0.0009392845366055721, Test Loss: 0.0002318983314148996\n",
      "Epoch: 72, Iteration: 100, Loss: 0.023198496433906257\n",
      "Epoch: 72, Iteration: 200, Loss: 0.02362369319598656\n",
      "Epoch: 72, Iteration: 300, Loss: 0.022858022726722993\n",
      "Epoch: 72, Iteration: 400, Loss: 0.022802023057010956\n",
      "Epoch: 72, Iteration: 500, Loss: 0.023083066364051774\n",
      "Epoch: 72, Iteration: 600, Loss: 0.024155091887223534\n",
      "Epoch: 72, Iteration: 700, Loss: 0.02412461253697984\n",
      "Epoch: 72, Iteration: 800, Loss: 0.023291973702725954\n",
      "Epoch: 72, Train Loss: 0.0009359838474544049, Test Loss: 0.00023089563169608897\n",
      "Epoch: 73, Iteration: 100, Loss: 0.023147566564148292\n",
      "Epoch: 73, Iteration: 200, Loss: 0.023149613771238364\n",
      "Epoch: 73, Iteration: 300, Loss: 0.023843051108997315\n",
      "Epoch: 73, Iteration: 400, Loss: 0.02269484211865347\n",
      "Epoch: 73, Iteration: 500, Loss: 0.023333473509410396\n",
      "Epoch: 73, Iteration: 600, Loss: 0.022641153642325662\n",
      "Epoch: 73, Iteration: 700, Loss: 0.0231065077095991\n",
      "Epoch: 73, Iteration: 800, Loss: 0.023145206563640386\n",
      "Epoch: 73, Train Loss: 0.0009227372933402314, Test Loss: 0.0002304233882489318\n",
      "Epoch: 74, Iteration: 100, Loss: 0.023553092614747584\n",
      "Epoch: 74, Iteration: 200, Loss: 0.02202811121242121\n",
      "Epoch: 74, Iteration: 300, Loss: 0.02402892663667444\n",
      "Epoch: 74, Iteration: 400, Loss: 0.02322765826829709\n",
      "Epoch: 74, Iteration: 500, Loss: 0.023038082625134848\n",
      "Epoch: 74, Iteration: 600, Loss: 0.022954981366638094\n",
      "Epoch: 74, Iteration: 700, Loss: 0.022657763125607744\n",
      "Epoch: 74, Iteration: 800, Loss: 0.021891740150749683\n",
      "Epoch: 74, Train Loss: 0.0009151721667725817, Test Loss: 0.00023168019183497223\n",
      "Epoch: 75, Iteration: 100, Loss: 0.022644524520728737\n",
      "Epoch: 75, Iteration: 200, Loss: 0.022479115024907514\n",
      "Epoch: 75, Iteration: 300, Loss: 0.02305323470500298\n",
      "Epoch: 75, Iteration: 400, Loss: 0.022968441218836233\n",
      "Epoch: 75, Iteration: 500, Loss: 0.022561769525054842\n",
      "Epoch: 75, Iteration: 600, Loss: 0.022982063324889168\n",
      "Epoch: 75, Iteration: 700, Loss: 0.022898320239619352\n",
      "Epoch: 75, Iteration: 800, Loss: 0.0226080506836297\n",
      "Epoch: 75, Train Loss: 0.0009076316701596998, Test Loss: 0.0002285984770206022\n",
      "Epoch: 76, Iteration: 100, Loss: 0.0224955865269294\n",
      "Epoch: 76, Iteration: 200, Loss: 0.022564411658095196\n",
      "Epoch: 76, Iteration: 300, Loss: 0.02236862893914804\n",
      "Epoch: 76, Iteration: 400, Loss: 0.023467238395824097\n",
      "Epoch: 76, Iteration: 500, Loss: 0.021957390767056495\n",
      "Epoch: 76, Iteration: 600, Loss: 0.022471001982921734\n",
      "Epoch: 76, Iteration: 700, Loss: 0.022226287284865975\n",
      "Epoch: 76, Iteration: 800, Loss: 0.02160958095919341\n",
      "Epoch: 76, Train Loss: 0.0008991143711405905, Test Loss: 0.00022761808109602759\n",
      "Epoch: 77, Iteration: 100, Loss: 0.022022416000254452\n",
      "Epoch: 77, Iteration: 200, Loss: 0.022913546170457266\n",
      "Epoch: 77, Iteration: 300, Loss: 0.022504882916109636\n",
      "Epoch: 77, Iteration: 400, Loss: 0.022426410228945315\n",
      "Epoch: 77, Iteration: 500, Loss: 0.021402610087534413\n",
      "Epoch: 77, Iteration: 600, Loss: 0.022124966082628816\n",
      "Epoch: 77, Iteration: 700, Loss: 0.02253011536959093\n",
      "Epoch: 77, Iteration: 800, Loss: 0.022801838102168404\n",
      "Epoch: 77, Train Loss: 0.000891569623826093, Test Loss: 0.0002227690770834779\n",
      "Epoch: 78, Iteration: 100, Loss: 0.023076790967024863\n",
      "Epoch: 78, Iteration: 200, Loss: 0.021924024098552763\n",
      "Epoch: 78, Iteration: 300, Loss: 0.021810042730066925\n",
      "Epoch: 78, Iteration: 400, Loss: 0.0213115515653044\n",
      "Epoch: 78, Iteration: 500, Loss: 0.02226497934316285\n",
      "Epoch: 78, Iteration: 600, Loss: 0.022536797434440814\n",
      "Epoch: 78, Iteration: 700, Loss: 0.021853052428923547\n",
      "Epoch: 78, Iteration: 800, Loss: 0.02142047589586582\n",
      "Epoch: 78, Train Loss: 0.0008817488486016993, Test Loss: 0.00021669361364817427\n",
      "Epoch: 79, Iteration: 100, Loss: 0.021229047197266482\n",
      "Epoch: 79, Iteration: 200, Loss: 0.022688247088808566\n",
      "Epoch: 79, Iteration: 300, Loss: 0.022740883665392175\n",
      "Epoch: 79, Iteration: 400, Loss: 0.02206768569885753\n",
      "Epoch: 79, Iteration: 500, Loss: 0.02159444146673195\n",
      "Epoch: 79, Iteration: 600, Loss: 0.021955520031042397\n",
      "Epoch: 79, Iteration: 700, Loss: 0.02131263008050155\n",
      "Epoch: 79, Iteration: 800, Loss: 0.021341252548154444\n",
      "Epoch: 79, Train Loss: 0.0008736280392461633, Test Loss: 0.00022201745678061268\n",
      "Epoch: 80, Iteration: 100, Loss: 0.021595626312773675\n",
      "Epoch: 80, Iteration: 200, Loss: 0.02128503164567519\n",
      "Epoch: 80, Iteration: 300, Loss: 0.02203741270932369\n",
      "Epoch: 80, Iteration: 400, Loss: 0.021663948718924075\n",
      "Epoch: 80, Iteration: 500, Loss: 0.02186575591622386\n",
      "Epoch: 80, Iteration: 600, Loss: 0.02195256217964925\n",
      "Epoch: 80, Iteration: 700, Loss: 0.02115399285685271\n",
      "Epoch: 80, Iteration: 800, Loss: 0.021536439802730456\n",
      "Epoch: 80, Train Loss: 0.0008642282992121785, Test Loss: 0.00021361271058925973\n",
      "Epoch: 81, Iteration: 100, Loss: 0.020986616640584543\n",
      "Epoch: 81, Iteration: 200, Loss: 0.021568016716628335\n",
      "Epoch: 81, Iteration: 300, Loss: 0.021467453610966913\n",
      "Epoch: 81, Iteration: 400, Loss: 0.022188486414961517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 81, Iteration: 500, Loss: 0.021171055108425207\n",
      "Epoch: 81, Iteration: 600, Loss: 0.02101589234371204\n",
      "Epoch: 81, Iteration: 700, Loss: 0.021150466141989455\n",
      "Epoch: 81, Iteration: 800, Loss: 0.02190155464631971\n",
      "Epoch: 81, Train Loss: 0.0008590485779128642, Test Loss: 0.0002181987918045346\n",
      "Epoch: 82, Iteration: 100, Loss: 0.02110339084174484\n",
      "Epoch: 82, Iteration: 200, Loss: 0.021074074393254705\n",
      "Epoch: 82, Iteration: 300, Loss: 0.02158471454458777\n",
      "Epoch: 82, Iteration: 400, Loss: 0.020950140606146306\n",
      "Epoch: 82, Iteration: 500, Loss: 0.021369026828324422\n",
      "Epoch: 82, Iteration: 600, Loss: 0.02091290373937227\n",
      "Epoch: 82, Iteration: 700, Loss: 0.021240654416033067\n",
      "Epoch: 82, Iteration: 800, Loss: 0.021196158093516715\n",
      "Epoch: 82, Train Loss: 0.000849407178112134, Test Loss: 0.00021584957627759826\n",
      "Epoch: 83, Iteration: 100, Loss: 0.02164810913382098\n",
      "Epoch: 83, Iteration: 200, Loss: 0.02103813945723232\n",
      "Epoch: 83, Iteration: 300, Loss: 0.021115199881023727\n",
      "Epoch: 83, Iteration: 400, Loss: 0.02132117486326024\n",
      "Epoch: 83, Iteration: 500, Loss: 0.02057175853406079\n",
      "Epoch: 83, Iteration: 600, Loss: 0.02168682758929208\n",
      "Epoch: 83, Iteration: 700, Loss: 0.02086165640503168\n",
      "Epoch: 83, Iteration: 800, Loss: 0.020334868531790562\n",
      "Epoch: 83, Train Loss: 0.0008419712221229552, Test Loss: 0.0002099164006646834\n",
      "Epoch: 84, Iteration: 100, Loss: 0.02041968521371018\n",
      "Epoch: 84, Iteration: 200, Loss: 0.021149518128368072\n",
      "Epoch: 84, Iteration: 300, Loss: 0.020848418251262046\n",
      "Epoch: 84, Iteration: 400, Loss: 0.02077820774866268\n",
      "Epoch: 84, Iteration: 500, Loss: 0.020599491937900893\n",
      "Epoch: 84, Iteration: 600, Loss: 0.02123431429208722\n",
      "Epoch: 84, Iteration: 700, Loss: 0.02136593645263929\n",
      "Epoch: 84, Iteration: 800, Loss: 0.02016807277686894\n",
      "Epoch: 84, Train Loss: 0.0008350027553294035, Test Loss: 0.00021255679847297047\n",
      "Epoch: 85, Iteration: 100, Loss: 0.021002002467866987\n",
      "Epoch: 85, Iteration: 200, Loss: 0.021387411950854585\n",
      "Epoch: 85, Iteration: 300, Loss: 0.02054174052318558\n",
      "Epoch: 85, Iteration: 400, Loss: 0.020490460316068493\n",
      "Epoch: 85, Iteration: 500, Loss: 0.02085737833112944\n",
      "Epoch: 85, Iteration: 600, Loss: 0.020598731643985957\n",
      "Epoch: 85, Iteration: 700, Loss: 0.020201133462251164\n",
      "Epoch: 85, Iteration: 800, Loss: 0.02111030042578932\n",
      "Epoch: 85, Train Loss: 0.0008320837159730142, Test Loss: 0.00021107157146180486\n",
      "Epoch: 86, Iteration: 100, Loss: 0.020968758122762665\n",
      "Epoch: 86, Iteration: 200, Loss: 0.020405492949066684\n",
      "Epoch: 86, Iteration: 300, Loss: 0.02023141854442656\n",
      "Epoch: 86, Iteration: 400, Loss: 0.019956472795456648\n",
      "Epoch: 86, Iteration: 500, Loss: 0.020318566195783205\n",
      "Epoch: 86, Iteration: 600, Loss: 0.020521872778772376\n",
      "Epoch: 86, Iteration: 700, Loss: 0.02123706972633954\n",
      "Epoch: 86, Iteration: 800, Loss: 0.02057494273321936\n",
      "Epoch: 86, Train Loss: 0.0008204392700878289, Test Loss: 0.00020275609972800906\n",
      "Epoch: 87, Iteration: 100, Loss: 0.021055009725387208\n",
      "Epoch: 87, Iteration: 200, Loss: 0.020543324179016054\n",
      "Epoch: 87, Iteration: 300, Loss: 0.019822815200313926\n",
      "Epoch: 87, Iteration: 400, Loss: 0.020552722096908838\n",
      "Epoch: 87, Iteration: 500, Loss: 0.02072634374781046\n",
      "Epoch: 87, Iteration: 600, Loss: 0.020246743923053145\n",
      "Epoch: 87, Iteration: 700, Loss: 0.019924494772567414\n",
      "Epoch: 87, Iteration: 800, Loss: 0.02042749003157951\n",
      "Epoch: 87, Train Loss: 0.000818486637671903, Test Loss: 0.00020327597353843465\n",
      "Epoch: 88, Iteration: 100, Loss: 0.020539580495096743\n",
      "Epoch: 88, Iteration: 200, Loss: 0.020765299574122764\n",
      "Epoch: 88, Iteration: 300, Loss: 0.0204954989021644\n",
      "Epoch: 88, Iteration: 400, Loss: 0.020112861064262688\n",
      "Epoch: 88, Iteration: 500, Loss: 0.02035101676301565\n",
      "Epoch: 88, Iteration: 600, Loss: 0.0198307753453264\n",
      "Epoch: 88, Iteration: 700, Loss: 0.020205196662573144\n",
      "Epoch: 88, Iteration: 800, Loss: 0.02036166271136608\n",
      "Epoch: 88, Train Loss: 0.0008124597325694665, Test Loss: 0.00020167909397353088\n",
      "Epoch: 89, Iteration: 100, Loss: 0.019999342184746638\n",
      "Epoch: 89, Iteration: 200, Loss: 0.020771521783899516\n",
      "Epoch: 89, Iteration: 300, Loss: 0.0203623292472912\n",
      "Epoch: 89, Iteration: 400, Loss: 0.021098195487866178\n",
      "Epoch: 89, Iteration: 500, Loss: 0.02053176797926426\n",
      "Epoch: 89, Iteration: 600, Loss: 0.019723339690244757\n",
      "Epoch: 89, Iteration: 700, Loss: 0.019119596923701465\n",
      "Epoch: 89, Iteration: 800, Loss: 0.02029132633469999\n",
      "Epoch: 89, Train Loss: 0.0008087585887937132, Test Loss: 0.0002034284957831571\n",
      "Epoch: 90, Iteration: 100, Loss: 0.019889217292075045\n",
      "Epoch: 90, Iteration: 200, Loss: 0.019709027648787014\n",
      "Epoch: 90, Iteration: 300, Loss: 0.02028481411980465\n",
      "Epoch: 90, Iteration: 400, Loss: 0.02052034964435734\n",
      "Epoch: 90, Iteration: 500, Loss: 0.02003902076103259\n",
      "Epoch: 90, Iteration: 600, Loss: 0.020106177413254045\n",
      "Epoch: 90, Iteration: 700, Loss: 0.020520302874501795\n",
      "Epoch: 90, Iteration: 800, Loss: 0.019889267554390244\n",
      "Epoch: 90, Train Loss: 0.0008050461275844238, Test Loss: 0.0002001131310590118\n",
      "Epoch: 91, Iteration: 100, Loss: 0.019972140100435354\n",
      "Epoch: 91, Iteration: 200, Loss: 0.02008021496294532\n",
      "Epoch: 91, Iteration: 300, Loss: 0.020938312998623587\n",
      "Epoch: 91, Iteration: 400, Loss: 0.020121927722357213\n",
      "Epoch: 91, Iteration: 500, Loss: 0.01964167828555219\n",
      "Epoch: 91, Iteration: 600, Loss: 0.019950678833993152\n",
      "Epoch: 91, Iteration: 700, Loss: 0.019437137263594195\n",
      "Epoch: 91, Iteration: 800, Loss: 0.020314061650424264\n",
      "Epoch: 91, Train Loss: 0.0008012385578737015, Test Loss: 0.0002012967816977826\n",
      "Epoch: 92, Iteration: 100, Loss: 0.020195123623125255\n",
      "Epoch: 92, Iteration: 200, Loss: 0.01946337186382152\n",
      "Epoch: 92, Iteration: 300, Loss: 0.020127265917835757\n",
      "Epoch: 92, Iteration: 400, Loss: 0.020308545223088004\n",
      "Epoch: 92, Iteration: 500, Loss: 0.01956398658512626\n",
      "Epoch: 92, Iteration: 600, Loss: 0.01983700408891309\n",
      "Epoch: 92, Iteration: 700, Loss: 0.01960739542846568\n",
      "Epoch: 92, Iteration: 800, Loss: 0.019570492222555913\n",
      "Epoch: 92, Train Loss: 0.0007952101415179859, Test Loss: 0.0001967722513956503\n",
      "Epoch: 93, Iteration: 100, Loss: 0.0201047609152738\n",
      "Epoch: 93, Iteration: 200, Loss: 0.01994799693056848\n",
      "Epoch: 93, Iteration: 300, Loss: 0.019589371426263824\n",
      "Epoch: 93, Iteration: 400, Loss: 0.020358156238216907\n",
      "Epoch: 93, Iteration: 500, Loss: 0.019799539775704034\n",
      "Epoch: 93, Iteration: 600, Loss: 0.01887181354686618\n",
      "Epoch: 93, Iteration: 700, Loss: 0.01924616286123637\n",
      "Epoch: 93, Iteration: 800, Loss: 0.01983390119858086\n",
      "Epoch: 93, Train Loss: 0.0007908224660773388, Test Loss: 0.00020089688440484384\n",
      "Epoch: 94, Iteration: 100, Loss: 0.01978046201111283\n",
      "Epoch: 94, Iteration: 200, Loss: 0.02004419056174811\n",
      "Epoch: 94, Iteration: 300, Loss: 0.01962545829883311\n",
      "Epoch: 94, Iteration: 400, Loss: 0.01966361005906947\n",
      "Epoch: 94, Iteration: 500, Loss: 0.018985492060892284\n",
      "Epoch: 94, Iteration: 600, Loss: 0.020397960717673413\n",
      "Epoch: 94, Iteration: 700, Loss: 0.019303379085613415\n",
      "Epoch: 94, Iteration: 800, Loss: 0.019467971986159682\n",
      "Epoch: 94, Train Loss: 0.000785197832912404, Test Loss: 0.00019277057087119703\n",
      "Epoch: 95, Iteration: 100, Loss: 0.019317875397973694\n",
      "Epoch: 95, Iteration: 200, Loss: 0.020015720729134046\n",
      "Epoch: 95, Iteration: 300, Loss: 0.01915584494417999\n",
      "Epoch: 95, Iteration: 400, Loss: 0.020353239480755292\n",
      "Epoch: 95, Iteration: 500, Loss: 0.019317605198011734\n",
      "Epoch: 95, Iteration: 600, Loss: 0.019166262005455792\n",
      "Epoch: 95, Iteration: 700, Loss: 0.01914766590925865\n",
      "Epoch: 95, Iteration: 800, Loss: 0.01977476626052521\n",
      "Epoch: 95, Train Loss: 0.0007818495736844743, Test Loss: 0.00019727353577143707\n",
      "Epoch: 96, Iteration: 100, Loss: 0.01944045770505909\n",
      "Epoch: 96, Iteration: 200, Loss: 0.019725282487343065\n",
      "Epoch: 96, Iteration: 300, Loss: 0.020479557395447046\n",
      "Epoch: 96, Iteration: 400, Loss: 0.019257691150414757\n",
      "Epoch: 96, Iteration: 500, Loss: 0.019707250263309106\n",
      "Epoch: 96, Iteration: 600, Loss: 0.019450074774795212\n",
      "Epoch: 96, Iteration: 700, Loss: 0.018712628247158136\n",
      "Epoch: 96, Iteration: 800, Loss: 0.01888982974924147\n",
      "Epoch: 96, Train Loss: 0.0007758314579388359, Test Loss: 0.0001961752937431769\n",
      "Epoch: 97, Iteration: 100, Loss: 0.02050862746546045\n",
      "Epoch: 97, Iteration: 200, Loss: 0.01942236570175737\n",
      "Epoch: 97, Iteration: 300, Loss: 0.019403855985729024\n",
      "Epoch: 97, Iteration: 400, Loss: 0.019297360617201775\n",
      "Epoch: 97, Iteration: 500, Loss: 0.019459764997009188\n",
      "Epoch: 97, Iteration: 600, Loss: 0.018904122553067282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 97, Iteration: 700, Loss: 0.018830656583304517\n",
      "Epoch: 97, Iteration: 800, Loss: 0.01907644262246322\n",
      "Epoch: 97, Train Loss: 0.0007742641514868901, Test Loss: 0.0001913558359527188\n",
      "Epoch: 98, Iteration: 100, Loss: 0.019161990188877098\n",
      "Epoch: 98, Iteration: 200, Loss: 0.01955060221371241\n",
      "Epoch: 98, Iteration: 300, Loss: 0.019013718992937356\n",
      "Epoch: 98, Iteration: 400, Loss: 0.01950902762473561\n",
      "Epoch: 98, Iteration: 500, Loss: 0.01910083316033706\n",
      "Epoch: 98, Iteration: 600, Loss: 0.01864267108612694\n",
      "Epoch: 98, Iteration: 700, Loss: 0.01952875305141788\n",
      "Epoch: 98, Iteration: 800, Loss: 0.018834663103916682\n",
      "Epoch: 98, Train Loss: 0.0007670576953622062, Test Loss: 0.00019098224268833492\n",
      "Epoch: 99, Iteration: 100, Loss: 0.01889586563629564\n",
      "Epoch: 99, Iteration: 200, Loss: 0.018935617335955612\n",
      "Epoch: 99, Iteration: 300, Loss: 0.019798479086603038\n",
      "Epoch: 99, Iteration: 400, Loss: 0.01899462702567689\n",
      "Epoch: 99, Iteration: 500, Loss: 0.01932631977251731\n",
      "Epoch: 99, Iteration: 600, Loss: 0.019074411247856915\n",
      "Epoch: 99, Iteration: 700, Loss: 0.018724920388194732\n",
      "Epoch: 99, Iteration: 800, Loss: 0.018804669001838192\n",
      "Epoch: 99, Train Loss: 0.0007643174678612981, Test Loss: 0.0001893546595546623\n",
      "Epoch: 100, Iteration: 100, Loss: 0.018891151557909325\n",
      "Epoch: 100, Iteration: 200, Loss: 0.019497377434163354\n",
      "Epoch: 100, Iteration: 300, Loss: 0.01931689264893066\n",
      "Epoch: 100, Iteration: 400, Loss: 0.01881348036113195\n",
      "Epoch: 100, Iteration: 500, Loss: 0.01862966950284317\n",
      "Epoch: 100, Iteration: 600, Loss: 0.018722077562415507\n",
      "Epoch: 100, Iteration: 700, Loss: 0.01942935406259494\n",
      "Epoch: 100, Iteration: 800, Loss: 0.01911679477780126\n",
      "Epoch: 100, Train Loss: 0.0007618417084101193, Test Loss: 0.00018917003256608926\n",
      "Epoch: 101, Iteration: 100, Loss: 0.01912154619640205\n",
      "Epoch: 101, Iteration: 200, Loss: 0.019350354952621274\n",
      "Epoch: 101, Iteration: 300, Loss: 0.019373718125279993\n",
      "Epoch: 101, Iteration: 400, Loss: 0.018613118096254766\n",
      "Epoch: 101, Iteration: 500, Loss: 0.018685277049371507\n",
      "Epoch: 101, Iteration: 600, Loss: 0.01981440068630036\n",
      "Epoch: 101, Iteration: 700, Loss: 0.01898955176875461\n",
      "Epoch: 101, Iteration: 800, Loss: 0.018974102960783057\n",
      "Epoch: 101, Train Loss: 0.0007610520590238878, Test Loss: 0.00018876565231089716\n",
      "Epoch: 102, Iteration: 100, Loss: 0.018551346103777178\n",
      "Epoch: 102, Iteration: 200, Loss: 0.019068047535256483\n",
      "Epoch: 102, Iteration: 300, Loss: 0.019160748372087255\n",
      "Epoch: 102, Iteration: 400, Loss: 0.01930230378638953\n",
      "Epoch: 102, Iteration: 500, Loss: 0.018635963650012854\n",
      "Epoch: 102, Iteration: 600, Loss: 0.019224863426643424\n",
      "Epoch: 102, Iteration: 700, Loss: 0.01960773723840248\n",
      "Epoch: 102, Iteration: 800, Loss: 0.018658124055946246\n",
      "Epoch: 102, Train Loss: 0.0007584792010130443, Test Loss: 0.0001950897299872351\n",
      "Epoch: 103, Iteration: 100, Loss: 0.019273158512078226\n",
      "Epoch: 103, Iteration: 200, Loss: 0.019313719785714056\n",
      "Epoch: 103, Iteration: 300, Loss: 0.01862182949844282\n",
      "Epoch: 103, Iteration: 400, Loss: 0.01806239995494252\n",
      "Epoch: 103, Iteration: 500, Loss: 0.019812876038486138\n",
      "Epoch: 103, Iteration: 600, Loss: 0.018875741006922908\n",
      "Epoch: 103, Iteration: 700, Loss: 0.018509580273530446\n",
      "Epoch: 103, Iteration: 800, Loss: 0.018656023326911964\n",
      "Epoch: 103, Train Loss: 0.0007535393194516494, Test Loss: 0.00018917814116819852\n",
      "Epoch: 104, Iteration: 100, Loss: 0.018894394670496695\n",
      "Epoch: 104, Iteration: 200, Loss: 0.018285601086972747\n",
      "Epoch: 104, Iteration: 300, Loss: 0.018545512735727243\n",
      "Epoch: 104, Iteration: 400, Loss: 0.0195524534501601\n",
      "Epoch: 104, Iteration: 500, Loss: 0.018416882361634634\n",
      "Epoch: 104, Iteration: 600, Loss: 0.01923710625123931\n",
      "Epoch: 104, Iteration: 700, Loss: 0.018076910062518436\n",
      "Epoch: 104, Iteration: 800, Loss: 0.018788197252433747\n",
      "Epoch: 104, Train Loss: 0.0007508973057772364, Test Loss: 0.00018471566680443262\n",
      "Epoch: 105, Iteration: 100, Loss: 0.019571699987864122\n",
      "Epoch: 105, Iteration: 200, Loss: 0.018278500872838777\n",
      "Epoch: 105, Iteration: 300, Loss: 0.018320383249374572\n",
      "Epoch: 105, Iteration: 400, Loss: 0.018476423851097934\n",
      "Epoch: 105, Iteration: 500, Loss: 0.018348879733821377\n",
      "Epoch: 105, Iteration: 600, Loss: 0.018940054942504503\n",
      "Epoch: 105, Iteration: 700, Loss: 0.018803867293172516\n",
      "Epoch: 105, Iteration: 800, Loss: 0.01881537734880112\n",
      "Epoch: 105, Train Loss: 0.0007480350602324301, Test Loss: 0.0001862666254537502\n",
      "Epoch: 106, Iteration: 100, Loss: 0.01811475550493924\n",
      "Epoch: 106, Iteration: 200, Loss: 0.019042641404666938\n",
      "Epoch: 106, Iteration: 300, Loss: 0.01883876605279511\n",
      "Epoch: 106, Iteration: 400, Loss: 0.018667361582629383\n",
      "Epoch: 106, Iteration: 500, Loss: 0.019048533096793108\n",
      "Epoch: 106, Iteration: 600, Loss: 0.01894206627184758\n",
      "Epoch: 106, Iteration: 700, Loss: 0.01837847480783239\n",
      "Epoch: 106, Iteration: 800, Loss: 0.01855599272676045\n",
      "Epoch: 106, Train Loss: 0.0007467005556220756, Test Loss: 0.00018832993638392726\n",
      "Epoch: 107, Iteration: 100, Loss: 0.01815856181201525\n",
      "Epoch: 107, Iteration: 200, Loss: 0.018668097036425024\n",
      "Epoch: 107, Iteration: 300, Loss: 0.018586050246085506\n",
      "Epoch: 107, Iteration: 400, Loss: 0.018280747754033655\n",
      "Epoch: 107, Iteration: 500, Loss: 0.019817523658275604\n",
      "Epoch: 107, Iteration: 600, Loss: 0.018385897084954195\n",
      "Epoch: 107, Iteration: 700, Loss: 0.018413272671750747\n",
      "Epoch: 107, Iteration: 800, Loss: 0.01782987674960168\n",
      "Epoch: 107, Train Loss: 0.0007421364375386893, Test Loss: 0.00018789281019531104\n",
      "Epoch: 108, Iteration: 100, Loss: 0.01838309677259531\n",
      "Epoch: 108, Iteration: 200, Loss: 0.01945661986246705\n",
      "Epoch: 108, Iteration: 300, Loss: 0.01773800478258636\n",
      "Epoch: 108, Iteration: 400, Loss: 0.01840899900707882\n",
      "Epoch: 108, Iteration: 500, Loss: 0.018329838567296974\n",
      "Epoch: 108, Iteration: 600, Loss: 0.019074401097896043\n",
      "Epoch: 108, Iteration: 700, Loss: 0.018101418943842873\n",
      "Epoch: 108, Iteration: 800, Loss: 0.01939834453514777\n",
      "Epoch: 108, Train Loss: 0.0007430616868346015, Test Loss: 0.00018497332724021567\n",
      "Epoch: 109, Iteration: 100, Loss: 0.018627419631229714\n",
      "Epoch: 109, Iteration: 200, Loss: 0.018025666126050055\n",
      "Epoch: 109, Iteration: 300, Loss: 0.017969522770727053\n",
      "Epoch: 109, Iteration: 400, Loss: 0.01814458465378266\n",
      "Epoch: 109, Iteration: 500, Loss: 0.019085419669863768\n",
      "Epoch: 109, Iteration: 600, Loss: 0.01835524164926028\n",
      "Epoch: 109, Iteration: 700, Loss: 0.018478026424418204\n",
      "Epoch: 109, Iteration: 800, Loss: 0.018745951834716834\n",
      "Epoch: 109, Train Loss: 0.0007354906552405771, Test Loss: 0.00018361685363026464\n",
      "Epoch: 110, Iteration: 100, Loss: 0.018725391098996624\n",
      "Epoch: 110, Iteration: 200, Loss: 0.018587193662824575\n",
      "Epoch: 110, Iteration: 300, Loss: 0.017221809401235078\n",
      "Epoch: 110, Iteration: 400, Loss: 0.01849657145794481\n",
      "Epoch: 110, Iteration: 500, Loss: 0.018699992855545133\n",
      "Epoch: 110, Iteration: 600, Loss: 0.01890647123218514\n",
      "Epoch: 110, Iteration: 700, Loss: 0.018408359537716024\n",
      "Epoch: 110, Iteration: 800, Loss: 0.018615280016092584\n",
      "Epoch: 110, Train Loss: 0.0007338871516036256, Test Loss: 0.00018270149910023161\n",
      "Epoch: 111, Iteration: 100, Loss: 0.0184162453879253\n",
      "Epoch: 111, Iteration: 200, Loss: 0.017653120652539656\n",
      "Epoch: 111, Iteration: 300, Loss: 0.01813775012124097\n",
      "Epoch: 111, Iteration: 400, Loss: 0.01889647428470198\n",
      "Epoch: 111, Iteration: 500, Loss: 0.018304196128156036\n",
      "Epoch: 111, Iteration: 600, Loss: 0.018267985069542192\n",
      "Epoch: 111, Iteration: 700, Loss: 0.018602123571326956\n",
      "Epoch: 111, Iteration: 800, Loss: 0.01768571358115878\n",
      "Epoch: 111, Train Loss: 0.0007297362069546219, Test Loss: 0.0001826257364312616\n",
      "Epoch: 112, Iteration: 100, Loss: 0.018241837198729627\n",
      "Epoch: 112, Iteration: 200, Loss: 0.018724143141298555\n",
      "Epoch: 112, Iteration: 300, Loss: 0.01800204381288495\n",
      "Epoch: 112, Iteration: 400, Loss: 0.018273354457051028\n",
      "Epoch: 112, Iteration: 500, Loss: 0.0187518044403987\n",
      "Epoch: 112, Iteration: 600, Loss: 0.017704606099869125\n",
      "Epoch: 112, Iteration: 700, Loss: 0.01804879265546333\n",
      "Epoch: 112, Iteration: 800, Loss: 0.017716582253342494\n",
      "Epoch: 112, Train Loss: 0.0007273773081675236, Test Loss: 0.00018708798292678812\n",
      "Epoch: 113, Iteration: 100, Loss: 0.019100488265394233\n",
      "Epoch: 113, Iteration: 200, Loss: 0.017619244747038465\n",
      "Epoch: 113, Iteration: 300, Loss: 0.018012553235166706\n",
      "Epoch: 113, Iteration: 400, Loss: 0.017942155805940274\n",
      "Epoch: 113, Iteration: 500, Loss: 0.017825336573878303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 113, Iteration: 600, Loss: 0.019219859619624913\n",
      "Epoch: 113, Iteration: 700, Loss: 0.018436193677189294\n",
      "Epoch: 113, Iteration: 800, Loss: 0.017291250704147387\n",
      "Epoch: 113, Train Loss: 0.0007262350472998226, Test Loss: 0.0001829627002449921\n",
      "Epoch: 114, Iteration: 100, Loss: 0.017374147100781556\n",
      "Epoch: 114, Iteration: 200, Loss: 0.018560764292487875\n",
      "Epoch: 114, Iteration: 300, Loss: 0.018428877519909292\n",
      "Epoch: 114, Iteration: 400, Loss: 0.018796727061271667\n",
      "Epoch: 114, Iteration: 500, Loss: 0.01886997983092442\n",
      "Epoch: 114, Iteration: 600, Loss: 0.017369550056173466\n",
      "Epoch: 114, Iteration: 700, Loss: 0.01767247996031074\n",
      "Epoch: 114, Iteration: 800, Loss: 0.017874223383842036\n",
      "Epoch: 114, Train Loss: 0.0007244056294707598, Test Loss: 0.00018225929311965915\n",
      "Epoch: 115, Iteration: 100, Loss: 0.017622818602831103\n",
      "Epoch: 115, Iteration: 200, Loss: 0.017404731690476183\n",
      "Epoch: 115, Iteration: 300, Loss: 0.01796006129734451\n",
      "Epoch: 115, Iteration: 400, Loss: 0.0182699997239979\n",
      "Epoch: 115, Iteration: 500, Loss: 0.018267298539285548\n",
      "Epoch: 115, Iteration: 600, Loss: 0.018360565663897432\n",
      "Epoch: 115, Iteration: 700, Loss: 0.018197578443505336\n",
      "Epoch: 115, Iteration: 800, Loss: 0.017665522624156438\n",
      "Epoch: 115, Train Loss: 0.0007193517801928558, Test Loss: 0.00017724127300413034\n",
      "Epoch: 116, Iteration: 100, Loss: 0.01750099156924989\n",
      "Epoch: 116, Iteration: 200, Loss: 0.01750919448386412\n",
      "Epoch: 116, Iteration: 300, Loss: 0.01826904740300961\n",
      "Epoch: 116, Iteration: 400, Loss: 0.01746688489220105\n",
      "Epoch: 116, Iteration: 500, Loss: 0.01836573742912151\n",
      "Epoch: 116, Iteration: 600, Loss: 0.017787655728170648\n",
      "Epoch: 116, Iteration: 700, Loss: 0.017923189399880357\n",
      "Epoch: 116, Iteration: 800, Loss: 0.017776715460058767\n",
      "Epoch: 116, Train Loss: 0.0007158255410833447, Test Loss: 0.00017888738226825757\n",
      "Epoch: 117, Iteration: 100, Loss: 0.017671548906946555\n",
      "Epoch: 117, Iteration: 200, Loss: 0.017580977779289242\n",
      "Epoch: 117, Iteration: 300, Loss: 0.018581311116577126\n",
      "Epoch: 117, Iteration: 400, Loss: 0.019111446468741633\n",
      "Epoch: 117, Iteration: 500, Loss: 0.01784082864469383\n",
      "Epoch: 117, Iteration: 600, Loss: 0.01689650971093215\n",
      "Epoch: 117, Iteration: 700, Loss: 0.018295711604878306\n",
      "Epoch: 117, Iteration: 800, Loss: 0.01727851749456022\n",
      "Epoch: 117, Train Loss: 0.0007139651648593479, Test Loss: 0.00017680631106084188\n",
      "Epoch: 118, Iteration: 100, Loss: 0.01799883601779584\n",
      "Epoch: 118, Iteration: 200, Loss: 0.017730815736285876\n",
      "Epoch: 118, Iteration: 300, Loss: 0.017251441277039703\n",
      "Epoch: 118, Iteration: 400, Loss: 0.018174793920479715\n",
      "Epoch: 118, Iteration: 500, Loss: 0.017614929347473662\n",
      "Epoch: 118, Iteration: 600, Loss: 0.017961834870220628\n",
      "Epoch: 118, Iteration: 700, Loss: 0.018457926998962648\n",
      "Epoch: 118, Iteration: 800, Loss: 0.017720011324854568\n",
      "Epoch: 118, Train Loss: 0.0007137840949758042, Test Loss: 0.0001824855016695211\n",
      "Epoch: 119, Iteration: 100, Loss: 0.01818514906335622\n",
      "Epoch: 119, Iteration: 200, Loss: 0.017524371447507292\n",
      "Epoch: 119, Iteration: 300, Loss: 0.017338972349534743\n",
      "Epoch: 119, Iteration: 400, Loss: 0.017067294858861715\n",
      "Epoch: 119, Iteration: 500, Loss: 0.017959677716135047\n",
      "Epoch: 119, Iteration: 600, Loss: 0.017584572946361732\n",
      "Epoch: 119, Iteration: 700, Loss: 0.017719556140946224\n",
      "Epoch: 119, Iteration: 800, Loss: 0.017952368783880956\n",
      "Epoch: 119, Train Loss: 0.000709433397963538, Test Loss: 0.00017722944457707326\n",
      "Epoch: 120, Iteration: 100, Loss: 0.01797442162205698\n",
      "Epoch: 120, Iteration: 200, Loss: 0.017484694515587762\n",
      "Epoch: 120, Iteration: 300, Loss: 0.01696864469704451\n",
      "Epoch: 120, Iteration: 400, Loss: 0.017440717041608877\n",
      "Epoch: 120, Iteration: 500, Loss: 0.017737644666340202\n",
      "Epoch: 120, Iteration: 600, Loss: 0.017873433345812373\n",
      "Epoch: 120, Iteration: 700, Loss: 0.017803009548515547\n",
      "Epoch: 120, Iteration: 800, Loss: 0.017965006954909768\n",
      "Epoch: 120, Train Loss: 0.0007084037705291328, Test Loss: 0.00017810776599168923\n",
      "Epoch: 121, Iteration: 100, Loss: 0.017100390963605605\n",
      "Epoch: 121, Iteration: 200, Loss: 0.01814398728311062\n",
      "Epoch: 121, Iteration: 300, Loss: 0.017739265094860457\n",
      "Epoch: 121, Iteration: 400, Loss: 0.018358112167334184\n",
      "Epoch: 121, Iteration: 500, Loss: 0.018852920569770504\n",
      "Epoch: 121, Iteration: 600, Loss: 0.017550041106005665\n",
      "Epoch: 121, Iteration: 700, Loss: 0.017650382258580066\n",
      "Epoch: 121, Iteration: 800, Loss: 0.01633596221654443\n",
      "Epoch: 121, Train Loss: 0.0007078699538044841, Test Loss: 0.00017542913698265114\n",
      "Epoch: 122, Iteration: 100, Loss: 0.017410288681276143\n",
      "Epoch: 122, Iteration: 200, Loss: 0.01711352216807427\n",
      "Epoch: 122, Iteration: 300, Loss: 0.017949410306755453\n",
      "Epoch: 122, Iteration: 400, Loss: 0.016904518168303184\n",
      "Epoch: 122, Iteration: 500, Loss: 0.01750412956607761\n",
      "Epoch: 122, Iteration: 600, Loss: 0.017890772680402733\n",
      "Epoch: 122, Iteration: 700, Loss: 0.018204612206318416\n",
      "Epoch: 122, Iteration: 800, Loss: 0.01764663252833998\n",
      "Epoch: 122, Train Loss: 0.0007057223842674566, Test Loss: 0.00018031581657432225\n",
      "Epoch: 123, Iteration: 100, Loss: 0.017445347635657527\n",
      "Epoch: 123, Iteration: 200, Loss: 0.016994796744256746\n",
      "Epoch: 123, Iteration: 300, Loss: 0.018017980641161557\n",
      "Epoch: 123, Iteration: 400, Loss: 0.01780952391709434\n",
      "Epoch: 123, Iteration: 500, Loss: 0.01769617926038336\n",
      "Epoch: 123, Iteration: 600, Loss: 0.01754603613517247\n",
      "Epoch: 123, Iteration: 700, Loss: 0.01742466795258224\n",
      "Epoch: 123, Iteration: 800, Loss: 0.018145385445677675\n",
      "Epoch: 123, Train Loss: 0.0007046381428246043, Test Loss: 0.0001747391536638188\n",
      "Epoch: 124, Iteration: 100, Loss: 0.017679855714959558\n",
      "Epoch: 124, Iteration: 200, Loss: 0.018113263518898748\n",
      "Epoch: 124, Iteration: 300, Loss: 0.01634257604018785\n",
      "Epoch: 124, Iteration: 400, Loss: 0.017146260252047796\n",
      "Epoch: 124, Iteration: 500, Loss: 0.01788553786900593\n",
      "Epoch: 124, Iteration: 600, Loss: 0.017611070732527878\n",
      "Epoch: 124, Iteration: 700, Loss: 0.017422947988961823\n",
      "Epoch: 124, Iteration: 800, Loss: 0.017751149178366177\n",
      "Epoch: 124, Train Loss: 0.0007025257239082275, Test Loss: 0.00017241123513305723\n",
      "Epoch: 125, Iteration: 100, Loss: 0.017814035396440886\n",
      "Epoch: 125, Iteration: 200, Loss: 0.01678642882325221\n",
      "Epoch: 125, Iteration: 300, Loss: 0.017576341626408976\n",
      "Epoch: 125, Iteration: 400, Loss: 0.017831581659265794\n",
      "Epoch: 125, Iteration: 500, Loss: 0.017307980357145425\n",
      "Epoch: 125, Iteration: 600, Loss: 0.017461925439420156\n",
      "Epoch: 125, Iteration: 700, Loss: 0.017267236842599232\n",
      "Epoch: 125, Iteration: 800, Loss: 0.017230903278687038\n",
      "Epoch: 125, Train Loss: 0.0006966608018071974, Test Loss: 0.000175939845433566\n",
      "Epoch: 126, Iteration: 100, Loss: 0.017531812583911233\n",
      "Epoch: 126, Iteration: 200, Loss: 0.01679305920697516\n",
      "Epoch: 126, Iteration: 300, Loss: 0.017834602258517407\n",
      "Epoch: 126, Iteration: 400, Loss: 0.018248991866130382\n",
      "Epoch: 126, Iteration: 500, Loss: 0.016541259952646215\n",
      "Epoch: 126, Iteration: 600, Loss: 0.01735444388759788\n",
      "Epoch: 126, Iteration: 700, Loss: 0.01735377080331091\n",
      "Epoch: 126, Iteration: 800, Loss: 0.017542035573569592\n",
      "Epoch: 126, Train Loss: 0.000697777174147112, Test Loss: 0.0001774246388378935\n",
      "Epoch: 127, Iteration: 100, Loss: 0.017674413829809055\n",
      "Epoch: 127, Iteration: 200, Loss: 0.017260782879020553\n",
      "Epoch: 127, Iteration: 300, Loss: 0.017064337982446887\n",
      "Epoch: 127, Iteration: 400, Loss: 0.017136229362222366\n",
      "Epoch: 127, Iteration: 500, Loss: 0.016835534304846078\n",
      "Epoch: 127, Iteration: 600, Loss: 0.018267994877533056\n",
      "Epoch: 127, Iteration: 700, Loss: 0.01776066210004501\n",
      "Epoch: 127, Iteration: 800, Loss: 0.017963641221285798\n",
      "Epoch: 127, Train Loss: 0.0007001725884507441, Test Loss: 0.0001744705196757037\n",
      "Epoch: 128, Iteration: 100, Loss: 0.017991158871154767\n",
      "Epoch: 128, Iteration: 200, Loss: 0.017145950594567694\n",
      "Epoch: 128, Iteration: 300, Loss: 0.017938257195055485\n",
      "Epoch: 128, Iteration: 400, Loss: 0.01706186539377086\n",
      "Epoch: 128, Iteration: 500, Loss: 0.017719412222504616\n",
      "Epoch: 128, Iteration: 600, Loss: 0.016769115667557344\n",
      "Epoch: 128, Iteration: 700, Loss: 0.018914941276307218\n",
      "Epoch: 128, Iteration: 800, Loss: 0.0164339295370155\n",
      "Epoch: 128, Train Loss: 0.0006999202085831116, Test Loss: 0.00017716103495063243\n",
      "Epoch: 129, Iteration: 100, Loss: 0.017749572842149064\n",
      "Epoch: 129, Iteration: 200, Loss: 0.016817036434076726\n",
      "Epoch: 129, Iteration: 300, Loss: 0.017554747719259467\n",
      "Epoch: 129, Iteration: 400, Loss: 0.01756080850464059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 129, Iteration: 500, Loss: 0.017234546110557858\n",
      "Epoch: 129, Iteration: 600, Loss: 0.017700475378660485\n",
      "Epoch: 129, Iteration: 700, Loss: 0.0168336369752069\n",
      "Epoch: 129, Iteration: 800, Loss: 0.016847785329446197\n",
      "Epoch: 129, Train Loss: 0.0006914126588107293, Test Loss: 0.00017203277703599945\n",
      "Epoch: 130, Iteration: 100, Loss: 0.017752974046743475\n",
      "Epoch: 130, Iteration: 200, Loss: 0.01645668534183642\n",
      "Epoch: 130, Iteration: 300, Loss: 0.016889858670765534\n",
      "Epoch: 130, Iteration: 400, Loss: 0.017772088925994467\n",
      "Epoch: 130, Iteration: 500, Loss: 0.01692995796474861\n",
      "Epoch: 130, Iteration: 600, Loss: 0.018014148765360005\n",
      "Epoch: 130, Iteration: 700, Loss: 0.017675443712505512\n",
      "Epoch: 130, Iteration: 800, Loss: 0.016949675453361124\n",
      "Epoch: 130, Train Loss: 0.0006910775950948884, Test Loss: 0.00017108196344506639\n",
      "Epoch: 131, Iteration: 100, Loss: 0.017117115698056296\n",
      "Epoch: 131, Iteration: 200, Loss: 0.017643254577706102\n",
      "Epoch: 131, Iteration: 300, Loss: 0.01721615593851311\n",
      "Epoch: 131, Iteration: 400, Loss: 0.017592451098607853\n",
      "Epoch: 131, Iteration: 500, Loss: 0.016989272640785202\n",
      "Epoch: 131, Iteration: 600, Loss: 0.01717260802979581\n",
      "Epoch: 131, Iteration: 700, Loss: 0.017501468195405323\n",
      "Epoch: 131, Iteration: 800, Loss: 0.018126696260878816\n",
      "Epoch: 131, Train Loss: 0.0006937941711064436, Test Loss: 0.00017985773871251497\n",
      "Epoch: 132, Iteration: 100, Loss: 0.016914107181946747\n",
      "Epoch: 132, Iteration: 200, Loss: 0.017067092361685354\n",
      "Epoch: 132, Iteration: 300, Loss: 0.017405045400664676\n",
      "Epoch: 132, Iteration: 400, Loss: 0.017432263783121016\n",
      "Epoch: 132, Iteration: 500, Loss: 0.01669450710323872\n",
      "Epoch: 132, Iteration: 600, Loss: 0.018208571025752462\n",
      "Epoch: 132, Iteration: 700, Loss: 0.017109928958234377\n",
      "Epoch: 132, Iteration: 800, Loss: 0.017324618122074753\n",
      "Epoch: 132, Train Loss: 0.0006890997292891291, Test Loss: 0.00017758447006704234\n",
      "Epoch: 133, Iteration: 100, Loss: 0.017090293826186098\n",
      "Epoch: 133, Iteration: 200, Loss: 0.017442897958972026\n",
      "Epoch: 133, Iteration: 300, Loss: 0.016843199300637934\n",
      "Epoch: 133, Iteration: 400, Loss: 0.017320229919278063\n",
      "Epoch: 133, Iteration: 500, Loss: 0.01758025540766539\n",
      "Epoch: 133, Iteration: 600, Loss: 0.017385898623615503\n",
      "Epoch: 133, Iteration: 700, Loss: 0.016754796721215826\n",
      "Epoch: 133, Iteration: 800, Loss: 0.01660272036679089\n",
      "Epoch: 133, Train Loss: 0.0006863232903865978, Test Loss: 0.00016883644634135676\n",
      "Epoch: 134, Iteration: 100, Loss: 0.01655970215506386\n",
      "Epoch: 134, Iteration: 200, Loss: 0.01761017067474313\n",
      "Epoch: 134, Iteration: 300, Loss: 0.017575096193468198\n",
      "Epoch: 134, Iteration: 400, Loss: 0.016935777552134823\n",
      "Epoch: 134, Iteration: 500, Loss: 0.0174056517862482\n",
      "Epoch: 134, Iteration: 600, Loss: 0.01636277624493232\n",
      "Epoch: 134, Iteration: 700, Loss: 0.016879224196600262\n",
      "Epoch: 134, Iteration: 800, Loss: 0.017162535004899837\n",
      "Epoch: 134, Train Loss: 0.0006833513341640221, Test Loss: 0.0001709425106978855\n",
      "Epoch: 135, Iteration: 100, Loss: 0.017437199618143495\n",
      "Epoch: 135, Iteration: 200, Loss: 0.016668641124852\n",
      "Epoch: 135, Iteration: 300, Loss: 0.01694089933153009\n",
      "Epoch: 135, Iteration: 400, Loss: 0.017331648006802425\n",
      "Epoch: 135, Iteration: 500, Loss: 0.017103335172578227\n",
      "Epoch: 135, Iteration: 600, Loss: 0.016827013445436023\n",
      "Epoch: 135, Iteration: 700, Loss: 0.017349143032333814\n",
      "Epoch: 135, Iteration: 800, Loss: 0.01664573580637807\n",
      "Epoch: 135, Train Loss: 0.0006820513035437899, Test Loss: 0.00016865982481735557\n",
      "Epoch: 136, Iteration: 100, Loss: 0.017626142252993304\n",
      "Epoch: 136, Iteration: 200, Loss: 0.016640795714920387\n",
      "Epoch: 136, Iteration: 300, Loss: 0.016579312898102216\n",
      "Epoch: 136, Iteration: 400, Loss: 0.01733102415164467\n",
      "Epoch: 136, Iteration: 500, Loss: 0.017175482054881286\n",
      "Epoch: 136, Iteration: 600, Loss: 0.016486361077113543\n",
      "Epoch: 136, Iteration: 700, Loss: 0.01694447733461857\n",
      "Epoch: 136, Iteration: 800, Loss: 0.01702198638668051\n",
      "Epoch: 136, Train Loss: 0.0006784994380086154, Test Loss: 0.00017338573008835796\n",
      "Epoch: 137, Iteration: 100, Loss: 0.01637493653834099\n",
      "Epoch: 137, Iteration: 200, Loss: 0.016493891962454654\n",
      "Epoch: 137, Iteration: 300, Loss: 0.01704354567482369\n",
      "Epoch: 137, Iteration: 400, Loss: 0.01708776695159031\n",
      "Epoch: 137, Iteration: 500, Loss: 0.018048543090117164\n",
      "Epoch: 137, Iteration: 600, Loss: 0.016897658118978143\n",
      "Epoch: 137, Iteration: 700, Loss: 0.01732000836636871\n",
      "Epoch: 137, Iteration: 800, Loss: 0.017384173625032417\n",
      "Epoch: 137, Train Loss: 0.0006823215940360984, Test Loss: 0.00016776148321977194\n",
      "Epoch: 138, Iteration: 100, Loss: 0.016539205549634062\n",
      "Epoch: 138, Iteration: 200, Loss: 0.017759670692612417\n",
      "Epoch: 138, Iteration: 300, Loss: 0.016696303231583443\n",
      "Epoch: 138, Iteration: 400, Loss: 0.016159536651684903\n",
      "Epoch: 138, Iteration: 500, Loss: 0.016639491899695713\n",
      "Epoch: 138, Iteration: 600, Loss: 0.017569165938766673\n",
      "Epoch: 138, Iteration: 700, Loss: 0.017213986327988096\n",
      "Epoch: 138, Iteration: 800, Loss: 0.01741518530616304\n",
      "Epoch: 138, Train Loss: 0.000681459110369423, Test Loss: 0.0001689499031984642\n",
      "Epoch: 139, Iteration: 100, Loss: 0.01651946341735311\n",
      "Epoch: 139, Iteration: 200, Loss: 0.017469729653385002\n",
      "Epoch: 139, Iteration: 300, Loss: 0.016910717633436434\n",
      "Epoch: 139, Iteration: 400, Loss: 0.01665457985654939\n",
      "Epoch: 139, Iteration: 500, Loss: 0.016257173876510933\n",
      "Epoch: 139, Iteration: 600, Loss: 0.0170463256072253\n",
      "Epoch: 139, Iteration: 700, Loss: 0.017005218607664574\n",
      "Epoch: 139, Iteration: 800, Loss: 0.017103669750213157\n",
      "Epoch: 139, Train Loss: 0.0006739760629743153, Test Loss: 0.00016722347963416803\n",
      "Epoch: 140, Iteration: 100, Loss: 0.01747769531357335\n",
      "Epoch: 140, Iteration: 200, Loss: 0.016284796525724232\n",
      "Epoch: 140, Iteration: 300, Loss: 0.0166416052961722\n",
      "Epoch: 140, Iteration: 400, Loss: 0.01757427384291077\n",
      "Epoch: 140, Iteration: 500, Loss: 0.016997040627757087\n",
      "Epoch: 140, Iteration: 600, Loss: 0.016690818694769405\n",
      "Epoch: 140, Iteration: 700, Loss: 0.01653963619901333\n",
      "Epoch: 140, Iteration: 800, Loss: 0.01694896147819236\n",
      "Epoch: 140, Train Loss: 0.0006762993938964469, Test Loss: 0.00017281327668758762\n",
      "Epoch: 141, Iteration: 100, Loss: 0.017411955348507036\n",
      "Epoch: 141, Iteration: 200, Loss: 0.017105119026382454\n",
      "Epoch: 141, Iteration: 300, Loss: 0.017366097832564265\n",
      "Epoch: 141, Iteration: 400, Loss: 0.0165478884591721\n",
      "Epoch: 141, Iteration: 500, Loss: 0.016589301987551153\n",
      "Epoch: 141, Iteration: 600, Loss: 0.016391751683840994\n",
      "Epoch: 141, Iteration: 700, Loss: 0.016873529442818835\n",
      "Epoch: 141, Iteration: 800, Loss: 0.016277712791634258\n",
      "Epoch: 141, Train Loss: 0.0006719874450802795, Test Loss: 0.00016778249739243301\n",
      "Epoch: 142, Iteration: 100, Loss: 0.016312312858644873\n",
      "Epoch: 142, Iteration: 200, Loss: 0.016772505230619572\n",
      "Epoch: 142, Iteration: 300, Loss: 0.016673483434715308\n",
      "Epoch: 142, Iteration: 400, Loss: 0.01715885059093125\n",
      "Epoch: 142, Iteration: 500, Loss: 0.016156853504071478\n",
      "Epoch: 142, Iteration: 600, Loss: 0.016840731586853508\n",
      "Epoch: 142, Iteration: 700, Loss: 0.016720266197808087\n",
      "Epoch: 142, Iteration: 800, Loss: 0.016837277406011708\n",
      "Epoch: 142, Train Loss: 0.0006698761854217762, Test Loss: 0.00017026934379358745\n",
      "Epoch: 143, Iteration: 100, Loss: 0.016514991730218753\n",
      "Epoch: 143, Iteration: 200, Loss: 0.016714211662474554\n",
      "Epoch: 143, Iteration: 300, Loss: 0.016547907420317642\n",
      "Epoch: 143, Iteration: 400, Loss: 0.017439752293284982\n",
      "Epoch: 143, Iteration: 500, Loss: 0.017329896465525962\n",
      "Epoch: 143, Iteration: 600, Loss: 0.01631957303470699\n",
      "Epoch: 143, Iteration: 700, Loss: 0.01719944181968458\n",
      "Epoch: 143, Iteration: 800, Loss: 0.01656360957713332\n",
      "Epoch: 143, Train Loss: 0.0006742239238687336, Test Loss: 0.00016613248435910232\n",
      "Epoch: 144, Iteration: 100, Loss: 0.01653880220692372\n",
      "Epoch: 144, Iteration: 200, Loss: 0.016526588231499773\n",
      "Epoch: 144, Iteration: 300, Loss: 0.01684743240912212\n",
      "Epoch: 144, Iteration: 400, Loss: 0.017532246798509732\n",
      "Epoch: 144, Iteration: 500, Loss: 0.016193289549846668\n",
      "Epoch: 144, Iteration: 600, Loss: 0.01640264143497916\n",
      "Epoch: 144, Iteration: 700, Loss: 0.016569183266256005\n",
      "Epoch: 144, Iteration: 800, Loss: 0.017128627179772593\n",
      "Epoch: 144, Train Loss: 0.0006699737808924631, Test Loss: 0.0001695148375508706\n",
      "Epoch: 145, Iteration: 100, Loss: 0.01748195591790136\n",
      "Epoch: 145, Iteration: 200, Loss: 0.01726858218171401\n",
      "Epoch: 145, Iteration: 300, Loss: 0.01673861192102777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 145, Iteration: 400, Loss: 0.016156600242538843\n",
      "Epoch: 145, Iteration: 500, Loss: 0.01651166811643634\n",
      "Epoch: 145, Iteration: 600, Loss: 0.01556746980349999\n",
      "Epoch: 145, Iteration: 700, Loss: 0.01703412036295049\n",
      "Epoch: 145, Iteration: 800, Loss: 0.016965566952421796\n",
      "Epoch: 145, Train Loss: 0.000666679876588536, Test Loss: 0.00016840197366430566\n",
      "Epoch: 146, Iteration: 100, Loss: 0.016725277331715915\n",
      "Epoch: 146, Iteration: 200, Loss: 0.01686370250536129\n",
      "Epoch: 146, Iteration: 300, Loss: 0.016089295022538863\n",
      "Epoch: 146, Iteration: 400, Loss: 0.017570193027495407\n",
      "Epoch: 146, Iteration: 500, Loss: 0.017820553614001255\n",
      "Epoch: 146, Iteration: 600, Loss: 0.01686684470041655\n",
      "Epoch: 146, Iteration: 700, Loss: 0.01670794029632816\n",
      "Epoch: 146, Iteration: 800, Loss: 0.016054179475759156\n",
      "Epoch: 146, Train Loss: 0.0006720811086838253, Test Loss: 0.0001664683695540287\n",
      "Epoch: 147, Iteration: 100, Loss: 0.016142978027346544\n",
      "Epoch: 147, Iteration: 200, Loss: 0.016927750475588255\n",
      "Epoch: 147, Iteration: 300, Loss: 0.016362259368179366\n",
      "Epoch: 147, Iteration: 400, Loss: 0.017227177268068772\n",
      "Epoch: 147, Iteration: 500, Loss: 0.01645424672460649\n",
      "Epoch: 147, Iteration: 600, Loss: 0.017366178377415054\n",
      "Epoch: 147, Iteration: 700, Loss: 0.016596719033259433\n",
      "Epoch: 147, Iteration: 800, Loss: 0.016048634141043294\n",
      "Epoch: 147, Train Loss: 0.0006649919753974145, Test Loss: 0.00016634557888390333\n",
      "Epoch: 148, Iteration: 100, Loss: 0.016995560137729626\n",
      "Epoch: 148, Iteration: 200, Loss: 0.016250771797786\n",
      "Epoch: 148, Iteration: 300, Loss: 0.01658020040486008\n",
      "Epoch: 148, Iteration: 400, Loss: 0.01685351946798619\n",
      "Epoch: 148, Iteration: 500, Loss: 0.016778593242634088\n",
      "Epoch: 148, Iteration: 600, Loss: 0.016130455580423586\n",
      "Epoch: 148, Iteration: 700, Loss: 0.01680092521564802\n",
      "Epoch: 148, Iteration: 800, Loss: 0.016594454733422026\n",
      "Epoch: 148, Train Loss: 0.0006639707952492102, Test Loss: 0.00017142365244002627\n",
      "Epoch: 149, Iteration: 100, Loss: 0.016205067026021425\n",
      "Epoch: 149, Iteration: 200, Loss: 0.016479709302075207\n",
      "Epoch: 149, Iteration: 300, Loss: 0.016256570088444278\n",
      "Epoch: 149, Iteration: 400, Loss: 0.01709073858364718\n",
      "Epoch: 149, Iteration: 500, Loss: 0.01654490813962184\n",
      "Epoch: 149, Iteration: 600, Loss: 0.016117538150865585\n",
      "Epoch: 149, Iteration: 700, Loss: 0.016518105934665073\n",
      "Epoch: 149, Iteration: 800, Loss: 0.01709633140853839\n",
      "Epoch   150: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 149, Train Loss: 0.0006619087112064297, Test Loss: 0.00016606400193955561\n",
      "Epoch: 150, Iteration: 100, Loss: 0.01637271451181732\n",
      "Epoch: 150, Iteration: 200, Loss: 0.01565593473787885\n",
      "Epoch: 150, Iteration: 300, Loss: 0.016244082762568723\n",
      "Epoch: 150, Iteration: 400, Loss: 0.016046949102019425\n",
      "Epoch: 150, Iteration: 500, Loss: 0.015878092337516136\n",
      "Epoch: 150, Iteration: 600, Loss: 0.015758595516672358\n",
      "Epoch: 150, Iteration: 700, Loss: 0.016046891774749383\n",
      "Epoch: 150, Iteration: 800, Loss: 0.01648848432523664\n",
      "Epoch: 150, Train Loss: 0.0006425038895375001, Test Loss: 0.0001620506897406687\n",
      "Epoch: 151, Iteration: 100, Loss: 0.01616268663929077\n",
      "Epoch: 151, Iteration: 200, Loss: 0.016145099587447476\n",
      "Epoch: 151, Iteration: 300, Loss: 0.01615520883933641\n",
      "Epoch: 151, Iteration: 400, Loss: 0.015902838968031574\n",
      "Epoch: 151, Iteration: 500, Loss: 0.015656206232961267\n",
      "Epoch: 151, Iteration: 600, Loss: 0.015801401052158326\n",
      "Epoch: 151, Iteration: 700, Loss: 0.01570650690700859\n",
      "Epoch: 151, Iteration: 800, Loss: 0.016882586482097395\n",
      "Epoch: 151, Train Loss: 0.00064201665312467, Test Loss: 0.00016185208362573327\n",
      "Epoch: 152, Iteration: 100, Loss: 0.016728467890061438\n",
      "Epoch: 152, Iteration: 200, Loss: 0.015448242113052402\n",
      "Epoch: 152, Iteration: 300, Loss: 0.015974242909578606\n",
      "Epoch: 152, Iteration: 400, Loss: 0.0163710445194738\n",
      "Epoch: 152, Iteration: 500, Loss: 0.015826061106054112\n",
      "Epoch: 152, Iteration: 600, Loss: 0.015567178095807321\n",
      "Epoch: 152, Iteration: 700, Loss: 0.016094635204353835\n",
      "Epoch: 152, Iteration: 800, Loss: 0.016455904507893138\n",
      "Epoch: 152, Train Loss: 0.0006414665476097682, Test Loss: 0.00016269962200605255\n",
      "Epoch: 153, Iteration: 100, Loss: 0.015809434560651425\n",
      "Epoch: 153, Iteration: 200, Loss: 0.016737710211600643\n",
      "Epoch: 153, Iteration: 300, Loss: 0.01611662302457262\n",
      "Epoch: 153, Iteration: 400, Loss: 0.01647816137119662\n",
      "Epoch: 153, Iteration: 500, Loss: 0.015523403038969263\n",
      "Epoch: 153, Iteration: 600, Loss: 0.016081825575383846\n",
      "Epoch: 153, Iteration: 700, Loss: 0.015874471064307727\n",
      "Epoch: 153, Iteration: 800, Loss: 0.015990136060281657\n",
      "Epoch: 153, Train Loss: 0.0006414320722449258, Test Loss: 0.0001620248418174199\n",
      "Epoch: 154, Iteration: 100, Loss: 0.015744820986583363\n",
      "Epoch: 154, Iteration: 200, Loss: 0.01642445418110583\n",
      "Epoch: 154, Iteration: 300, Loss: 0.0162574763817247\n",
      "Epoch: 154, Iteration: 400, Loss: 0.01619100609968882\n",
      "Epoch: 154, Iteration: 500, Loss: 0.015616990611306392\n",
      "Epoch: 154, Iteration: 600, Loss: 0.016276242611638736\n",
      "Epoch: 154, Iteration: 700, Loss: 0.015979297539161053\n",
      "Epoch: 154, Iteration: 800, Loss: 0.016099708343972452\n",
      "Epoch: 154, Train Loss: 0.0006409181091944636, Test Loss: 0.00016188646333017452\n",
      "Epoch: 155, Iteration: 100, Loss: 0.016005233621399384\n",
      "Epoch: 155, Iteration: 200, Loss: 0.016069548575615045\n",
      "Epoch: 155, Iteration: 300, Loss: 0.015944170183502138\n",
      "Epoch: 155, Iteration: 400, Loss: 0.016331654813257046\n",
      "Epoch: 155, Iteration: 500, Loss: 0.016157399280928075\n",
      "Epoch: 155, Iteration: 600, Loss: 0.015722717289463617\n",
      "Epoch: 155, Iteration: 700, Loss: 0.015458992660569493\n",
      "Epoch: 155, Iteration: 800, Loss: 0.016162833460839465\n",
      "Epoch: 155, Train Loss: 0.0006408726996412256, Test Loss: 0.00016194158715860112\n",
      "Epoch: 156, Iteration: 100, Loss: 0.016105152491945773\n",
      "Epoch: 156, Iteration: 200, Loss: 0.0160153439064743\n",
      "Epoch: 156, Iteration: 300, Loss: 0.01631912410084624\n",
      "Epoch: 156, Iteration: 400, Loss: 0.015967752078722697\n",
      "Epoch: 156, Iteration: 500, Loss: 0.015786166106408928\n",
      "Epoch: 156, Iteration: 600, Loss: 0.016134107361722272\n",
      "Epoch: 156, Iteration: 700, Loss: 0.015808904354344122\n",
      "Epoch: 156, Iteration: 800, Loss: 0.015856759127927944\n",
      "Epoch: 156, Train Loss: 0.0006408111611676235, Test Loss: 0.00016179465187696337\n",
      "Epoch: 157, Iteration: 100, Loss: 0.015888947229541373\n",
      "Epoch: 157, Iteration: 200, Loss: 0.015728938895335887\n",
      "Epoch: 157, Iteration: 300, Loss: 0.016471042421471793\n",
      "Epoch: 157, Iteration: 400, Loss: 0.01566371613444062\n",
      "Epoch: 157, Iteration: 500, Loss: 0.016222977952565998\n",
      "Epoch: 157, Iteration: 600, Loss: 0.01592222580075031\n",
      "Epoch: 157, Iteration: 700, Loss: 0.016606832869001664\n",
      "Epoch: 157, Iteration: 800, Loss: 0.01587396880495362\n",
      "Epoch: 157, Train Loss: 0.0006403704485354392, Test Loss: 0.00016161334446860984\n",
      "Epoch: 158, Iteration: 100, Loss: 0.01647077262896346\n",
      "Epoch: 158, Iteration: 200, Loss: 0.01581110659026308\n",
      "Epoch: 158, Iteration: 300, Loss: 0.016026575503929053\n",
      "Epoch: 158, Iteration: 400, Loss: 0.016218575940001756\n",
      "Epoch: 158, Iteration: 500, Loss: 0.015598164813127369\n",
      "Epoch: 158, Iteration: 600, Loss: 0.016136489546624944\n",
      "Epoch: 158, Iteration: 700, Loss: 0.015845750262087677\n",
      "Epoch: 158, Iteration: 800, Loss: 0.01601334286533529\n",
      "Epoch: 158, Train Loss: 0.0006407880750235303, Test Loss: 0.0001614436163276773\n",
      "Epoch: 159, Iteration: 100, Loss: 0.015585337147058453\n",
      "Epoch: 159, Iteration: 200, Loss: 0.015872471092734486\n",
      "Epoch: 159, Iteration: 300, Loss: 0.015453306412382517\n",
      "Epoch: 159, Iteration: 400, Loss: 0.01663218111207243\n",
      "Epoch: 159, Iteration: 500, Loss: 0.016409125055361073\n",
      "Epoch: 159, Iteration: 600, Loss: 0.015829168005438987\n",
      "Epoch: 159, Iteration: 700, Loss: 0.016459304933960084\n",
      "Epoch: 159, Iteration: 800, Loss: 0.015258661522238981\n",
      "Epoch: 159, Train Loss: 0.0006400382343796618, Test Loss: 0.00016169750040733954\n",
      "Epoch: 160, Iteration: 100, Loss: 0.015758036490296945\n",
      "Epoch: 160, Iteration: 200, Loss: 0.01627237708453322\n",
      "Epoch: 160, Iteration: 300, Loss: 0.016004752156732138\n",
      "Epoch: 160, Iteration: 400, Loss: 0.015490139696339611\n",
      "Epoch: 160, Iteration: 500, Loss: 0.016310019855154678\n",
      "Epoch: 160, Iteration: 600, Loss: 0.016173713192983996\n",
      "Epoch: 160, Iteration: 700, Loss: 0.016156920268258546\n",
      "Epoch: 160, Iteration: 800, Loss: 0.015856534759222995\n",
      "Epoch: 160, Train Loss: 0.00064021451463778, Test Loss: 0.00016205967015003108\n",
      "Epoch: 161, Iteration: 100, Loss: 0.01625144263380207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 161, Iteration: 200, Loss: 0.015496400708798319\n",
      "Epoch: 161, Iteration: 300, Loss: 0.01595251762046246\n",
      "Epoch: 161, Iteration: 400, Loss: 0.015940347824653145\n",
      "Epoch: 161, Iteration: 500, Loss: 0.01610591961798491\n",
      "Epoch: 161, Iteration: 600, Loss: 0.0160677276289789\n",
      "Epoch: 161, Iteration: 700, Loss: 0.015981481890776195\n",
      "Epoch: 161, Iteration: 800, Loss: 0.01608067711640615\n",
      "Epoch: 161, Train Loss: 0.0006397757347949999, Test Loss: 0.0001617618267797792\n",
      "Epoch: 162, Iteration: 100, Loss: 0.01581269098096527\n",
      "Epoch: 162, Iteration: 200, Loss: 0.015723940799944103\n",
      "Epoch: 162, Iteration: 300, Loss: 0.016590500388701912\n",
      "Epoch: 162, Iteration: 400, Loss: 0.016394787606259342\n",
      "Epoch: 162, Iteration: 500, Loss: 0.015871082308876794\n",
      "Epoch: 162, Iteration: 600, Loss: 0.015909268535324372\n",
      "Epoch: 162, Iteration: 700, Loss: 0.015673963505832944\n",
      "Epoch: 162, Iteration: 800, Loss: 0.015587420159135945\n",
      "Epoch: 162, Train Loss: 0.0006393311213777606, Test Loss: 0.0001613004952572578\n",
      "Epoch: 163, Iteration: 100, Loss: 0.015607313966029324\n",
      "Epoch: 163, Iteration: 200, Loss: 0.0159676568582654\n",
      "Epoch: 163, Iteration: 300, Loss: 0.016125127680425067\n",
      "Epoch: 163, Iteration: 400, Loss: 0.015710711188148707\n",
      "Epoch: 163, Iteration: 500, Loss: 0.015818773114006035\n",
      "Epoch: 163, Iteration: 600, Loss: 0.015433549568115268\n",
      "Epoch: 163, Iteration: 700, Loss: 0.01658088139811298\n",
      "Epoch: 163, Iteration: 800, Loss: 0.016077584063168615\n",
      "Epoch: 163, Train Loss: 0.0006381314573957036, Test Loss: 0.00016150218735058462\n",
      "Epoch: 164, Iteration: 100, Loss: 0.01585311258531874\n",
      "Epoch: 164, Iteration: 200, Loss: 0.01589534527010983\n",
      "Epoch: 164, Iteration: 300, Loss: 0.015702376353146974\n",
      "Epoch: 164, Iteration: 400, Loss: 0.016207886750635225\n",
      "Epoch: 164, Iteration: 500, Loss: 0.016109429896459915\n",
      "Epoch: 164, Iteration: 600, Loss: 0.016202712729864288\n",
      "Epoch: 164, Iteration: 700, Loss: 0.016002203075913712\n",
      "Epoch: 164, Iteration: 800, Loss: 0.01554740936262533\n",
      "Epoch   165: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 164, Train Loss: 0.000638863746853908, Test Loss: 0.00016139854954916143\n",
      "Epoch: 165, Iteration: 100, Loss: 0.015776194304635283\n",
      "Epoch: 165, Iteration: 200, Loss: 0.015376809315057471\n",
      "Epoch: 165, Iteration: 300, Loss: 0.01579262378072599\n",
      "Epoch: 165, Iteration: 400, Loss: 0.01611569329543272\n",
      "Epoch: 165, Iteration: 500, Loss: 0.01614067224727478\n",
      "Epoch: 165, Iteration: 600, Loss: 0.015978060924680904\n",
      "Epoch: 165, Iteration: 700, Loss: 0.016382936941226944\n",
      "Epoch: 165, Iteration: 800, Loss: 0.016080918183433823\n",
      "Epoch: 165, Train Loss: 0.0006370737272959036, Test Loss: 0.00016100201569336643\n",
      "Epoch: 166, Iteration: 100, Loss: 0.015999960603949148\n",
      "Epoch: 166, Iteration: 200, Loss: 0.016177856661670376\n",
      "Epoch: 166, Iteration: 300, Loss: 0.015381555349449627\n",
      "Epoch: 166, Iteration: 400, Loss: 0.015659373537346255\n",
      "Epoch: 166, Iteration: 500, Loss: 0.015784858391270973\n",
      "Epoch: 166, Iteration: 600, Loss: 0.015823834291950334\n",
      "Epoch: 166, Iteration: 700, Loss: 0.016420426160038915\n",
      "Epoch: 166, Iteration: 800, Loss: 0.015832726850931067\n",
      "Epoch: 166, Train Loss: 0.0006368153863401401, Test Loss: 0.00016099626379739924\n",
      "Epoch: 167, Iteration: 100, Loss: 0.015904046063951682\n",
      "Epoch: 167, Iteration: 200, Loss: 0.016107507770357188\n",
      "Epoch: 167, Iteration: 300, Loss: 0.015872754309384618\n",
      "Epoch: 167, Iteration: 400, Loss: 0.016347414915799163\n",
      "Epoch: 167, Iteration: 500, Loss: 0.01572543194924947\n",
      "Epoch: 167, Iteration: 600, Loss: 0.015826992734218948\n",
      "Epoch: 167, Iteration: 700, Loss: 0.015331345101003535\n",
      "Epoch: 167, Iteration: 800, Loss: 0.016055153690103907\n",
      "Epoch: 167, Train Loss: 0.000636747975799916, Test Loss: 0.0001610219528566518\n",
      "Epoch: 168, Iteration: 100, Loss: 0.01578924697241746\n",
      "Epoch: 168, Iteration: 200, Loss: 0.0155934027134208\n",
      "Epoch: 168, Iteration: 300, Loss: 0.016247834311798215\n",
      "Epoch: 168, Iteration: 400, Loss: 0.016292234613501932\n",
      "Epoch: 168, Iteration: 500, Loss: 0.016255423091934063\n",
      "Epoch: 168, Iteration: 600, Loss: 0.016242825593508314\n",
      "Epoch: 168, Iteration: 700, Loss: 0.015302097657695413\n",
      "Epoch: 168, Iteration: 800, Loss: 0.015761310707603116\n",
      "Epoch: 168, Train Loss: 0.0006367077007927301, Test Loss: 0.00016096944243819904\n",
      "Epoch: 169, Iteration: 100, Loss: 0.015795241262821946\n",
      "Epoch: 169, Iteration: 200, Loss: 0.016056601554737426\n",
      "Epoch: 169, Iteration: 300, Loss: 0.016169266185897868\n",
      "Epoch: 169, Iteration: 400, Loss: 0.016190912712772842\n",
      "Epoch: 169, Iteration: 500, Loss: 0.016413050645496696\n",
      "Epoch: 169, Iteration: 600, Loss: 0.015521358509431593\n",
      "Epoch: 169, Iteration: 700, Loss: 0.015700021052907687\n",
      "Epoch: 169, Iteration: 800, Loss: 0.01566508723044535\n",
      "Epoch: 169, Train Loss: 0.0006366339351561817, Test Loss: 0.00016102487225920254\n",
      "Epoch: 170, Iteration: 100, Loss: 0.01633314429636812\n",
      "Epoch: 170, Iteration: 200, Loss: 0.01596953484113328\n",
      "Epoch: 170, Iteration: 300, Loss: 0.015494509869313333\n",
      "Epoch: 170, Iteration: 400, Loss: 0.015984177465725224\n",
      "Epoch: 170, Iteration: 500, Loss: 0.016024177682993468\n",
      "Epoch: 170, Iteration: 600, Loss: 0.01570451763109304\n",
      "Epoch: 170, Iteration: 700, Loss: 0.01585811765107792\n",
      "Epoch: 170, Iteration: 800, Loss: 0.016238631469605025\n",
      "Epoch: 170, Train Loss: 0.0006366131771508763, Test Loss: 0.0001609831808208947\n",
      "Epoch: 171, Iteration: 100, Loss: 0.015750227903481573\n",
      "Epoch: 171, Iteration: 200, Loss: 0.015889109374256805\n",
      "Epoch: 171, Iteration: 300, Loss: 0.015619389574567322\n",
      "Epoch: 171, Iteration: 400, Loss: 0.015847482718527317\n",
      "Epoch: 171, Iteration: 500, Loss: 0.016549213221878745\n",
      "Epoch: 171, Iteration: 600, Loss: 0.015708046608779114\n",
      "Epoch: 171, Iteration: 700, Loss: 0.015722086311143357\n",
      "Epoch: 171, Iteration: 800, Loss: 0.01626347091223579\n",
      "Epoch   172: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 171, Train Loss: 0.0006366524629295945, Test Loss: 0.00016099748086057655\n",
      "Epoch: 172, Iteration: 100, Loss: 0.015900903192232363\n",
      "Epoch: 172, Iteration: 200, Loss: 0.015766694392368663\n",
      "Epoch: 172, Iteration: 300, Loss: 0.016188736590265762\n",
      "Epoch: 172, Iteration: 400, Loss: 0.01607842603698373\n",
      "Epoch: 172, Iteration: 500, Loss: 0.015634012102964334\n",
      "Epoch: 172, Iteration: 600, Loss: 0.016168691065104213\n",
      "Epoch: 172, Iteration: 700, Loss: 0.016283114644465968\n",
      "Epoch: 172, Iteration: 800, Loss: 0.015287732567230705\n",
      "Epoch: 172, Train Loss: 0.0006363396102311476, Test Loss: 0.0001609785580730778\n",
      "Epoch: 173, Iteration: 100, Loss: 0.016085627692518756\n",
      "Epoch: 173, Iteration: 200, Loss: 0.015914694296952803\n",
      "Epoch: 173, Iteration: 300, Loss: 0.01579542752733687\n",
      "Epoch: 173, Iteration: 400, Loss: 0.015584229528030846\n",
      "Epoch: 173, Iteration: 500, Loss: 0.015855699050007388\n",
      "Epoch: 173, Iteration: 600, Loss: 0.015793318787473254\n",
      "Epoch: 173, Iteration: 700, Loss: 0.015803434798726812\n",
      "Epoch: 173, Iteration: 800, Loss: 0.016248217958491296\n",
      "Epoch: 173, Train Loss: 0.0006363096702355711, Test Loss: 0.0001609721406114026\n",
      "Epoch: 174, Iteration: 100, Loss: 0.016181827704713214\n",
      "Epoch: 174, Iteration: 200, Loss: 0.015604297295794822\n",
      "Epoch: 174, Iteration: 300, Loss: 0.01612412551185116\n",
      "Epoch: 174, Iteration: 400, Loss: 0.01605530783854192\n",
      "Epoch: 174, Iteration: 500, Loss: 0.015500556241022423\n",
      "Epoch: 174, Iteration: 600, Loss: 0.01553207555116387\n",
      "Epoch: 174, Iteration: 700, Loss: 0.016188403656997252\n",
      "Epoch: 174, Iteration: 800, Loss: 0.016022297662857454\n",
      "Epoch: 174, Train Loss: 0.0006363109918445357, Test Loss: 0.00016097631362456752\n",
      "Epoch: 175, Iteration: 100, Loss: 0.016577015929215122\n",
      "Epoch: 175, Iteration: 200, Loss: 0.015912805589323398\n",
      "Epoch: 175, Iteration: 300, Loss: 0.0162041203584522\n",
      "Epoch: 175, Iteration: 400, Loss: 0.015598110672726762\n",
      "Epoch: 175, Iteration: 500, Loss: 0.015477999215363525\n",
      "Epoch: 175, Iteration: 600, Loss: 0.015419598414155189\n",
      "Epoch: 175, Iteration: 700, Loss: 0.016288181584968697\n",
      "Epoch: 175, Iteration: 800, Loss: 0.015393124616821297\n",
      "Epoch: 175, Train Loss: 0.0006363131759059508, Test Loss: 0.00016096977317573777\n",
      "Epoch: 176, Iteration: 100, Loss: 0.015766389878990594\n",
      "Epoch: 176, Iteration: 200, Loss: 0.015563858825771604\n",
      "Epoch: 176, Iteration: 300, Loss: 0.015704042147262953\n",
      "Epoch: 176, Iteration: 400, Loss: 0.015985353813448455\n",
      "Epoch: 176, Iteration: 500, Loss: 0.015875864046392962\n",
      "Epoch: 176, Iteration: 600, Loss: 0.016139735955221113\n",
      "Epoch: 176, Iteration: 700, Loss: 0.016263591547613032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176, Iteration: 800, Loss: 0.01567584742588224\n",
      "Epoch: 176, Train Loss: 0.0006363093561958522, Test Loss: 0.00016097559763010257\n",
      "Epoch: 177, Iteration: 100, Loss: 0.015899833153525833\n",
      "Epoch: 177, Iteration: 200, Loss: 0.015937228294205852\n",
      "Epoch: 177, Iteration: 300, Loss: 0.01599939670995809\n",
      "Epoch: 177, Iteration: 400, Loss: 0.01612798189307796\n",
      "Epoch: 177, Iteration: 500, Loss: 0.016078150256362278\n",
      "Epoch: 177, Iteration: 600, Loss: 0.015844756111619063\n",
      "Epoch: 177, Iteration: 700, Loss: 0.015636980773706455\n",
      "Epoch: 177, Iteration: 800, Loss: 0.015830409771297127\n",
      "Epoch   178: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 177, Train Loss: 0.0006363285865853562, Test Loss: 0.00016097980615128183\n",
      "Epoch: 178, Iteration: 100, Loss: 0.015827053270186298\n",
      "Epoch: 178, Iteration: 200, Loss: 0.01613191895012278\n",
      "Epoch: 178, Iteration: 300, Loss: 0.015654255541448947\n",
      "Epoch: 178, Iteration: 400, Loss: 0.016051508267992176\n",
      "Epoch: 178, Iteration: 500, Loss: 0.015911364294879604\n",
      "Epoch: 178, Iteration: 600, Loss: 0.01627769531478407\n",
      "Epoch: 178, Iteration: 700, Loss: 0.015694429603172466\n",
      "Epoch: 178, Iteration: 800, Loss: 0.015597703168168664\n",
      "Epoch: 178, Train Loss: 0.0006362616670884467, Test Loss: 0.00016097376207699829\n",
      "Epoch: 179, Iteration: 100, Loss: 0.015394119283882901\n",
      "Epoch: 179, Iteration: 200, Loss: 0.016109231786685996\n",
      "Epoch: 179, Iteration: 300, Loss: 0.016172213552636094\n",
      "Epoch: 179, Iteration: 400, Loss: 0.01604416778718587\n",
      "Epoch: 179, Iteration: 500, Loss: 0.01629127650085138\n",
      "Epoch: 179, Iteration: 600, Loss: 0.015374079361208715\n",
      "Epoch: 179, Iteration: 700, Loss: 0.015749278871226124\n",
      "Epoch: 179, Iteration: 800, Loss: 0.016262363649730105\n",
      "Epoch: 179, Train Loss: 0.0006362666894774562, Test Loss: 0.00016097412597546826\n",
      "Epoch: 180, Iteration: 100, Loss: 0.01538005968905054\n",
      "Epoch: 180, Iteration: 200, Loss: 0.015456866902241018\n",
      "Epoch: 180, Iteration: 300, Loss: 0.015404105004563462\n",
      "Epoch: 180, Iteration: 400, Loss: 0.016555435824557208\n",
      "Epoch: 180, Iteration: 500, Loss: 0.015288005277398042\n",
      "Epoch: 180, Iteration: 600, Loss: 0.01619209614727879\n",
      "Epoch: 180, Iteration: 700, Loss: 0.016226256659138016\n",
      "Epoch: 180, Iteration: 800, Loss: 0.016526162493391894\n",
      "Epoch: 180, Train Loss: 0.0006362702612685198, Test Loss: 0.00016097466865461104\n",
      "Epoch: 181, Iteration: 100, Loss: 0.016072489037469495\n",
      "Epoch: 181, Iteration: 200, Loss: 0.015660970311728306\n",
      "Epoch: 181, Iteration: 300, Loss: 0.016079113047453575\n",
      "Epoch: 181, Iteration: 400, Loss: 0.01595111296774121\n",
      "Epoch: 181, Iteration: 500, Loss: 0.015946496081596706\n",
      "Epoch: 181, Iteration: 600, Loss: 0.015970412168826442\n",
      "Epoch: 181, Iteration: 700, Loss: 0.01627513163839467\n",
      "Epoch: 181, Iteration: 800, Loss: 0.015409212974191178\n",
      "Epoch: 181, Train Loss: 0.0006362711324387076, Test Loss: 0.0001609750751694042\n",
      "Epoch: 182, Iteration: 100, Loss: 0.015502904781897087\n",
      "Epoch: 182, Iteration: 200, Loss: 0.01550926348863868\n",
      "Epoch: 182, Iteration: 300, Loss: 0.016011464089388028\n",
      "Epoch: 182, Iteration: 400, Loss: 0.0159214090090245\n",
      "Epoch: 182, Iteration: 500, Loss: 0.016273201719741337\n",
      "Epoch: 182, Iteration: 600, Loss: 0.015901226754067466\n",
      "Epoch: 182, Iteration: 700, Loss: 0.015922492792014964\n",
      "Epoch: 182, Iteration: 800, Loss: 0.016332053499354515\n",
      "Epoch: 182, Train Loss: 0.0006362798573177797, Test Loss: 0.00016097337561300076\n",
      "Epoch: 183, Iteration: 100, Loss: 0.016195376694668084\n",
      "Epoch: 183, Iteration: 200, Loss: 0.01589372772286879\n",
      "Epoch: 183, Iteration: 300, Loss: 0.015977980066963937\n",
      "Epoch: 183, Iteration: 400, Loss: 0.015613451701938175\n",
      "Epoch: 183, Iteration: 500, Loss: 0.01620166497741593\n",
      "Epoch: 183, Iteration: 600, Loss: 0.01560909382533282\n",
      "Epoch: 183, Iteration: 700, Loss: 0.016074443570687436\n",
      "Epoch: 183, Iteration: 800, Loss: 0.015932997222989798\n",
      "Epoch: 183, Train Loss: 0.0006362613346073604, Test Loss: 0.00016097522243210393\n",
      "Epoch: 184, Iteration: 100, Loss: 0.01608347717410652\n",
      "Epoch: 184, Iteration: 200, Loss: 0.0157455411463161\n",
      "Epoch: 184, Iteration: 300, Loss: 0.01578685252025025\n",
      "Epoch: 184, Iteration: 400, Loss: 0.01580101220315555\n",
      "Epoch: 184, Iteration: 500, Loss: 0.015809130629349966\n",
      "Epoch: 184, Iteration: 600, Loss: 0.015976874507032335\n",
      "Epoch: 184, Iteration: 700, Loss: 0.016326499324350152\n",
      "Epoch: 184, Iteration: 800, Loss: 0.015692689274146687\n",
      "Epoch: 184, Train Loss: 0.0006362689534738158, Test Loss: 0.00016097316098901603\n",
      "Epoch: 185, Iteration: 100, Loss: 0.015962401688739192\n",
      "Epoch: 185, Iteration: 200, Loss: 0.01580111324437894\n",
      "Epoch: 185, Iteration: 300, Loss: 0.01618974820303265\n",
      "Epoch: 185, Iteration: 400, Loss: 0.015640174882719293\n",
      "Epoch: 185, Iteration: 500, Loss: 0.01570241031004116\n",
      "Epoch: 185, Iteration: 600, Loss: 0.016118309707962908\n",
      "Epoch: 185, Iteration: 700, Loss: 0.015898463323537726\n",
      "Epoch: 185, Iteration: 800, Loss: 0.016149439215951134\n",
      "Epoch: 185, Train Loss: 0.0006362650706942412, Test Loss: 0.00016097347576638968\n",
      "Epoch: 186, Iteration: 100, Loss: 0.015648103100829758\n",
      "Epoch: 186, Iteration: 200, Loss: 0.01572120287892176\n",
      "Epoch: 186, Iteration: 300, Loss: 0.015936343792418484\n",
      "Epoch: 186, Iteration: 400, Loss: 0.01585546739079291\n",
      "Epoch: 186, Iteration: 500, Loss: 0.01669607213989366\n",
      "Epoch: 186, Iteration: 600, Loss: 0.016092376346932724\n",
      "Epoch: 186, Iteration: 700, Loss: 0.015500415116548538\n",
      "Epoch: 186, Iteration: 800, Loss: 0.015728803802630864\n",
      "Epoch: 186, Train Loss: 0.0006362686141526588, Test Loss: 0.00016097467324818797\n",
      "Epoch: 187, Iteration: 100, Loss: 0.015773138686199673\n",
      "Epoch: 187, Iteration: 200, Loss: 0.015897373486950528\n",
      "Epoch: 187, Iteration: 300, Loss: 0.015663990219763946\n",
      "Epoch: 187, Iteration: 400, Loss: 0.01617767936841119\n",
      "Epoch: 187, Iteration: 500, Loss: 0.01592709762917366\n",
      "Epoch: 187, Iteration: 600, Loss: 0.016074613711680286\n",
      "Epoch: 187, Iteration: 700, Loss: 0.015984804973413702\n",
      "Epoch: 187, Iteration: 800, Loss: 0.01567998484824784\n",
      "Epoch: 187, Train Loss: 0.0006362704362938596, Test Loss: 0.00016097419125790823\n",
      "Epoch: 188, Iteration: 100, Loss: 0.015947929117828608\n",
      "Epoch: 188, Iteration: 200, Loss: 0.015911657828837633\n",
      "Epoch: 188, Iteration: 300, Loss: 0.015876311706961133\n",
      "Epoch: 188, Iteration: 400, Loss: 0.016126175403769594\n",
      "Epoch: 188, Iteration: 500, Loss: 0.01615093072905438\n",
      "Epoch: 188, Iteration: 600, Loss: 0.01576584073336562\n",
      "Epoch: 188, Iteration: 700, Loss: 0.01568827687879093\n",
      "Epoch: 188, Iteration: 800, Loss: 0.015747346260468476\n",
      "Epoch: 188, Train Loss: 0.0006362730207341221, Test Loss: 0.00016097566509197685\n",
      "Epoch: 189, Iteration: 100, Loss: 0.015629694462404586\n",
      "Epoch: 189, Iteration: 200, Loss: 0.0157251102355076\n",
      "Epoch: 189, Iteration: 300, Loss: 0.016051873652031645\n",
      "Epoch: 189, Iteration: 400, Loss: 0.015960257202095818\n",
      "Epoch: 189, Iteration: 500, Loss: 0.015882729523582384\n",
      "Epoch: 189, Iteration: 600, Loss: 0.015984382982423995\n",
      "Epoch: 189, Iteration: 700, Loss: 0.01589641905593453\n",
      "Epoch: 189, Iteration: 800, Loss: 0.01578353127843002\n",
      "Epoch: 189, Train Loss: 0.0006362670002983184, Test Loss: 0.0001609740474823034\n",
      "Epoch: 190, Iteration: 100, Loss: 0.015454825894266833\n",
      "Epoch: 190, Iteration: 200, Loss: 0.015750809070596006\n",
      "Epoch: 190, Iteration: 300, Loss: 0.015751468199596275\n",
      "Epoch: 190, Iteration: 400, Loss: 0.01584026748605538\n",
      "Epoch: 190, Iteration: 500, Loss: 0.01612428748921957\n",
      "Epoch: 190, Iteration: 600, Loss: 0.016196233795199078\n",
      "Epoch: 190, Iteration: 700, Loss: 0.016560599986405578\n",
      "Epoch: 190, Iteration: 800, Loss: 0.015972923043591436\n",
      "Epoch: 190, Train Loss: 0.0006362663998138717, Test Loss: 0.00016097410273934555\n",
      "Epoch: 191, Iteration: 100, Loss: 0.016381819361413363\n",
      "Epoch: 191, Iteration: 200, Loss: 0.01600060312921414\n",
      "Epoch: 191, Iteration: 300, Loss: 0.015990591542504262\n",
      "Epoch: 191, Iteration: 400, Loss: 0.015566181013127789\n",
      "Epoch: 191, Iteration: 500, Loss: 0.015586242247081827\n",
      "Epoch: 191, Iteration: 600, Loss: 0.01608679680066416\n",
      "Epoch: 191, Iteration: 700, Loss: 0.015657331758120563\n",
      "Epoch: 191, Iteration: 800, Loss: 0.016389665237511508\n",
      "Epoch: 191, Train Loss: 0.0006362624006189753, Test Loss: 0.0001609749118459501\n",
      "Epoch: 192, Iteration: 100, Loss: 0.016000831630663015\n",
      "Epoch: 192, Iteration: 200, Loss: 0.0162572877961793\n",
      "Epoch: 192, Iteration: 300, Loss: 0.01602888816705672\n",
      "Epoch: 192, Iteration: 400, Loss: 0.01641179579019081\n",
      "Epoch: 192, Iteration: 500, Loss: 0.015628785535227507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 192, Iteration: 600, Loss: 0.015993817360140383\n",
      "Epoch: 192, Iteration: 700, Loss: 0.0158130782074295\n",
      "Epoch: 192, Iteration: 800, Loss: 0.015483097951801028\n",
      "Epoch: 192, Train Loss: 0.0006362608074860257, Test Loss: 0.00016097757283465138\n",
      "Epoch: 193, Iteration: 100, Loss: 0.01490407691017026\n",
      "Epoch: 193, Iteration: 200, Loss: 0.015763145398523193\n",
      "Epoch: 193, Iteration: 300, Loss: 0.016279649760690518\n",
      "Epoch: 193, Iteration: 400, Loss: 0.015674829526687972\n",
      "Epoch: 193, Iteration: 500, Loss: 0.01600625655555632\n",
      "Epoch: 193, Iteration: 600, Loss: 0.016027871053665876\n",
      "Epoch: 193, Iteration: 700, Loss: 0.01616201049182564\n",
      "Epoch: 193, Iteration: 800, Loss: 0.016337992172339\n",
      "Epoch: 193, Train Loss: 0.0006362665028508199, Test Loss: 0.00016097486356309775\n",
      "Epoch: 194, Iteration: 100, Loss: 0.015726775767689105\n",
      "Epoch: 194, Iteration: 200, Loss: 0.016108747950056568\n",
      "Epoch: 194, Iteration: 300, Loss: 0.016048023877374362\n",
      "Epoch: 194, Iteration: 400, Loss: 0.01613346751400968\n",
      "Epoch: 194, Iteration: 500, Loss: 0.01564636817056453\n",
      "Epoch: 194, Iteration: 600, Loss: 0.015855688929150347\n",
      "Epoch: 194, Iteration: 700, Loss: 0.015953442169120535\n",
      "Epoch: 194, Iteration: 800, Loss: 0.0157082975783851\n",
      "Epoch: 194, Train Loss: 0.000636265295310094, Test Loss: 0.00016097409998990536\n",
      "Epoch: 195, Iteration: 100, Loss: 0.015669599706598092\n",
      "Epoch: 195, Iteration: 200, Loss: 0.016318845075147692\n",
      "Epoch: 195, Iteration: 300, Loss: 0.015741761162644252\n",
      "Epoch: 195, Iteration: 400, Loss: 0.01511717462562956\n",
      "Epoch: 195, Iteration: 500, Loss: 0.01599506294587627\n",
      "Epoch: 195, Iteration: 600, Loss: 0.01584626860130811\n",
      "Epoch: 195, Iteration: 700, Loss: 0.016226354644459207\n",
      "Epoch: 195, Iteration: 800, Loss: 0.016076918560429476\n",
      "Epoch: 195, Train Loss: 0.0006362729860978815, Test Loss: 0.00016097426542573424\n",
      "Epoch: 196, Iteration: 100, Loss: 0.015749542450066656\n",
      "Epoch: 196, Iteration: 200, Loss: 0.01641554550587898\n",
      "Epoch: 196, Iteration: 300, Loss: 0.016626536795229185\n",
      "Epoch: 196, Iteration: 400, Loss: 0.01636350096669048\n",
      "Epoch: 196, Iteration: 500, Loss: 0.015364052138465922\n",
      "Epoch: 196, Iteration: 600, Loss: 0.015445350043592043\n",
      "Epoch: 196, Iteration: 700, Loss: 0.01577122465823777\n",
      "Epoch: 196, Iteration: 800, Loss: 0.015728636004496366\n",
      "Epoch: 196, Train Loss: 0.0006362696863337492, Test Loss: 0.00016097310358606932\n",
      "Epoch: 197, Iteration: 100, Loss: 0.016761666636739392\n",
      "Epoch: 197, Iteration: 200, Loss: 0.015500043118663598\n",
      "Epoch: 197, Iteration: 300, Loss: 0.015847409558773506\n",
      "Epoch: 197, Iteration: 400, Loss: 0.016152405842149165\n",
      "Epoch: 197, Iteration: 500, Loss: 0.01629979085555533\n",
      "Epoch: 197, Iteration: 600, Loss: 0.015900417791272048\n",
      "Epoch: 197, Iteration: 700, Loss: 0.015386092083645053\n",
      "Epoch: 197, Iteration: 800, Loss: 0.015520571119850501\n",
      "Epoch: 197, Train Loss: 0.0006362636406500373, Test Loss: 0.0001609747687744702\n",
      "Epoch: 198, Iteration: 100, Loss: 0.015911834423604887\n",
      "Epoch: 198, Iteration: 200, Loss: 0.015873831929638982\n",
      "Epoch: 198, Iteration: 300, Loss: 0.01575440754822921\n",
      "Epoch: 198, Iteration: 400, Loss: 0.015635301286238246\n",
      "Epoch: 198, Iteration: 500, Loss: 0.015552477809251286\n",
      "Epoch: 198, Iteration: 600, Loss: 0.015743370473501272\n",
      "Epoch: 198, Iteration: 700, Loss: 0.016396366037952248\n",
      "Epoch: 198, Iteration: 800, Loss: 0.016311601037159562\n",
      "Epoch: 198, Train Loss: 0.0006362586787151825, Test Loss: 0.0001609749954021085\n",
      "Epoch: 199, Iteration: 100, Loss: 0.015861720239627175\n",
      "Epoch: 199, Iteration: 200, Loss: 0.01596400161361089\n",
      "Epoch: 199, Iteration: 300, Loss: 0.016220176570641343\n",
      "Epoch: 199, Iteration: 400, Loss: 0.015783632050442975\n",
      "Epoch: 199, Iteration: 500, Loss: 0.01620063312293496\n",
      "Epoch: 199, Iteration: 600, Loss: 0.015904311032500118\n",
      "Epoch: 199, Iteration: 700, Loss: 0.015636722309864126\n",
      "Epoch: 199, Iteration: 800, Loss: 0.0159424357698299\n",
      "Epoch: 199, Train Loss: 0.0006362659004887068, Test Loss: 0.0001609751656326929\n",
      "Epoch: 200, Iteration: 100, Loss: 0.016027938821935095\n",
      "Epoch: 200, Iteration: 200, Loss: 0.01562037529220106\n",
      "Epoch: 200, Iteration: 300, Loss: 0.016213744711421896\n",
      "Epoch: 200, Iteration: 400, Loss: 0.01631574444763828\n",
      "Epoch: 200, Iteration: 500, Loss: 0.016359432913304772\n",
      "Epoch: 200, Iteration: 600, Loss: 0.015842785782297142\n",
      "Epoch: 200, Iteration: 700, Loss: 0.015586345369229093\n",
      "Epoch: 200, Iteration: 800, Loss: 0.015519696949922945\n",
      "Epoch: 200, Train Loss: 0.0006362638817960611, Test Loss: 0.000160975610069643\n",
      "Epoch: 201, Iteration: 100, Loss: 0.0161746776138898\n",
      "Epoch: 201, Iteration: 200, Loss: 0.01585765150957741\n",
      "Epoch: 201, Iteration: 300, Loss: 0.01584451845701551\n",
      "Epoch: 201, Iteration: 400, Loss: 0.015412754481076263\n",
      "Epoch: 201, Iteration: 500, Loss: 0.016099735476018395\n",
      "Epoch: 201, Iteration: 600, Loss: 0.015757352470245678\n",
      "Epoch: 201, Iteration: 700, Loss: 0.016041166774812154\n",
      "Epoch: 201, Iteration: 800, Loss: 0.01600532872544136\n",
      "Epoch: 201, Train Loss: 0.0006362746945061412, Test Loss: 0.0001609737896719896\n",
      "Epoch: 202, Iteration: 100, Loss: 0.016531557761481963\n",
      "Epoch: 202, Iteration: 200, Loss: 0.01662872343149502\n",
      "Epoch: 202, Iteration: 300, Loss: 0.015492762453504838\n",
      "Epoch: 202, Iteration: 400, Loss: 0.015908339315501507\n",
      "Epoch: 202, Iteration: 500, Loss: 0.016099152417154983\n",
      "Epoch: 202, Iteration: 600, Loss: 0.015270312869688496\n",
      "Epoch: 202, Iteration: 700, Loss: 0.015962755132932216\n",
      "Epoch: 202, Iteration: 800, Loss: 0.015622095546859782\n",
      "Epoch: 202, Train Loss: 0.0006362639620327734, Test Loss: 0.00016097375996462348\n",
      "Epoch: 203, Iteration: 100, Loss: 0.016108860130771063\n",
      "Epoch: 203, Iteration: 200, Loss: 0.015541729146207217\n",
      "Epoch: 203, Iteration: 300, Loss: 0.015451021077751648\n",
      "Epoch: 203, Iteration: 400, Loss: 0.01604356297320919\n",
      "Epoch: 203, Iteration: 500, Loss: 0.01591639860998839\n",
      "Epoch: 203, Iteration: 600, Loss: 0.01560357035486959\n",
      "Epoch: 203, Iteration: 700, Loss: 0.01599254108441528\n",
      "Epoch: 203, Iteration: 800, Loss: 0.01628934219479561\n",
      "Epoch: 203, Train Loss: 0.0006362660660586548, Test Loss: 0.00016097481531377513\n",
      "Epoch: 204, Iteration: 100, Loss: 0.015557797727524303\n",
      "Epoch: 204, Iteration: 200, Loss: 0.015818810963537544\n",
      "Epoch: 204, Iteration: 300, Loss: 0.015644375307601877\n",
      "Epoch: 204, Iteration: 400, Loss: 0.016164958542503882\n",
      "Epoch: 204, Iteration: 500, Loss: 0.016203377905185334\n",
      "Epoch: 204, Iteration: 600, Loss: 0.015415889240102842\n",
      "Epoch: 204, Iteration: 700, Loss: 0.01616221344738733\n",
      "Epoch: 204, Iteration: 800, Loss: 0.016298685310175642\n",
      "Epoch: 204, Train Loss: 0.000636266336710866, Test Loss: 0.00016097457584423927\n",
      "Epoch: 205, Iteration: 100, Loss: 0.016364361414161976\n",
      "Epoch: 205, Iteration: 200, Loss: 0.016200721947825514\n",
      "Epoch: 205, Iteration: 300, Loss: 0.015719326001999434\n",
      "Epoch: 205, Iteration: 400, Loss: 0.015522756009886507\n",
      "Epoch: 205, Iteration: 500, Loss: 0.015557610859104898\n",
      "Epoch: 205, Iteration: 600, Loss: 0.015421382595377509\n",
      "Epoch: 205, Iteration: 700, Loss: 0.01593132413108833\n",
      "Epoch: 205, Iteration: 800, Loss: 0.016464636421005707\n",
      "Epoch: 205, Train Loss: 0.0006362590897900229, Test Loss: 0.00016097516264854437\n",
      "Epoch: 206, Iteration: 100, Loss: 0.015952614368870854\n",
      "Epoch: 206, Iteration: 200, Loss: 0.015895343974989373\n",
      "Epoch: 206, Iteration: 300, Loss: 0.015773112521856092\n",
      "Epoch: 206, Iteration: 400, Loss: 0.01588604559219675\n",
      "Epoch: 206, Iteration: 500, Loss: 0.015551449665508699\n",
      "Epoch: 206, Iteration: 600, Loss: 0.016443687338323798\n",
      "Epoch: 206, Iteration: 700, Loss: 0.016063457253039815\n",
      "Epoch: 206, Iteration: 800, Loss: 0.01586521651915973\n",
      "Epoch: 206, Train Loss: 0.0006362607760351121, Test Loss: 0.00016097577252132338\n",
      "Epoch: 207, Iteration: 100, Loss: 0.015892752053332515\n",
      "Epoch: 207, Iteration: 200, Loss: 0.01595185458427295\n",
      "Epoch: 207, Iteration: 300, Loss: 0.015415842543006875\n",
      "Epoch: 207, Iteration: 400, Loss: 0.01637194965587696\n",
      "Epoch: 207, Iteration: 500, Loss: 0.015271421463694423\n",
      "Epoch: 207, Iteration: 600, Loss: 0.015990798536222428\n",
      "Epoch: 207, Iteration: 700, Loss: 0.016238508229434956\n",
      "Epoch: 207, Iteration: 800, Loss: 0.0161860255029751\n",
      "Epoch: 207, Train Loss: 0.0006362614651051809, Test Loss: 0.00016097447361200533\n",
      "Epoch: 208, Iteration: 100, Loss: 0.016402577064582147\n",
      "Epoch: 208, Iteration: 200, Loss: 0.015817023588169832\n",
      "Epoch: 208, Iteration: 300, Loss: 0.016109207077533938\n",
      "Epoch: 208, Iteration: 400, Loss: 0.01564755415165564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 208, Iteration: 500, Loss: 0.015776794389239512\n",
      "Epoch: 208, Iteration: 600, Loss: 0.015820921449630987\n",
      "Epoch: 208, Iteration: 700, Loss: 0.01610967308806721\n",
      "Epoch: 208, Iteration: 800, Loss: 0.015552422155451495\n",
      "Epoch: 208, Train Loss: 0.0006362689778499503, Test Loss: 0.00016097380948807693\n",
      "Epoch: 209, Iteration: 100, Loss: 0.016323363925039303\n",
      "Epoch: 209, Iteration: 200, Loss: 0.015830812706553843\n",
      "Epoch: 209, Iteration: 300, Loss: 0.016261633376416285\n",
      "Epoch: 209, Iteration: 400, Loss: 0.015136707843339536\n",
      "Epoch: 209, Iteration: 500, Loss: 0.015531583572737873\n",
      "Epoch: 209, Iteration: 600, Loss: 0.01631188752799062\n",
      "Epoch: 209, Iteration: 700, Loss: 0.016191501934372354\n",
      "Epoch: 209, Iteration: 800, Loss: 0.015581030784233008\n",
      "Epoch: 209, Train Loss: 0.0006362699584612701, Test Loss: 0.0001609748046513119\n",
      "Epoch: 210, Iteration: 100, Loss: 0.016038696790928952\n",
      "Epoch: 210, Iteration: 200, Loss: 0.015365369203209411\n",
      "Epoch: 210, Iteration: 300, Loss: 0.01572177767957328\n",
      "Epoch: 210, Iteration: 400, Loss: 0.01561975383083336\n",
      "Epoch: 210, Iteration: 500, Loss: 0.015394783185911365\n",
      "Epoch: 210, Iteration: 600, Loss: 0.016506667241628747\n",
      "Epoch: 210, Iteration: 700, Loss: 0.01620450462360168\n",
      "Epoch: 210, Iteration: 800, Loss: 0.016367205636925064\n",
      "Epoch: 210, Train Loss: 0.0006362667580458126, Test Loss: 0.00016097586251519544\n",
      "Epoch: 211, Iteration: 100, Loss: 0.016197992947127204\n",
      "Epoch: 211, Iteration: 200, Loss: 0.015724568511359394\n",
      "Epoch: 211, Iteration: 300, Loss: 0.016109903932374436\n",
      "Epoch: 211, Iteration: 400, Loss: 0.01633219583891332\n",
      "Epoch: 211, Iteration: 500, Loss: 0.016402375556936022\n",
      "Epoch: 211, Iteration: 600, Loss: 0.015432781205163337\n",
      "Epoch: 211, Iteration: 700, Loss: 0.01534650863322895\n",
      "Epoch: 211, Iteration: 800, Loss: 0.016156416277226526\n",
      "Epoch: 211, Train Loss: 0.0006362664023956631, Test Loss: 0.0001609735298163605\n",
      "Epoch: 212, Iteration: 100, Loss: 0.016026556491851807\n",
      "Epoch: 212, Iteration: 200, Loss: 0.01601396893238416\n",
      "Epoch: 212, Iteration: 300, Loss: 0.0161616195255192\n",
      "Epoch: 212, Iteration: 400, Loss: 0.0159015364915831\n",
      "Epoch: 212, Iteration: 500, Loss: 0.015630160494765732\n",
      "Epoch: 212, Iteration: 600, Loss: 0.01606037237070268\n",
      "Epoch: 212, Iteration: 700, Loss: 0.01585281762527302\n",
      "Epoch: 212, Iteration: 800, Loss: 0.015762775670737028\n",
      "Epoch: 212, Train Loss: 0.0006362672425508242, Test Loss: 0.00016097682307571954\n",
      "Epoch: 213, Iteration: 100, Loss: 0.01575311790656997\n",
      "Epoch: 213, Iteration: 200, Loss: 0.015543430847174022\n",
      "Epoch: 213, Iteration: 300, Loss: 0.015341731188527774\n",
      "Epoch: 213, Iteration: 400, Loss: 0.01586714891891461\n",
      "Epoch: 213, Iteration: 500, Loss: 0.016086501644167583\n",
      "Epoch: 213, Iteration: 600, Loss: 0.016321035807777662\n",
      "Epoch: 213, Iteration: 700, Loss: 0.016058269095083233\n",
      "Epoch: 213, Iteration: 800, Loss: 0.016103266250866\n",
      "Epoch: 213, Train Loss: 0.0006362644480801539, Test Loss: 0.0001609748655413535\n",
      "Epoch: 214, Iteration: 100, Loss: 0.015813992395123933\n",
      "Epoch: 214, Iteration: 200, Loss: 0.016277789705782197\n",
      "Epoch: 214, Iteration: 300, Loss: 0.016617892448266502\n",
      "Epoch: 214, Iteration: 400, Loss: 0.015343834485975094\n",
      "Epoch: 214, Iteration: 500, Loss: 0.015550334748695605\n",
      "Epoch: 214, Iteration: 600, Loss: 0.01586919692636002\n",
      "Epoch: 214, Iteration: 700, Loss: 0.016116226550366264\n",
      "Epoch: 214, Iteration: 800, Loss: 0.015429981649504043\n",
      "Epoch: 214, Train Loss: 0.0006362669020896554, Test Loss: 0.0001609748368734099\n",
      "Epoch: 215, Iteration: 100, Loss: 0.015450892002263572\n",
      "Epoch: 215, Iteration: 200, Loss: 0.015812932600965723\n",
      "Epoch: 215, Iteration: 300, Loss: 0.015923882740025874\n",
      "Epoch: 215, Iteration: 400, Loss: 0.015807649215275887\n",
      "Epoch: 215, Iteration: 500, Loss: 0.016619735935819335\n",
      "Epoch: 215, Iteration: 600, Loss: 0.01629316318576457\n",
      "Epoch: 215, Iteration: 700, Loss: 0.01541798174730502\n",
      "Epoch: 215, Iteration: 800, Loss: 0.015721911506261677\n",
      "Epoch: 215, Train Loss: 0.0006362688731700439, Test Loss: 0.0001609750955890272\n",
      "Epoch: 216, Iteration: 100, Loss: 0.016486623040691484\n",
      "Epoch: 216, Iteration: 200, Loss: 0.01581337008974515\n",
      "Epoch: 216, Iteration: 300, Loss: 0.015561987856926862\n",
      "Epoch: 216, Iteration: 400, Loss: 0.016502319216670003\n",
      "Epoch: 216, Iteration: 500, Loss: 0.01540085503802402\n",
      "Epoch: 216, Iteration: 600, Loss: 0.01639974238787545\n",
      "Epoch: 216, Iteration: 700, Loss: 0.015500509871344548\n",
      "Epoch: 216, Iteration: 800, Loss: 0.015694730085670017\n",
      "Epoch: 216, Train Loss: 0.0006362694511224927, Test Loss: 0.00016097424929792037\n",
      "Epoch: 217, Iteration: 100, Loss: 0.01610047503345413\n",
      "Epoch: 217, Iteration: 200, Loss: 0.016066952230175957\n",
      "Epoch: 217, Iteration: 300, Loss: 0.016088695629150607\n",
      "Epoch: 217, Iteration: 400, Loss: 0.015898699457466137\n",
      "Epoch: 217, Iteration: 500, Loss: 0.01616992920753546\n",
      "Epoch: 217, Iteration: 600, Loss: 0.01585896274627885\n",
      "Epoch: 217, Iteration: 700, Loss: 0.015703857599874027\n",
      "Epoch: 217, Iteration: 800, Loss: 0.01578414352115942\n"
     ]
    }
   ],
   "source": [
    "train_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "test_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "\n",
    "patience = config[\"patience\"]\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"])):\n",
    "    loss_ae1_train = 0\n",
    "    loss_ae2_train = 0\n",
    "    loss_dyn_train = 0\n",
    "\n",
    "    current_train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    warmup = 1 if epoch <= config[\"warmup\"] else 0\n",
    "\n",
    "    encoder.train()\n",
    "    dynamics.train()\n",
    "    decoder.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i, (x_t, x_tau) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z_t = encoder(x_t)\n",
    "        x_t_pred = decoder(z_t)\n",
    "\n",
    "        z_tau_pred = dynamics(z_t)\n",
    "        z_tau = encoder(x_tau)\n",
    "        x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_ae1 = criterion(x_t, x_t_pred)\n",
    "        loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "        loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "        loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "        # Backward pass\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += loss_total.item()\n",
    "        epoch_train_loss += loss_total.item()\n",
    "\n",
    "        loss_ae1_train += loss_ae1.item()\n",
    "        loss_ae2_train += loss_ae2.item()\n",
    "        loss_dyn_train += loss_dyn.item() * warmup\n",
    "        counter += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Epoch: {}, Iteration: {}, Loss: {}\".format(epoch, i+1, current_train_loss))\n",
    "            current_train_loss = 0\n",
    "        \n",
    "    train_losses['loss_ae1'].append(loss_ae1_train / counter)\n",
    "    train_losses['loss_ae2'].append(loss_ae2_train / counter)\n",
    "    train_losses['loss_dyn'].append(loss_dyn_train / counter)\n",
    "    train_losses['loss_total'].append(epoch_train_loss / counter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_ae1_test = 0\n",
    "        loss_ae2_test = 0\n",
    "        loss_dyn_test = 0\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        encoder.eval()\n",
    "        dynamics.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        counter = 0\n",
    "        for i, (x_t, x_tau) in enumerate(test_loader,0):\n",
    "            # Forward pass\n",
    "            z_t = encoder(x_t)\n",
    "            x_t_pred = decoder(z_t)\n",
    "\n",
    "            z_tau_pred = dynamics(z_t)\n",
    "            z_tau = encoder(x_tau)\n",
    "            x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_ae1 = criterion(x_t, x_t_pred)\n",
    "            loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "            loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "            loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "            epoch_test_loss += loss_total.item()\n",
    "\n",
    "            loss_ae1_test += loss_ae1.item()\n",
    "            loss_ae2_test += loss_ae2.item()\n",
    "            loss_dyn_test += loss_dyn.item() * warmup\n",
    "            counter += 1\n",
    "\n",
    "        test_losses['loss_ae1'].append(loss_ae1_test / counter)\n",
    "        test_losses['loss_ae2'].append(loss_ae2_test / counter)\n",
    "        test_losses['loss_dyn'].append(loss_dyn_test / counter)\n",
    "        test_losses['loss_total'].append(epoch_test_loss / counter)\n",
    "\n",
    "        if epoch >= patience and np.mean(test_losses['loss_total'][-patience:]) >= np.mean(test_losses['loss_total'][-patience:-1]):\n",
    "            break\n",
    "\n",
    "        if epoch >= config[\"warmup\"]:\n",
    "            scheduler.step(epoch_test_loss / counter)\n",
    "        \n",
    "    print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch, epoch_train_loss / counter, epoch_test_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(encoder, os.path.join(config[\"model_dir\"], \"encoder.pt\"))\n",
    "torch.save(dynamics, os.path.join(config[\"model_dir\"], \"dynamics.pt\"))\n",
    "torch.save(decoder, os.path.join(config[\"model_dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_dir doesn't exist, create it\n",
    "if not os.path.exists(config[\"log_dir\"]):\n",
    "    os.makedirs(config[\"log_dir\"])\n",
    "\n",
    "# Save the losses as a pickle file\n",
    "with open(os.path.join(config[\"log_dir\"], \"losses.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": train_losses, \"test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
