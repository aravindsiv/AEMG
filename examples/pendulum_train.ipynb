{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from AEMG.data_utils import DynamicsDataset\n",
    "from AEMG.systems.utils import get_system\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from AEMG.models import *\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = get_system(\"pendulum\")\n",
    "config_fname = \"config/pendulum_none_1K.txt\"\n",
    "\n",
    "with open(config_fname, 'r') as f:\n",
    "    config = eval(f.read())\n",
    "\n",
    "dataset = DynamicsDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = Encoder(config[\"high_dims\"],config[\"low_dims\"])\n",
    "dynamics = LatentDynamics(config[\"low_dims\"])\n",
    "decoder  = Decoder(config[\"low_dims\"],config[\"high_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(set(list(encoder.parameters()) + list(dynamics.parameters()) + list(decoder.parameters())), \n",
    "    lr=config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.001, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "test_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "\n",
    "patience = config[\"patience\"]\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"])):\n",
    "    loss_ae1_train = 0\n",
    "    loss_ae2_train = 0\n",
    "    loss_dyn_train = 0\n",
    "\n",
    "    current_train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    warmup = 1 if epoch <= config[\"warmup\"] else 0\n",
    "\n",
    "    encoder.train()\n",
    "    dynamics.train()\n",
    "    decoder.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i, (x_t, x_tau) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z_t = encoder(x_t)\n",
    "        x_t_pred = decoder(z_t)\n",
    "\n",
    "        z_tau_pred = dynamics(z_t)\n",
    "        z_tau = encoder(x_tau)\n",
    "        x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_ae1 = criterion(x_t, x_t_pred)\n",
    "        loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "        loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "        loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "        # Backward pass\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += loss_total.item()\n",
    "        epoch_train_loss += loss_total.item()\n",
    "\n",
    "        loss_ae1_train += loss_ae1.item()\n",
    "        loss_ae2_train += loss_ae2.item()\n",
    "        loss_dyn_train += loss_dyn.item() * warmup\n",
    "        counter += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Epoch: {}, Iteration: {}, Loss: {}\".format(epoch, i+1, current_train_loss))\n",
    "            current_train_loss = 0\n",
    "        \n",
    "    train_losses['loss_ae1'].append(loss_ae1_train / counter)\n",
    "    train_losses['loss_ae2'].append(loss_ae2_train / counter)\n",
    "    train_losses['loss_dyn'].append(loss_dyn_train / counter)\n",
    "    train_losses['loss_total'].append(epoch_train_loss / counter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_ae1_test = 0\n",
    "        loss_ae2_test = 0\n",
    "        loss_dyn_test = 0\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        encoder.eval()\n",
    "        dynamics.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        counter = 0\n",
    "        for i, (x_t, x_tau) in enumerate(test_loader,0):\n",
    "            # Forward pass\n",
    "            z_t = encoder(x_t)\n",
    "            x_t_pred = decoder(z_t)\n",
    "\n",
    "            z_tau_pred = dynamics(z_t)\n",
    "            z_tau = encoder(x_tau)\n",
    "            x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_ae1 = criterion(x_t, x_t_pred)\n",
    "            loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "            loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "            loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "            epoch_test_loss += loss_total.item()\n",
    "\n",
    "            loss_ae1_test += loss_ae1.item()\n",
    "            loss_ae2_test += loss_ae2.item()\n",
    "            loss_dyn_test += loss_dyn.item() * warmup\n",
    "            counter += 1\n",
    "\n",
    "        test_losses['loss_ae1'].append(loss_ae1_test / counter)\n",
    "        test_losses['loss_ae2'].append(loss_ae2_test / counter)\n",
    "        test_losses['loss_dyn'].append(loss_dyn_test / counter)\n",
    "        test_losses['loss_total'].append(epoch_test_loss / counter)\n",
    "\n",
    "        if epoch >= patience and np.mean(test_losses['loss_total'][-patience:]) >= np.mean(test_losses['loss_total'][-patience:-1]):\n",
    "            break\n",
    "\n",
    "        if epoch >= config[\"warmup\"]:\n",
    "            scheduler.step(epoch_test_loss / counter)\n",
    "        \n",
    "    print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch, epoch_train_loss / counter, epoch_test_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(encoder, os.path.join(config[\"model_dir\"], \"encoder.pt\"))\n",
    "torch.save(dynamics, os.path.join(config[\"model_dir\"], \"dynamics.pt\"))\n",
    "torch.save(decoder, os.path.join(config[\"model_dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_dir doesn't exist, create it\n",
    "if not os.path.exists(config[\"log_dir\"]):\n",
    "    os.makedirs(config[\"log_dir\"])\n",
    "\n",
    "# Save the losses as a pickle file\n",
    "with open(os.path.join(config[\"log_dir\"], \"losses.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": train_losses, \"test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
