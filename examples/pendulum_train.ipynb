{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from AEMG.data_utils import DynamicsDataset\n",
    "from AEMG.systems.utils import get_system\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "from AEMG.models import *\n",
    "\n",
    "from tqdm.notebook import tqdm \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:24<00:00, 41.27it/s]\n"
     ]
    }
   ],
   "source": [
    "system = get_system(\"pendulum\")\n",
    "# config_fname = \"config/pendulum_lqr_1K.txt\"\n",
    "config_fname = \"config/physics_pendulum.txt\"\n",
    "\n",
    "with open(config_fname, 'r') as f:\n",
    "    config = eval(f.read())\n",
    "\n",
    "dataset = DynamicsDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder  = Encoder(config[\"high_dims\"],config[\"low_dims\"])\n",
    "dynamics = LatentDynamics(config[\"low_dims\"])\n",
    "decoder  = Decoder(config[\"low_dims\"],config[\"high_dims\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(set(list(encoder.parameters()) + list(dynamics.parameters()) + list(decoder.parameters())), \n",
    "    lr=config[\"learning_rate\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, threshold=0.001, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e66d7e130c254957bf2c31f52b7caf20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 9.994495432823896\n",
      "Epoch: 0, Iteration: 200, Loss: 2.9003128670156\n",
      "Epoch: 0, Iteration: 300, Loss: 1.7275239853188396\n",
      "Epoch: 0, Iteration: 400, Loss: 1.2929670689627528\n",
      "Epoch: 0, Iteration: 500, Loss: 1.0083316192030907\n",
      "Epoch: 0, Iteration: 600, Loss: 0.8789923274889588\n",
      "Epoch: 0, Iteration: 700, Loss: 0.8711279924027622\n",
      "Epoch: 0, Train Loss: 0.09743534713052213, Test Loss: 0.008019286694470794\n",
      "Epoch: 1, Iteration: 100, Loss: 0.7187251537106931\n",
      "Epoch: 1, Iteration: 200, Loss: 0.5815311335027218\n",
      "Epoch: 1, Iteration: 300, Loss: 0.5069702477194369\n",
      "Epoch: 1, Iteration: 400, Loss: 0.486048074439168\n",
      "Epoch: 1, Iteration: 500, Loss: 0.4653272957075387\n",
      "Epoch: 1, Iteration: 600, Loss: 0.44439219729974866\n",
      "Epoch: 1, Iteration: 700, Loss: 0.430659027537331\n",
      "Epoch: 1, Train Loss: 0.020214141178876162, Test Loss: 0.004042761436430738\n",
      "Epoch: 2, Iteration: 100, Loss: 0.40772153274156153\n",
      "Epoch: 2, Iteration: 200, Loss: 0.3895947812125087\n",
      "Epoch: 2, Iteration: 300, Loss: 0.39143262594006956\n",
      "Epoch: 2, Iteration: 400, Loss: 0.39585338067263365\n",
      "Epoch: 2, Iteration: 500, Loss: 0.3752116293180734\n",
      "Epoch: 2, Iteration: 600, Loss: 0.38323976611718535\n",
      "Epoch: 2, Iteration: 700, Loss: 0.37818908132612705\n",
      "Epoch: 2, Train Loss: 0.015380332809872926, Test Loss: 0.0036631828430108726\n",
      "Epoch: 3, Iteration: 100, Loss: 0.3681563357822597\n",
      "Epoch: 3, Iteration: 200, Loss: 0.36153888585977256\n",
      "Epoch: 3, Iteration: 300, Loss: 0.3660202238243073\n",
      "Epoch: 3, Iteration: 400, Loss: 0.3678325067739934\n",
      "Epoch: 3, Iteration: 500, Loss: 0.36308022239245474\n",
      "Epoch: 3, Iteration: 600, Loss: 0.3632324649952352\n",
      "Epoch: 3, Iteration: 700, Loss: 0.360782676609233\n",
      "Epoch: 3, Train Loss: 0.014507294736104086, Test Loss: 0.0035515539010521026\n",
      "Epoch: 4, Iteration: 100, Loss: 0.3590517893899232\n",
      "Epoch: 4, Iteration: 200, Loss: 0.35789858899079263\n",
      "Epoch: 4, Iteration: 300, Loss: 0.3528280481696129\n",
      "Epoch: 4, Iteration: 400, Loss: 0.35837112623266876\n",
      "Epoch: 4, Iteration: 500, Loss: 0.3490822387393564\n",
      "Epoch: 4, Iteration: 600, Loss: 0.3556336238980293\n",
      "Epoch: 4, Iteration: 700, Loss: 0.34525158489122987\n",
      "Epoch: 4, Train Loss: 0.014073293731780723, Test Loss: 0.003459734241478145\n",
      "Epoch: 5, Iteration: 100, Loss: 0.339335371972993\n",
      "Epoch: 5, Iteration: 200, Loss: 0.34285219525918365\n",
      "Epoch: 5, Iteration: 300, Loss: 0.33970920112915337\n",
      "Epoch: 5, Iteration: 400, Loss: 0.3478485841769725\n",
      "Epoch: 5, Iteration: 500, Loss: 0.3460927688283846\n",
      "Epoch: 5, Iteration: 600, Loss: 0.3383875950239599\n",
      "Epoch: 5, Iteration: 700, Loss: 0.3423743660096079\n",
      "Epoch: 5, Train Loss: 0.013624536713468843, Test Loss: 0.0033213331387378277\n",
      "Epoch: 6, Iteration: 100, Loss: 0.33561003347858787\n",
      "Epoch: 6, Iteration: 200, Loss: 0.32998393767047673\n",
      "Epoch: 6, Iteration: 300, Loss: 0.3329110383056104\n",
      "Epoch: 6, Iteration: 400, Loss: 0.3245293661020696\n",
      "Epoch: 6, Iteration: 500, Loss: 0.3214327769819647\n",
      "Epoch: 6, Iteration: 600, Loss: 0.3215359791647643\n",
      "Epoch: 6, Iteration: 700, Loss: 0.32364884042181075\n",
      "Epoch: 6, Train Loss: 0.012998753154533915, Test Loss: 0.00314280548482202\n",
      "Epoch: 7, Iteration: 100, Loss: 0.31396854552440345\n",
      "Epoch: 7, Iteration: 200, Loss: 0.30622183298692107\n",
      "Epoch: 7, Iteration: 300, Loss: 0.31475554092321545\n",
      "Epoch: 7, Iteration: 400, Loss: 0.3007020035292953\n",
      "Epoch: 7, Iteration: 500, Loss: 0.31383850472047925\n",
      "Epoch: 7, Iteration: 600, Loss: 0.30845312937162817\n",
      "Epoch: 7, Iteration: 700, Loss: 0.30361149297095835\n",
      "Epoch: 7, Train Loss: 0.01225665091711562, Test Loss: 0.003187009310349822\n",
      "Epoch: 8, Iteration: 100, Loss: 0.29357027169317007\n",
      "Epoch: 8, Iteration: 200, Loss: 0.2973456170875579\n",
      "Epoch: 8, Iteration: 300, Loss: 0.2926612086594105\n",
      "Epoch: 8, Iteration: 400, Loss: 0.28972376976162195\n",
      "Epoch: 8, Iteration: 500, Loss: 0.2784227814991027\n",
      "Epoch: 8, Iteration: 600, Loss: 0.2885434962809086\n",
      "Epoch: 8, Iteration: 700, Loss: 0.2848148322664201\n",
      "Epoch: 8, Train Loss: 0.011488728033145889, Test Loss: 0.0028227400267496704\n",
      "Epoch: 9, Iteration: 100, Loss: 0.2760771121829748\n",
      "Epoch: 9, Iteration: 200, Loss: 0.2845506110461429\n",
      "Epoch: 9, Iteration: 300, Loss: 0.2766110545489937\n",
      "Epoch: 9, Iteration: 400, Loss: 0.27366152266040444\n",
      "Epoch: 9, Iteration: 500, Loss: 0.2749502315418795\n",
      "Epoch: 9, Iteration: 600, Loss: 0.26351841702125967\n",
      "Epoch: 9, Iteration: 700, Loss: 0.2709210158791393\n",
      "Epoch: 9, Train Loss: 0.010931683710077777, Test Loss: 0.0026590752007905395\n",
      "Epoch: 10, Iteration: 100, Loss: 0.266282876720652\n",
      "Epoch: 10, Iteration: 200, Loss: 0.2754065312910825\n",
      "Epoch: 10, Iteration: 300, Loss: 0.2639563421253115\n",
      "Epoch: 10, Iteration: 400, Loss: 0.2609824810642749\n",
      "Epoch: 10, Iteration: 500, Loss: 0.26279463712126017\n",
      "Epoch: 10, Iteration: 600, Loss: 0.25490229576826096\n",
      "Epoch: 10, Iteration: 700, Loss: 0.25621611636597663\n",
      "Epoch: 10, Train Loss: 0.010471874049399048, Test Loss: 0.002496677718590945\n",
      "Epoch: 11, Iteration: 100, Loss: 0.2695753036532551\n",
      "Epoch: 11, Iteration: 200, Loss: 0.2596800326136872\n",
      "Epoch: 11, Iteration: 300, Loss: 0.2543834262760356\n",
      "Epoch: 11, Iteration: 400, Loss: 0.251263624872081\n",
      "Epoch: 11, Iteration: 500, Loss: 0.25543964689131826\n",
      "Epoch: 11, Iteration: 600, Loss: 0.2571744902525097\n",
      "Epoch: 11, Iteration: 700, Loss: 0.2519016187870875\n",
      "Epoch: 11, Train Loss: 0.010189756760955787, Test Loss: 0.0023933235654840248\n",
      "Epoch: 12, Iteration: 100, Loss: 0.25651023688260466\n",
      "Epoch: 12, Iteration: 200, Loss: 0.25242324627470225\n",
      "Epoch: 12, Iteration: 300, Loss: 0.24494361225515604\n",
      "Epoch: 12, Iteration: 400, Loss: 0.25570115307345986\n",
      "Epoch: 12, Iteration: 500, Loss: 0.2484868475003168\n",
      "Epoch: 12, Iteration: 600, Loss: 0.2489959233207628\n",
      "Epoch: 12, Iteration: 700, Loss: 0.2355286955134943\n",
      "Epoch: 12, Train Loss: 0.009888432827428914, Test Loss: 0.00255166127695702\n",
      "Epoch: 13, Iteration: 100, Loss: 0.24075138045009226\n",
      "Epoch: 13, Iteration: 200, Loss: 0.24205213214736432\n",
      "Epoch: 13, Iteration: 300, Loss: 0.25487172545399517\n",
      "Epoch: 13, Iteration: 400, Loss: 0.2310604426311329\n",
      "Epoch: 13, Iteration: 500, Loss: 0.2353837777627632\n",
      "Epoch: 13, Iteration: 600, Loss: 0.2344767622416839\n",
      "Epoch: 13, Iteration: 700, Loss: 0.2355138952843845\n",
      "Epoch: 13, Train Loss: 0.009536857969942502, Test Loss: 0.0023254286556039006\n",
      "Epoch: 14, Iteration: 100, Loss: 0.2394863865338266\n",
      "Epoch: 14, Iteration: 200, Loss: 0.23154452384915203\n",
      "Epoch: 14, Iteration: 300, Loss: 0.24252190196420997\n",
      "Epoch: 14, Iteration: 400, Loss: 0.23324792284984142\n",
      "Epoch: 14, Iteration: 500, Loss: 0.2347314398502931\n",
      "Epoch: 14, Iteration: 600, Loss: 0.22767314373049885\n",
      "Epoch: 14, Iteration: 700, Loss: 0.23143427236936986\n",
      "Epoch: 14, Train Loss: 0.009321255083777942, Test Loss: 0.002328918852144852\n",
      "Epoch: 15, Iteration: 100, Loss: 0.2402539865579456\n",
      "Epoch: 15, Iteration: 200, Loss: 0.2344141568755731\n",
      "Epoch: 15, Iteration: 300, Loss: 0.2274526107357815\n",
      "Epoch: 15, Iteration: 400, Loss: 0.2354943644022569\n",
      "Epoch: 15, Iteration: 500, Loss: 0.22957336530089378\n",
      "Epoch: 15, Iteration: 600, Loss: 0.22708447568584234\n",
      "Epoch: 15, Iteration: 700, Loss: 0.22518198494799435\n",
      "Epoch: 15, Train Loss: 0.009245654516271316, Test Loss: 0.0021908548421924936\n",
      "Epoch: 16, Iteration: 100, Loss: 0.23471513064578176\n",
      "Epoch: 16, Iteration: 200, Loss: 0.2176434117136523\n",
      "Epoch: 16, Iteration: 300, Loss: 0.2301158612826839\n",
      "Epoch: 16, Iteration: 400, Loss: 0.22507162240799516\n",
      "Epoch: 16, Iteration: 500, Loss: 0.2181963084731251\n",
      "Epoch: 16, Iteration: 600, Loss: 0.22077074460685253\n",
      "Epoch: 16, Iteration: 700, Loss: 0.22820501227397472\n",
      "Epoch: 16, Train Loss: 0.0089920188637916, Test Loss: 0.002137609383207746\n",
      "Epoch: 17, Iteration: 100, Loss: 0.22363224916625768\n",
      "Epoch: 17, Iteration: 200, Loss: 0.22837583196815103\n",
      "Epoch: 17, Iteration: 300, Loss: 0.2156727403635159\n",
      "Epoch: 17, Iteration: 400, Loss: 0.22659254760947078\n",
      "Epoch: 17, Iteration: 500, Loss: 0.21354776748921722\n",
      "Epoch: 17, Iteration: 600, Loss: 0.21696119708940387\n",
      "Epoch: 17, Iteration: 700, Loss: 0.21379179612267762\n",
      "Epoch: 17, Train Loss: 0.008746134017128497, Test Loss: 0.002164564017439261\n",
      "Epoch: 18, Iteration: 100, Loss: 0.22083002142608166\n",
      "Epoch: 18, Iteration: 200, Loss: 0.21566502912901342\n",
      "Epoch: 18, Iteration: 300, Loss: 0.20790555234998465\n",
      "Epoch: 18, Iteration: 400, Loss: 0.22821825300343335\n",
      "Epoch: 18, Iteration: 500, Loss: 0.21755858964752406\n",
      "Epoch: 18, Iteration: 600, Loss: 0.21888414525892586\n",
      "Epoch: 18, Iteration: 700, Loss: 0.2165864296257496\n",
      "Epoch: 18, Train Loss: 0.008683974361629225, Test Loss: 0.0023519560939166697\n",
      "Epoch: 19, Iteration: 100, Loss: 0.21332974161487073\n",
      "Epoch: 19, Iteration: 200, Loss: 0.2126713030738756\n",
      "Epoch: 19, Iteration: 300, Loss: 0.21998411021195352\n",
      "Epoch: 19, Iteration: 400, Loss: 0.21210396254900843\n",
      "Epoch: 19, Iteration: 500, Loss: 0.21842491428833455\n",
      "Epoch: 19, Iteration: 600, Loss: 0.2106957882642746\n",
      "Epoch: 19, Iteration: 700, Loss: 0.21321464085485786\n",
      "Epoch: 19, Train Loss: 0.008537080531241373, Test Loss: 0.00201948965259362\n",
      "Epoch: 20, Iteration: 100, Loss: 0.2101665858644992\n",
      "Epoch: 20, Iteration: 200, Loss: 0.20729319204110652\n",
      "Epoch: 20, Iteration: 300, Loss: 0.20925122336484492\n",
      "Epoch: 20, Iteration: 400, Loss: 0.21285871311556548\n",
      "Epoch: 20, Iteration: 500, Loss: 0.2272964696167037\n",
      "Epoch: 20, Iteration: 600, Loss: 0.2100035878829658\n",
      "Epoch: 20, Iteration: 700, Loss: 0.209153734263964\n",
      "Epoch: 20, Train Loss: 0.008522857920615934, Test Loss: 0.0020365824591135604\n",
      "Epoch: 21, Iteration: 100, Loss: 0.20907868794165552\n",
      "Epoch: 21, Iteration: 200, Loss: 0.21445394726470113\n",
      "Epoch: 21, Iteration: 300, Loss: 0.21083206101320684\n",
      "Epoch: 21, Iteration: 400, Loss: 0.2069845920195803\n",
      "Epoch: 21, Iteration: 500, Loss: 0.20393316389527172\n",
      "Epoch: 21, Iteration: 600, Loss: 0.21215319458860904\n",
      "Epoch: 21, Iteration: 700, Loss: 0.20416594890411943\n",
      "Epoch: 21, Train Loss: 0.008342141784378327, Test Loss: 0.002019132782588713\n",
      "Epoch: 22, Iteration: 100, Loss: 0.21565234777517617\n",
      "Epoch: 22, Iteration: 200, Loss: 0.21388562477659434\n",
      "Epoch: 22, Iteration: 300, Loss: 0.21011796058155596\n",
      "Epoch: 22, Iteration: 400, Loss: 0.20386502903420478\n",
      "Epoch: 22, Iteration: 500, Loss: 0.20480933459475636\n",
      "Epoch: 22, Iteration: 600, Loss: 0.2003208720125258\n",
      "Epoch: 22, Iteration: 700, Loss: 0.20925196108873934\n",
      "Epoch: 22, Train Loss: 0.00831302412552759, Test Loss: 0.0021273046504938975\n",
      "Epoch: 23, Iteration: 100, Loss: 0.20709560555405915\n",
      "Epoch: 23, Iteration: 200, Loss: 0.207104942179285\n",
      "Epoch: 23, Iteration: 300, Loss: 0.20380324055440724\n",
      "Epoch: 23, Iteration: 400, Loss: 0.20803833263926208\n",
      "Epoch: 23, Iteration: 500, Loss: 0.2149163024732843\n",
      "Epoch: 23, Iteration: 600, Loss: 0.2070616427809\n",
      "Epoch: 23, Iteration: 700, Loss: 0.20280005678068846\n",
      "Epoch: 23, Train Loss: 0.008257182026281952, Test Loss: 0.0020459057250991463\n",
      "Epoch: 24, Iteration: 100, Loss: 0.20578285760711879\n",
      "Epoch: 24, Iteration: 200, Loss: 0.2001734800869599\n",
      "Epoch: 24, Iteration: 300, Loss: 0.19760634505655617\n",
      "Epoch: 24, Iteration: 400, Loss: 0.20428184582851827\n",
      "Epoch: 24, Iteration: 500, Loss: 0.2043397434754297\n",
      "Epoch: 24, Iteration: 600, Loss: 0.202632992179133\n",
      "Epoch: 24, Iteration: 700, Loss: 0.20078988024033606\n",
      "Epoch: 24, Train Loss: 0.008078872995683923, Test Loss: 0.0021718623157357796\n",
      "Epoch: 25, Iteration: 100, Loss: 0.19415080768521875\n",
      "Epoch: 25, Iteration: 200, Loss: 0.20295872841961682\n",
      "Epoch: 25, Iteration: 300, Loss: 0.20069523446727544\n",
      "Epoch: 25, Iteration: 400, Loss: 0.20447004248853773\n",
      "Epoch: 25, Iteration: 500, Loss: 0.18724251398816705\n",
      "Epoch: 25, Iteration: 600, Loss: 0.20478183415252715\n",
      "Epoch: 25, Iteration: 700, Loss: 0.198197114339564\n",
      "Epoch: 25, Train Loss: 0.007997550416912419, Test Loss: 0.0019249089556979016\n",
      "Epoch: 26, Iteration: 100, Loss: 0.1928566781571135\n",
      "Epoch: 26, Iteration: 200, Loss: 0.19611180340871215\n",
      "Epoch: 26, Iteration: 300, Loss: 0.20131539704743773\n",
      "Epoch: 26, Iteration: 400, Loss: 0.20712480728980154\n",
      "Epoch: 26, Iteration: 500, Loss: 0.2013174690073356\n",
      "Epoch: 26, Iteration: 600, Loss: 0.19971002207603306\n",
      "Epoch: 26, Iteration: 700, Loss: 0.19266542594414204\n",
      "Epoch: 26, Train Loss: 0.007928375470801257, Test Loss: 0.0018865964439464733\n",
      "Epoch: 27, Iteration: 100, Loss: 0.19773323752451688\n",
      "Epoch: 27, Iteration: 200, Loss: 0.19253610691521317\n",
      "Epoch: 27, Iteration: 300, Loss: 0.2009050108026713\n",
      "Epoch: 27, Iteration: 400, Loss: 0.20049418322741985\n",
      "Epoch: 27, Iteration: 500, Loss: 0.1925584365380928\n",
      "Epoch: 27, Iteration: 600, Loss: 0.19032061251346022\n",
      "Epoch: 27, Iteration: 700, Loss: 0.1988288046559319\n",
      "Epoch: 27, Train Loss: 0.007818135608686135, Test Loss: 0.001971110159647651\n",
      "Epoch: 28, Iteration: 100, Loss: 0.1923197879223153\n",
      "Epoch: 28, Iteration: 200, Loss: 0.19017339847050607\n",
      "Epoch: 28, Iteration: 300, Loss: 0.19970924989320338\n",
      "Epoch: 28, Iteration: 400, Loss: 0.19523766986094415\n",
      "Epoch: 28, Iteration: 500, Loss: 0.20268337579909712\n",
      "Epoch: 28, Iteration: 600, Loss: 0.18738555489107966\n",
      "Epoch: 28, Iteration: 700, Loss: 0.19958097650669515\n",
      "Epoch: 28, Train Loss: 0.007804204844287597, Test Loss: 0.001930379094555974\n",
      "Epoch: 29, Iteration: 100, Loss: 0.20175662729889154\n",
      "Epoch: 29, Iteration: 200, Loss: 0.20203575445339084\n",
      "Epoch: 29, Iteration: 300, Loss: 0.19256580108776689\n",
      "Epoch: 29, Iteration: 400, Loss: 0.18480731919407845\n",
      "Epoch: 29, Iteration: 500, Loss: 0.19707909249700606\n",
      "Epoch: 29, Iteration: 600, Loss: 0.19615195679944009\n",
      "Epoch: 29, Iteration: 700, Loss: 0.1953769844258204\n",
      "Epoch: 29, Train Loss: 0.0077676481846719984, Test Loss: 0.0018840696837287396\n",
      "Epoch: 30, Iteration: 100, Loss: 0.19066188810393214\n",
      "Epoch: 30, Iteration: 200, Loss: 0.20716277498286217\n",
      "Epoch: 30, Iteration: 300, Loss: 0.1914317315677181\n",
      "Epoch: 30, Iteration: 400, Loss: 0.18850872211623937\n",
      "Epoch: 30, Iteration: 500, Loss: 0.1902780681848526\n",
      "Epoch: 30, Iteration: 600, Loss: 0.18333808379247785\n",
      "Epoch: 30, Iteration: 700, Loss: 0.1916320009622723\n",
      "Epoch: 30, Train Loss: 0.007648045230307616, Test Loss: 0.002041098042973317\n",
      "Epoch: 31, Iteration: 100, Loss: 0.1912818195996806\n",
      "Epoch: 31, Iteration: 200, Loss: 0.18009762337896973\n",
      "Epoch: 31, Iteration: 300, Loss: 0.1885187498992309\n",
      "Epoch: 31, Iteration: 400, Loss: 0.19820357114076614\n",
      "Epoch: 31, Iteration: 500, Loss: 0.18998886097688228\n",
      "Epoch: 31, Iteration: 600, Loss: 0.19084304734133184\n",
      "Epoch: 31, Iteration: 700, Loss: 0.18641370546538383\n",
      "Epoch: 31, Train Loss: 0.007596725521725602, Test Loss: 0.0021909239381784572\n",
      "Epoch: 32, Iteration: 100, Loss: 0.189486718387343\n",
      "Epoch: 32, Iteration: 200, Loss: 0.18354894616641104\n",
      "Epoch: 32, Iteration: 300, Loss: 0.18115865753497928\n",
      "Epoch: 32, Iteration: 400, Loss: 0.18293197511229664\n",
      "Epoch: 32, Iteration: 500, Loss: 0.19784910755697638\n",
      "Epoch: 32, Iteration: 600, Loss: 0.18417893932200968\n",
      "Epoch: 32, Iteration: 700, Loss: 0.1890060598962009\n",
      "Epoch: 32, Train Loss: 0.007418664523866028, Test Loss: 0.0019063270778860897\n",
      "Epoch: 33, Iteration: 100, Loss: 0.1847276381449774\n",
      "Epoch: 33, Iteration: 200, Loss: 0.1888812017859891\n",
      "Epoch: 33, Iteration: 300, Loss: 0.17979223234578967\n",
      "Epoch: 33, Iteration: 400, Loss: 0.18166191107593477\n",
      "Epoch: 33, Iteration: 500, Loss: 0.187829983769916\n",
      "Epoch: 33, Iteration: 600, Loss: 0.18353256420232356\n",
      "Epoch: 33, Iteration: 700, Loss: 0.19695979240350425\n",
      "Epoch: 33, Train Loss: 0.007453485553269274, Test Loss: 0.0019496138131944462\n",
      "Epoch: 34, Iteration: 100, Loss: 0.19587222952395678\n",
      "Epoch: 34, Iteration: 200, Loss: 0.194155418779701\n",
      "Epoch: 34, Iteration: 300, Loss: 0.18269246548879892\n",
      "Epoch: 34, Iteration: 400, Loss: 0.181845439132303\n",
      "Epoch: 34, Iteration: 500, Loss: 0.17650667356792837\n",
      "Epoch: 34, Iteration: 600, Loss: 0.1864785305224359\n",
      "Epoch: 34, Iteration: 700, Loss: 0.18387736368458718\n",
      "Epoch: 34, Train Loss: 0.007393261601682752, Test Loss: 0.0018273328087525442\n",
      "Epoch: 35, Iteration: 100, Loss: 0.1874689923133701\n",
      "Epoch: 35, Iteration: 200, Loss: 0.18496103724464774\n",
      "Epoch: 35, Iteration: 300, Loss: 0.1800141801359132\n",
      "Epoch: 35, Iteration: 400, Loss: 0.18841452442575246\n",
      "Epoch: 35, Iteration: 500, Loss: 0.18301732919644564\n",
      "Epoch: 35, Iteration: 600, Loss: 0.18580824497621506\n",
      "Epoch: 35, Iteration: 700, Loss: 0.18038659496232867\n",
      "Epoch: 35, Train Loss: 0.007341271988698281, Test Loss: 0.001799752987571992\n",
      "Epoch: 36, Iteration: 100, Loss: 0.18085143459029496\n",
      "Epoch: 36, Iteration: 200, Loss: 0.17383541027083993\n",
      "Epoch: 36, Iteration: 300, Loss: 0.18841890676412731\n",
      "Epoch: 36, Iteration: 400, Loss: 0.18061100330669433\n",
      "Epoch: 36, Iteration: 500, Loss: 0.1883805487304926\n",
      "Epoch: 36, Iteration: 600, Loss: 0.21338919119443744\n",
      "Epoch: 36, Iteration: 700, Loss: 0.18038541730493307\n",
      "Epoch: 36, Train Loss: 0.007445391103392467, Test Loss: 0.001981971964123659\n",
      "Epoch: 37, Iteration: 100, Loss: 0.18227588525041938\n",
      "Epoch: 37, Iteration: 200, Loss: 0.18265417939983308\n",
      "Epoch: 37, Iteration: 300, Loss: 0.17490242433268577\n",
      "Epoch: 37, Iteration: 400, Loss: 0.17597861087415367\n",
      "Epoch: 37, Iteration: 500, Loss: 0.18297293328214437\n",
      "Epoch: 37, Iteration: 600, Loss: 0.17409030930139124\n",
      "Epoch: 37, Iteration: 700, Loss: 0.18494834925513715\n",
      "Epoch: 37, Train Loss: 0.007157194779720158, Test Loss: 0.0016719554329756648\n",
      "Epoch: 38, Iteration: 100, Loss: 0.18667192151769996\n",
      "Epoch: 38, Iteration: 200, Loss: 0.1740140828769654\n",
      "Epoch: 38, Iteration: 300, Loss: 0.1809772920096293\n",
      "Epoch: 38, Iteration: 400, Loss: 0.17611751856748015\n",
      "Epoch: 38, Iteration: 500, Loss: 0.184056363417767\n",
      "Epoch: 38, Iteration: 600, Loss: 0.18014718603808433\n",
      "Epoch: 38, Iteration: 700, Loss: 0.1813790164887905\n",
      "Epoch: 38, Train Loss: 0.007212183061637916, Test Loss: 0.001711826574173756\n",
      "Epoch: 39, Iteration: 100, Loss: 0.17708695435430855\n",
      "Epoch: 39, Iteration: 200, Loss: 0.19132583762984723\n",
      "Epoch: 39, Iteration: 300, Loss: 0.18464678199961782\n",
      "Epoch: 39, Iteration: 400, Loss: 0.17035918682813644\n",
      "Epoch: 39, Iteration: 500, Loss: 0.1745293849380687\n",
      "Epoch: 39, Iteration: 600, Loss: 0.194090545643121\n",
      "Epoch: 39, Iteration: 700, Loss: 0.17623025365173817\n",
      "Epoch: 39, Train Loss: 0.007277611219906248, Test Loss: 0.0016414506878936663\n",
      "Epoch: 40, Iteration: 100, Loss: 0.17761130107101053\n",
      "Epoch: 40, Iteration: 200, Loss: 0.17931913991924375\n",
      "Epoch: 40, Iteration: 300, Loss: 0.18960837263148278\n",
      "Epoch: 40, Iteration: 400, Loss: 0.1787706466857344\n",
      "Epoch: 40, Iteration: 500, Loss: 0.17297353618778288\n",
      "Epoch: 40, Iteration: 600, Loss: 0.18679875554516912\n",
      "Epoch: 40, Iteration: 700, Loss: 0.17509479774162173\n",
      "Epoch: 40, Train Loss: 0.007203491030377335, Test Loss: 0.001840794132440351\n",
      "Epoch: 41, Iteration: 100, Loss: 0.17993802565615624\n",
      "Epoch: 41, Iteration: 200, Loss: 0.1800426299450919\n",
      "Epoch: 41, Iteration: 300, Loss: 0.17732442216947675\n",
      "Epoch: 41, Iteration: 400, Loss: 0.17157956317532808\n",
      "Epoch: 41, Iteration: 500, Loss: 0.17300410382449627\n",
      "Epoch: 41, Iteration: 600, Loss: 0.1746694064931944\n",
      "Epoch: 41, Iteration: 700, Loss: 0.17566319822799414\n",
      "Epoch: 41, Train Loss: 0.006979741994291544, Test Loss: 0.0016955585771938786\n",
      "Epoch: 42, Iteration: 100, Loss: 0.17509649123530835\n",
      "Epoch: 42, Iteration: 200, Loss: 0.17554737615864724\n",
      "Epoch: 42, Iteration: 300, Loss: 0.16918611933942884\n",
      "Epoch: 42, Iteration: 400, Loss: 0.1749824561411515\n",
      "Epoch: 42, Iteration: 500, Loss: 0.17285632528364658\n",
      "Epoch: 42, Iteration: 600, Loss: 0.1798396441154182\n",
      "Epoch: 42, Iteration: 700, Loss: 0.18066598440054804\n",
      "Epoch: 42, Train Loss: 0.006996432105079293, Test Loss: 0.0017220990249188616\n",
      "Epoch: 43, Iteration: 100, Loss: 0.16820346750319004\n",
      "Epoch: 43, Iteration: 200, Loss: 0.18311439105309546\n",
      "Epoch: 43, Iteration: 300, Loss: 0.1636878059944138\n",
      "Epoch: 43, Iteration: 400, Loss: 0.18961727188434452\n",
      "Epoch: 43, Iteration: 500, Loss: 0.1681583106983453\n",
      "Epoch: 43, Iteration: 600, Loss: 0.17848177545238286\n",
      "Epoch: 43, Iteration: 700, Loss: 0.18426455289591104\n",
      "Epoch: 43, Train Loss: 0.007014724952168763, Test Loss: 0.0016679222852690146\n",
      "Epoch: 44, Iteration: 100, Loss: 0.1721437848173082\n",
      "Epoch: 44, Iteration: 200, Loss: 0.17298786784522235\n",
      "Epoch: 44, Iteration: 300, Loss: 0.17098539986182004\n",
      "Epoch: 44, Iteration: 400, Loss: 0.16447563224937767\n",
      "Epoch: 44, Iteration: 500, Loss: 0.17140850808937103\n",
      "Epoch: 44, Iteration: 600, Loss: 0.1782126547768712\n",
      "Epoch: 44, Iteration: 700, Loss: 0.17598126409575343\n",
      "Epoch: 44, Train Loss: 0.006882107992423698, Test Loss: 0.0016216253605671226\n",
      "Epoch: 45, Iteration: 100, Loss: 0.17402312636841089\n",
      "Epoch: 45, Iteration: 200, Loss: 0.17426517012063414\n",
      "Epoch: 45, Iteration: 300, Loss: 0.1724013175116852\n",
      "Epoch: 45, Iteration: 400, Loss: 0.18371711357031018\n",
      "Epoch: 45, Iteration: 500, Loss: 0.17525501584168524\n",
      "Epoch: 45, Iteration: 600, Loss: 0.17034356796648353\n",
      "Epoch: 45, Iteration: 700, Loss: 0.1619420557981357\n",
      "Epoch: 45, Train Loss: 0.006924071268877015, Test Loss: 0.0016667851753300056\n",
      "Epoch: 46, Iteration: 100, Loss: 0.16962478798814118\n",
      "Epoch: 46, Iteration: 200, Loss: 0.17867614014539868\n",
      "Epoch: 46, Iteration: 300, Loss: 0.16368476685602218\n",
      "Epoch: 46, Iteration: 400, Loss: 0.17528283270075917\n",
      "Epoch: 46, Iteration: 500, Loss: 0.17638364131562412\n",
      "Epoch: 46, Iteration: 600, Loss: 0.1820181452203542\n",
      "Epoch: 46, Iteration: 700, Loss: 0.17524053464876488\n",
      "Epoch: 46, Train Loss: 0.006970449799264315, Test Loss: 0.002033604849711992\n",
      "Epoch: 47, Iteration: 100, Loss: 0.18156961782369763\n",
      "Epoch: 47, Iteration: 200, Loss: 0.165726201958023\n",
      "Epoch: 47, Iteration: 300, Loss: 0.1793788819340989\n",
      "Epoch: 47, Iteration: 400, Loss: 0.1660290175350383\n",
      "Epoch: 47, Iteration: 500, Loss: 0.17764685803558677\n",
      "Epoch: 47, Iteration: 600, Loss: 0.18303259485401213\n",
      "Epoch: 47, Iteration: 700, Loss: 0.17405381181742996\n",
      "Epoch: 47, Train Loss: 0.006969808070571162, Test Loss: 0.001897110515856184\n",
      "Epoch: 48, Iteration: 100, Loss: 0.1692574949702248\n",
      "Epoch: 48, Iteration: 200, Loss: 0.1727095392998308\n",
      "Epoch: 48, Iteration: 300, Loss: 0.1789108713855967\n",
      "Epoch: 48, Iteration: 400, Loss: 0.1754613562952727\n",
      "Epoch: 48, Iteration: 500, Loss: 0.17694978707004339\n",
      "Epoch: 48, Iteration: 600, Loss: 0.16481899353675544\n",
      "Epoch: 48, Iteration: 700, Loss: 0.17742950899992138\n",
      "Epoch: 48, Train Loss: 0.006915324577712454, Test Loss: 0.0017584539024392144\n",
      "Epoch: 49, Iteration: 100, Loss: 0.171371640288271\n",
      "Epoch: 49, Iteration: 200, Loss: 0.17723636433947831\n",
      "Epoch: 49, Iteration: 300, Loss: 0.16607793618459255\n",
      "Epoch: 49, Iteration: 400, Loss: 0.18530312436632812\n",
      "Epoch: 49, Iteration: 500, Loss: 0.1755895046517253\n",
      "Epoch: 49, Iteration: 600, Loss: 0.16454536688979715\n",
      "Epoch: 49, Iteration: 700, Loss: 0.1660699537023902\n",
      "Epoch: 49, Train Loss: 0.006886895192728844, Test Loss: 0.0017639950016746298\n",
      "Epoch: 50, Iteration: 100, Loss: 0.17564122704789042\n",
      "Epoch: 50, Iteration: 200, Loss: 0.16119665186852217\n",
      "Epoch: 50, Iteration: 300, Loss: 0.17817359417676926\n",
      "Epoch: 50, Iteration: 400, Loss: 0.1759636193746701\n",
      "Epoch: 50, Iteration: 500, Loss: 0.17566455819178373\n",
      "Epoch: 50, Iteration: 600, Loss: 0.168872618698515\n",
      "Epoch: 50, Iteration: 700, Loss: 0.16664563782978803\n",
      "Epoch: 50, Train Loss: 0.0068217139452463015, Test Loss: 0.0016093298917985522\n",
      "Epoch: 51, Iteration: 100, Loss: 0.1721668615937233\n",
      "Epoch: 51, Iteration: 200, Loss: 0.17307305138092488\n",
      "Epoch: 51, Iteration: 300, Loss: 0.1761457014363259\n",
      "Epoch: 51, Iteration: 400, Loss: 0.17768984218128026\n",
      "Epoch: 51, Iteration: 500, Loss: 0.16432286915369332\n",
      "Epoch: 51, Iteration: 600, Loss: 0.16631212015636265\n",
      "Epoch: 51, Iteration: 700, Loss: 0.16897895885631442\n",
      "Epoch: 51, Train Loss: 0.006888659248943441, Test Loss: 0.001566791412769817\n",
      "Epoch: 52, Iteration: 100, Loss: 0.16995011107064784\n",
      "Epoch: 52, Iteration: 200, Loss: 0.17691456916509196\n",
      "Epoch: 52, Iteration: 300, Loss: 0.18844175420235842\n",
      "Epoch: 52, Iteration: 400, Loss: 0.1666249476838857\n",
      "Epoch: 52, Iteration: 500, Loss: 0.17552759475074708\n",
      "Epoch: 52, Iteration: 600, Loss: 0.17270407814066857\n",
      "Epoch: 52, Iteration: 700, Loss: 0.17716071126051247\n",
      "Epoch: 52, Train Loss: 0.006931385280040558, Test Loss: 0.0016797591073554941\n",
      "Epoch: 53, Iteration: 100, Loss: 0.16161885613109916\n",
      "Epoch: 53, Iteration: 200, Loss: 0.1607162935542874\n",
      "Epoch: 53, Iteration: 300, Loss: 0.17307764547877014\n",
      "Epoch: 53, Iteration: 400, Loss: 0.17707033641636372\n",
      "Epoch: 53, Iteration: 500, Loss: 0.18703065416775644\n",
      "Epoch: 53, Iteration: 600, Loss: 0.1638974848901853\n",
      "Epoch: 53, Iteration: 700, Loss: 0.17161688377382234\n",
      "Epoch: 53, Train Loss: 0.0068294981314102185, Test Loss: 0.0017281798197655008\n",
      "Epoch: 54, Iteration: 100, Loss: 0.1618520695483312\n",
      "Epoch: 54, Iteration: 200, Loss: 0.15966837073210627\n",
      "Epoch: 54, Iteration: 300, Loss: 0.17047826724592596\n",
      "Epoch: 54, Iteration: 400, Loss: 0.18321300542447716\n",
      "Epoch: 54, Iteration: 500, Loss: 0.17184064723551273\n",
      "Epoch: 54, Iteration: 600, Loss: 0.16255330492276698\n",
      "Epoch: 54, Iteration: 700, Loss: 0.16775903827510774\n",
      "Epoch: 54, Train Loss: 0.006702676184650045, Test Loss: 0.0016114956588717177\n",
      "Epoch: 55, Iteration: 100, Loss: 0.15921072755008936\n",
      "Epoch: 55, Iteration: 200, Loss: 0.160840806143824\n",
      "Epoch: 55, Iteration: 300, Loss: 0.16662290145177394\n",
      "Epoch: 55, Iteration: 400, Loss: 0.16554493003059179\n",
      "Epoch: 55, Iteration: 500, Loss: 0.17523730697575957\n",
      "Epoch: 55, Iteration: 600, Loss: 0.17619500623550266\n",
      "Epoch: 55, Iteration: 700, Loss: 0.16961969423573464\n",
      "Epoch: 55, Train Loss: 0.0067276943914475855, Test Loss: 0.0016664728958858178\n",
      "Epoch: 56, Iteration: 100, Loss: 0.17026148457080126\n",
      "Epoch: 56, Iteration: 200, Loss: 0.1635177287971601\n",
      "Epoch: 56, Iteration: 300, Loss: 0.16901654552202672\n",
      "Epoch: 56, Iteration: 400, Loss: 0.16318537283223122\n",
      "Epoch: 56, Iteration: 500, Loss: 0.17037158471066505\n",
      "Epoch: 56, Iteration: 600, Loss: 0.177812019654084\n",
      "Epoch: 56, Iteration: 700, Loss: 0.16769556479994208\n",
      "Epoch: 56, Train Loss: 0.006737375646189321, Test Loss: 0.0015610300534171983\n",
      "Epoch: 57, Iteration: 100, Loss: 0.17116861883550882\n",
      "Epoch: 57, Iteration: 200, Loss: 0.17688835377339274\n",
      "Epoch: 57, Iteration: 300, Loss: 0.17103167495224625\n",
      "Epoch: 57, Iteration: 400, Loss: 0.15768323466181755\n",
      "Epoch: 57, Iteration: 500, Loss: 0.16545590717578307\n",
      "Epoch: 57, Iteration: 600, Loss: 0.16335462202550843\n",
      "Epoch: 57, Iteration: 700, Loss: 0.1721704526571557\n",
      "Epoch: 57, Train Loss: 0.006708287452347576, Test Loss: 0.0016342230170266702\n",
      "Epoch: 58, Iteration: 100, Loss: 0.16784772975370288\n",
      "Epoch: 58, Iteration: 200, Loss: 0.16336647066054866\n",
      "Epoch: 58, Iteration: 300, Loss: 0.16749300819355994\n",
      "Epoch: 58, Iteration: 400, Loss: 0.16441279859282076\n",
      "Epoch: 58, Iteration: 500, Loss: 0.17065328650642186\n",
      "Epoch: 58, Iteration: 600, Loss: 0.162257430376485\n",
      "Epoch: 58, Iteration: 700, Loss: 0.1779359389329329\n",
      "Epoch: 58, Train Loss: 0.006722684467385989, Test Loss: 0.0018082727113505825\n",
      "Epoch: 59, Iteration: 100, Loss: 0.16794041596585885\n",
      "Epoch: 59, Iteration: 200, Loss: 0.17261966469231993\n",
      "Epoch: 59, Iteration: 300, Loss: 0.1711996269878\n",
      "Epoch: 59, Iteration: 400, Loss: 0.15844078757800162\n",
      "Epoch: 59, Iteration: 500, Loss: 0.1686810173559934\n",
      "Epoch: 59, Iteration: 600, Loss: 0.16568655706942081\n",
      "Epoch: 59, Iteration: 700, Loss: 0.16965632734354585\n",
      "Epoch: 59, Train Loss: 0.006657538616564125, Test Loss: 0.0016307222715113312\n",
      "Epoch: 60, Iteration: 100, Loss: 0.1600477558095008\n",
      "Epoch: 60, Iteration: 200, Loss: 0.16158346377778798\n",
      "Epoch: 60, Iteration: 300, Loss: 0.1561873322352767\n",
      "Epoch: 60, Iteration: 400, Loss: 0.16357174050062895\n",
      "Epoch: 60, Iteration: 500, Loss: 0.16230081644607708\n",
      "Epoch: 60, Iteration: 600, Loss: 0.16118739417288452\n",
      "Epoch: 60, Iteration: 700, Loss: 0.16742975660599768\n",
      "Epoch: 60, Train Loss: 0.006537354907195549, Test Loss: 0.0017874126543756573\n",
      "Epoch: 61, Iteration: 100, Loss: 0.16145452158525586\n",
      "Epoch: 61, Iteration: 200, Loss: 0.17194822820601985\n",
      "Epoch: 61, Iteration: 300, Loss: 0.16164222708903253\n",
      "Epoch: 61, Iteration: 400, Loss: 0.17265118507202715\n",
      "Epoch: 61, Iteration: 500, Loss: 0.15808940667193383\n",
      "Epoch: 61, Iteration: 600, Loss: 0.16525290720164776\n",
      "Epoch: 61, Iteration: 700, Loss: 0.1663769808365032\n",
      "Epoch: 61, Train Loss: 0.00665389717934886, Test Loss: 0.0016513433170621283\n",
      "Epoch: 62, Iteration: 100, Loss: 0.16629162046592683\n",
      "Epoch: 62, Iteration: 200, Loss: 0.15957159613026306\n",
      "Epoch: 62, Iteration: 300, Loss: 0.15720064635388553\n",
      "Epoch: 62, Iteration: 400, Loss: 0.16188076074467972\n",
      "Epoch: 62, Iteration: 500, Loss: 0.16190835577435791\n",
      "Epoch: 62, Iteration: 600, Loss: 0.16810771770542488\n",
      "Epoch: 62, Iteration: 700, Loss: 0.15678882104111835\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch: 62, Train Loss: 0.006443890008958988, Test Loss: 0.0016286221565678717\n",
      "Epoch: 63, Iteration: 100, Loss: 0.14727025316096842\n",
      "Epoch: 63, Iteration: 200, Loss: 0.14019352127797902\n",
      "Epoch: 63, Iteration: 300, Loss: 0.15081337094306946\n",
      "Epoch: 63, Iteration: 400, Loss: 0.14246617251774296\n",
      "Epoch: 63, Iteration: 500, Loss: 0.14595944859320298\n",
      "Epoch: 63, Iteration: 600, Loss: 0.1417017870116979\n",
      "Epoch: 63, Iteration: 700, Loss: 0.13946299499366432\n",
      "Epoch: 63, Train Loss: 0.005751078985340428, Test Loss: 0.0014298384511494079\n",
      "Epoch: 64, Iteration: 100, Loss: 0.14470082568004727\n",
      "Epoch: 64, Iteration: 200, Loss: 0.1429858000483364\n",
      "Epoch: 64, Iteration: 300, Loss: 0.1452603856450878\n",
      "Epoch: 64, Iteration: 400, Loss: 0.14572434243746102\n",
      "Epoch: 64, Iteration: 500, Loss: 0.1426405860693194\n",
      "Epoch: 64, Iteration: 600, Loss: 0.14333407738013193\n",
      "Epoch: 64, Iteration: 700, Loss: 0.14302415354177356\n",
      "Epoch: 64, Train Loss: 0.005731926223670598, Test Loss: 0.0014132480774424038\n",
      "Epoch: 65, Iteration: 100, Loss: 0.14821850374573842\n",
      "Epoch: 65, Iteration: 200, Loss: 0.14206045801984146\n",
      "Epoch: 65, Iteration: 300, Loss: 0.14491866540629417\n",
      "Epoch: 65, Iteration: 400, Loss: 0.1361699298140593\n",
      "Epoch: 65, Iteration: 500, Loss: 0.14143008523387834\n",
      "Epoch: 65, Iteration: 600, Loss: 0.14556797815021127\n",
      "Epoch: 65, Iteration: 700, Loss: 0.1420130513724871\n",
      "Epoch: 65, Train Loss: 0.005723397419205867, Test Loss: 0.001420135048974771\n",
      "Epoch: 66, Iteration: 100, Loss: 0.14471564121777192\n",
      "Epoch: 66, Iteration: 200, Loss: 0.14732348429970443\n",
      "Epoch: 66, Iteration: 300, Loss: 0.14228280703537166\n",
      "Epoch: 66, Iteration: 400, Loss: 0.14495098683983088\n",
      "Epoch: 66, Iteration: 500, Loss: 0.1425655407947488\n",
      "Epoch: 66, Iteration: 600, Loss: 0.13768750056624413\n",
      "Epoch: 66, Iteration: 700, Loss: 0.14061335352016613\n",
      "Epoch: 66, Train Loss: 0.005710316968907137, Test Loss: 0.0014144812172162346\n",
      "Epoch: 67, Iteration: 100, Loss: 0.14286314591299742\n",
      "Epoch: 67, Iteration: 200, Loss: 0.13932508166180924\n",
      "Epoch: 67, Iteration: 300, Loss: 0.14740154042374343\n",
      "Epoch: 67, Iteration: 400, Loss: 0.14838548214174807\n",
      "Epoch: 67, Iteration: 500, Loss: 0.13977326807798818\n",
      "Epoch: 67, Iteration: 600, Loss: 0.14401337405433878\n",
      "Epoch: 67, Iteration: 700, Loss: 0.1350189921213314\n",
      "Epoch: 67, Train Loss: 0.005691556204983499, Test Loss: 0.0014166938367998227\n",
      "Epoch: 68, Iteration: 100, Loss: 0.1453652911586687\n",
      "Epoch: 68, Iteration: 200, Loss: 0.14667814556742087\n",
      "Epoch: 68, Iteration: 300, Loss: 0.1430113529204391\n",
      "Epoch: 68, Iteration: 400, Loss: 0.14505286392522976\n",
      "Epoch: 68, Iteration: 500, Loss: 0.1440615127212368\n",
      "Epoch: 68, Iteration: 600, Loss: 0.14429401396773756\n",
      "Epoch: 68, Iteration: 700, Loss: 0.1343221805873327\n",
      "Epoch: 68, Train Loss: 0.00569970157375792, Test Loss: 0.001412662013899535\n",
      "Epoch: 69, Iteration: 100, Loss: 0.13985435653012246\n",
      "Epoch: 69, Iteration: 200, Loss: 0.14658429758856073\n",
      "Epoch: 69, Iteration: 300, Loss: 0.14249234384624287\n",
      "Epoch: 69, Iteration: 400, Loss: 0.13918844400905073\n",
      "Epoch: 69, Iteration: 500, Loss: 0.141769815585576\n",
      "Epoch: 69, Iteration: 600, Loss: 0.1437534672440961\n",
      "Epoch: 69, Iteration: 700, Loss: 0.1437876574927941\n",
      "Epoch: 69, Train Loss: 0.005685554921219591, Test Loss: 0.00139872277417453\n",
      "Epoch: 70, Iteration: 100, Loss: 0.1435713423998095\n",
      "Epoch: 70, Iteration: 200, Loss: 0.14184098248369992\n",
      "Epoch: 70, Iteration: 300, Loss: 0.13944668986368924\n",
      "Epoch: 70, Iteration: 400, Loss: 0.14389457745710388\n",
      "Epoch: 70, Iteration: 500, Loss: 0.14304865011945367\n",
      "Epoch: 70, Iteration: 600, Loss: 0.14136095176218078\n",
      "Epoch: 70, Iteration: 700, Loss: 0.14322941459249705\n",
      "Epoch: 70, Train Loss: 0.005685325975937303, Test Loss: 0.0014049170853104441\n",
      "Epoch: 71, Iteration: 100, Loss: 0.1428608358837664\n",
      "Epoch: 71, Iteration: 200, Loss: 0.13872639619512483\n",
      "Epoch: 71, Iteration: 300, Loss: 0.1420785338850692\n",
      "Epoch: 71, Iteration: 400, Loss: 0.1455606832751073\n",
      "Epoch: 71, Iteration: 500, Loss: 0.14232994709163904\n",
      "Epoch: 71, Iteration: 600, Loss: 0.1391044156625867\n",
      "Epoch: 71, Iteration: 700, Loss: 0.1457252528052777\n",
      "Epoch: 71, Train Loss: 0.0056720893803867516, Test Loss: 0.0014052501213154755\n",
      "Epoch: 72, Iteration: 100, Loss: 0.1404909283737652\n",
      "Epoch: 72, Iteration: 200, Loss: 0.13783289800630882\n",
      "Epoch: 72, Iteration: 300, Loss: 0.14157846703892574\n",
      "Epoch: 72, Iteration: 400, Loss: 0.14582327316747978\n",
      "Epoch: 72, Iteration: 500, Loss: 0.1433463673456572\n",
      "Epoch: 72, Iteration: 600, Loss: 0.1380136469961144\n",
      "Epoch: 72, Iteration: 700, Loss: 0.14254527015145868\n",
      "Epoch: 72, Train Loss: 0.005657798967440612, Test Loss: 0.0013985020635300315\n",
      "Epoch: 73, Iteration: 100, Loss: 0.1407470378326252\n",
      "Epoch: 73, Iteration: 200, Loss: 0.13888139731716365\n",
      "Epoch: 73, Iteration: 300, Loss: 0.137380997359287\n",
      "Epoch: 73, Iteration: 400, Loss: 0.13911075319629163\n",
      "Epoch: 73, Iteration: 500, Loss: 0.14568915573181584\n",
      "Epoch: 73, Iteration: 600, Loss: 0.1461382852285169\n",
      "Epoch: 73, Iteration: 700, Loss: 0.1421952156815678\n",
      "Epoch: 73, Train Loss: 0.005639284167264122, Test Loss: 0.0014110569821787066\n",
      "Epoch: 74, Iteration: 100, Loss: 0.1391560189658776\n",
      "Epoch: 74, Iteration: 200, Loss: 0.14498678472591564\n",
      "Epoch: 74, Iteration: 300, Loss: 0.14305676647927612\n",
      "Epoch: 74, Iteration: 400, Loss: 0.1389616984524764\n",
      "Epoch: 74, Iteration: 500, Loss: 0.14539926551515236\n",
      "Epoch: 74, Iteration: 600, Loss: 0.14243129291571677\n",
      "Epoch: 74, Iteration: 700, Loss: 0.13813543325522915\n",
      "Epoch: 74, Train Loss: 0.005645909136219416, Test Loss: 0.0014044756127987057\n",
      "Epoch: 75, Iteration: 100, Loss: 0.14422630629269406\n",
      "Epoch: 75, Iteration: 200, Loss: 0.14406020962633193\n",
      "Epoch: 75, Iteration: 300, Loss: 0.14382601965917274\n",
      "Epoch: 75, Iteration: 400, Loss: 0.14075310656335205\n",
      "Epoch: 75, Iteration: 500, Loss: 0.13843104609986767\n",
      "Epoch: 75, Iteration: 600, Loss: 0.14235400350298733\n",
      "Epoch: 75, Iteration: 700, Loss: 0.1388872725656256\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch: 75, Train Loss: 0.005640804210561328, Test Loss: 0.0013991922902641819\n",
      "Epoch: 76, Iteration: 100, Loss: 0.13682833343045786\n",
      "Epoch: 76, Iteration: 200, Loss: 0.1429802508209832\n",
      "Epoch: 76, Iteration: 300, Loss: 0.13958711142186075\n",
      "Epoch: 76, Iteration: 400, Loss: 0.13614646421046928\n",
      "Epoch: 76, Iteration: 500, Loss: 0.13947841961635277\n",
      "Epoch: 76, Iteration: 600, Loss: 0.14067134750075638\n",
      "Epoch: 76, Iteration: 700, Loss: 0.13773081672843546\n",
      "Epoch: 76, Train Loss: 0.005544959499966353, Test Loss: 0.0013761310267727822\n",
      "Epoch: 77, Iteration: 100, Loss: 0.13618706486886367\n",
      "Epoch: 77, Iteration: 200, Loss: 0.14101476519135758\n",
      "Epoch: 77, Iteration: 300, Loss: 0.1407332217786461\n",
      "Epoch: 77, Iteration: 400, Loss: 0.14071136189159006\n",
      "Epoch: 77, Iteration: 500, Loss: 0.1366135399439372\n",
      "Epoch: 77, Iteration: 600, Loss: 0.138225796923507\n",
      "Epoch: 77, Iteration: 700, Loss: 0.13692776841344312\n",
      "Epoch: 77, Train Loss: 0.005540575896739028, Test Loss: 0.0013769546369439922\n",
      "Epoch: 78, Iteration: 100, Loss: 0.13643779879203066\n",
      "Epoch: 78, Iteration: 200, Loss: 0.13877839443739504\n",
      "Epoch: 78, Iteration: 300, Loss: 0.13816881988896057\n",
      "Epoch: 78, Iteration: 400, Loss: 0.13844633399276063\n",
      "Epoch: 78, Iteration: 500, Loss: 0.13707914855331182\n",
      "Epoch: 78, Iteration: 600, Loss: 0.13701442972524092\n",
      "Epoch: 78, Iteration: 700, Loss: 0.1407411345280707\n",
      "Epoch: 78, Train Loss: 0.005539583016070537, Test Loss: 0.0013753453205572442\n",
      "Epoch: 79, Iteration: 100, Loss: 0.14243300555972382\n",
      "Epoch: 79, Iteration: 200, Loss: 0.13670707092387602\n",
      "Epoch: 79, Iteration: 300, Loss: 0.1361786127090454\n",
      "Epoch: 79, Iteration: 400, Loss: 0.1413096747128293\n",
      "Epoch: 79, Iteration: 500, Loss: 0.1376201271195896\n",
      "Epoch: 79, Iteration: 600, Loss: 0.14110872498713434\n",
      "Epoch: 79, Iteration: 700, Loss: 0.13499332556966692\n",
      "Epoch: 79, Train Loss: 0.005538168489583768, Test Loss: 0.001377020533545874\n",
      "Epoch: 80, Iteration: 100, Loss: 0.1390124176396057\n",
      "Epoch: 80, Iteration: 200, Loss: 0.13921566074714065\n",
      "Epoch: 80, Iteration: 300, Loss: 0.1408614392275922\n",
      "Epoch: 80, Iteration: 400, Loss: 0.1374314749846235\n",
      "Epoch: 80, Iteration: 500, Loss: 0.13903613825095817\n",
      "Epoch: 80, Iteration: 600, Loss: 0.13883698399877176\n",
      "Epoch: 80, Iteration: 700, Loss: 0.13782928872387856\n",
      "Epoch: 80, Train Loss: 0.005539239592035301, Test Loss: 0.0013736473047174513\n",
      "Epoch: 81, Iteration: 100, Loss: 0.1414938112720847\n",
      "Epoch: 81, Iteration: 200, Loss: 0.14483080041827634\n",
      "Epoch: 81, Iteration: 300, Loss: 0.1370718644466251\n",
      "Epoch: 81, Iteration: 400, Loss: 0.13924956240225583\n",
      "Epoch: 81, Iteration: 500, Loss: 0.13714676938252524\n",
      "Epoch: 81, Iteration: 600, Loss: 0.13720678491517901\n",
      "Epoch: 81, Iteration: 700, Loss: 0.1355320622678846\n",
      "Epoch: 81, Train Loss: 0.005535895805805922, Test Loss: 0.0013745464640669525\n",
      "Epoch: 82, Iteration: 100, Loss: 0.1441686077741906\n",
      "Epoch: 82, Iteration: 200, Loss: 0.13925590622238815\n",
      "Epoch: 82, Iteration: 300, Loss: 0.13836264447309077\n",
      "Epoch: 82, Iteration: 400, Loss: 0.1356800181674771\n",
      "Epoch: 82, Iteration: 500, Loss: 0.13702758244471624\n",
      "Epoch: 82, Iteration: 600, Loss: 0.14353340544039384\n",
      "Epoch: 82, Iteration: 700, Loss: 0.13847510982304811\n",
      "Epoch: 82, Train Loss: 0.005535852717584931, Test Loss: 0.0013765971775865182\n",
      "Epoch: 83, Iteration: 100, Loss: 0.14117516559781507\n",
      "Epoch: 83, Iteration: 200, Loss: 0.1416099388152361\n",
      "Epoch: 83, Iteration: 300, Loss: 0.13960818102350459\n",
      "Epoch: 83, Iteration: 400, Loss: 0.13987343321787193\n",
      "Epoch: 83, Iteration: 500, Loss: 0.1370670954347588\n",
      "Epoch: 83, Iteration: 600, Loss: 0.13346196815837175\n",
      "Epoch: 83, Iteration: 700, Loss: 0.1416329356143251\n",
      "Epoch: 83, Train Loss: 0.0055352315586060285, Test Loss: 0.0013779167164466343\n",
      "Epoch: 84, Iteration: 100, Loss: 0.14060155069455504\n",
      "Epoch: 84, Iteration: 200, Loss: 0.13792392751201987\n",
      "Epoch: 84, Iteration: 300, Loss: 0.14166680804919451\n",
      "Epoch: 84, Iteration: 400, Loss: 0.14049411413725466\n",
      "Epoch: 84, Iteration: 500, Loss: 0.14029036613646895\n",
      "Epoch: 84, Iteration: 600, Loss: 0.13408913451712579\n",
      "Epoch: 84, Iteration: 700, Loss: 0.14009633573004976\n",
      "Epoch: 84, Train Loss: 0.005531731492956169, Test Loss: 0.0013754673869698308\n",
      "Epoch: 85, Iteration: 100, Loss: 0.13911016244674101\n",
      "Epoch: 85, Iteration: 200, Loss: 0.13936442625708878\n",
      "Epoch: 85, Iteration: 300, Loss: 0.1412934361724183\n",
      "Epoch: 85, Iteration: 400, Loss: 0.13910704816225916\n",
      "Epoch: 85, Iteration: 500, Loss: 0.1388067738735117\n",
      "Epoch: 85, Iteration: 600, Loss: 0.13917421968653798\n",
      "Epoch: 85, Iteration: 700, Loss: 0.13602715730667114\n",
      "Epoch: 85, Train Loss: 0.005531971740128938, Test Loss: 0.0013730462308740243\n",
      "Epoch: 86, Iteration: 100, Loss: 0.13612623064545915\n",
      "Epoch: 86, Iteration: 200, Loss: 0.13738853577524424\n",
      "Epoch: 86, Iteration: 300, Loss: 0.13656813517445698\n",
      "Epoch: 86, Iteration: 400, Loss: 0.1387122153537348\n",
      "Epoch: 86, Iteration: 500, Loss: 0.14107831998262554\n",
      "Epoch: 86, Iteration: 600, Loss: 0.13989003840833902\n",
      "Epoch: 86, Iteration: 700, Loss: 0.13681566331069916\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch: 86, Train Loss: 0.005530136211309582, Test Loss: 0.0013757766041089781\n",
      "Epoch: 87, Iteration: 100, Loss: 0.13831537473015487\n",
      "Epoch: 87, Iteration: 200, Loss: 0.13784501026384532\n",
      "Epoch: 87, Iteration: 300, Loss: 0.13658496999414638\n",
      "Epoch: 87, Iteration: 400, Loss: 0.13815675280056894\n",
      "Epoch: 87, Iteration: 500, Loss: 0.14068860316183418\n",
      "Epoch: 87, Iteration: 600, Loss: 0.14161677903030068\n",
      "Epoch: 87, Iteration: 700, Loss: 0.13553570944350213\n",
      "Epoch: 87, Train Loss: 0.00552077421132708, Test Loss: 0.0013729043293278665\n",
      "Epoch: 88, Iteration: 100, Loss: 0.14069718262180686\n",
      "Epoch: 88, Iteration: 200, Loss: 0.13879839581204578\n",
      "Epoch: 88, Iteration: 300, Loss: 0.13907144009135664\n",
      "Epoch: 88, Iteration: 400, Loss: 0.1374741478357464\n",
      "Epoch: 88, Iteration: 500, Loss: 0.14023298298707232\n",
      "Epoch: 88, Iteration: 600, Loss: 0.14127175050089136\n",
      "Epoch: 88, Iteration: 700, Loss: 0.1371205974719487\n",
      "Epoch: 88, Train Loss: 0.0055209559306968, Test Loss: 0.0013723477913299576\n",
      "Epoch: 89, Iteration: 100, Loss: 0.1401770194643177\n",
      "Epoch: 89, Iteration: 200, Loss: 0.13948667090153322\n",
      "Epoch: 89, Iteration: 300, Loss: 0.13011763023678213\n",
      "Epoch: 89, Iteration: 400, Loss: 0.13960460957605392\n",
      "Epoch: 89, Iteration: 500, Loss: 0.13662205840228125\n",
      "Epoch: 89, Iteration: 600, Loss: 0.14060413045808673\n",
      "Epoch: 89, Iteration: 700, Loss: 0.14270464255241677\n",
      "Epoch: 89, Train Loss: 0.005521180216455832, Test Loss: 0.0013769139451324007\n",
      "Epoch: 90, Iteration: 100, Loss: 0.14083802897948772\n",
      "Epoch: 90, Iteration: 200, Loss: 0.13963661307934672\n",
      "Epoch: 90, Iteration: 300, Loss: 0.13863277150085196\n",
      "Epoch: 90, Iteration: 400, Loss: 0.13855563721153885\n",
      "Epoch: 90, Iteration: 500, Loss: 0.13433942320989445\n",
      "Epoch: 90, Iteration: 600, Loss: 0.13461458653910086\n",
      "Epoch: 90, Iteration: 700, Loss: 0.13897976779844612\n",
      "Epoch: 90, Train Loss: 0.005520822230901104, Test Loss: 0.0013712643965845927\n",
      "Epoch: 91, Iteration: 100, Loss: 0.1387590278754942\n",
      "Epoch: 91, Iteration: 200, Loss: 0.13707983103813604\n",
      "Epoch: 91, Iteration: 300, Loss: 0.13745635648956522\n",
      "Epoch: 91, Iteration: 400, Loss: 0.13909509248333052\n",
      "Epoch: 91, Iteration: 500, Loss: 0.1361094958265312\n",
      "Epoch: 91, Iteration: 600, Loss: 0.1388516102451831\n",
      "Epoch: 91, Iteration: 700, Loss: 0.13940248353173956\n",
      "Epoch: 91, Train Loss: 0.00552010351268109, Test Loss: 0.0013719906195183284\n",
      "Epoch: 92, Iteration: 100, Loss: 0.13869978324510157\n",
      "Epoch: 92, Iteration: 200, Loss: 0.13525751774432138\n",
      "Epoch: 92, Iteration: 300, Loss: 0.13880510459421203\n",
      "Epoch: 92, Iteration: 400, Loss: 0.1380492303869687\n",
      "Epoch: 92, Iteration: 500, Loss: 0.14068890875205398\n",
      "Epoch: 92, Iteration: 600, Loss: 0.1391639350913465\n",
      "Epoch: 92, Iteration: 700, Loss: 0.1403692231979221\n",
      "Epoch: 92, Train Loss: 0.005520646686491091, Test Loss: 0.0013715815648902207\n",
      "Epoch: 93, Iteration: 100, Loss: 0.1358742129523307\n",
      "Epoch: 93, Iteration: 200, Loss: 0.138462322531268\n",
      "Epoch: 93, Iteration: 300, Loss: 0.1396295300219208\n",
      "Epoch: 93, Iteration: 400, Loss: 0.134978681569919\n",
      "Epoch: 93, Iteration: 500, Loss: 0.1446277814102359\n",
      "Epoch: 93, Iteration: 600, Loss: 0.13754477200563997\n",
      "Epoch: 93, Iteration: 700, Loss: 0.13790406368207186\n",
      "Epoch: 93, Train Loss: 0.005520129433134571, Test Loss: 0.0013717936672037467\n",
      "Epoch: 94, Iteration: 100, Loss: 0.1387060065753758\n",
      "Epoch: 94, Iteration: 200, Loss: 0.13819441659143195\n",
      "Epoch: 94, Iteration: 300, Loss: 0.14106441021431237\n",
      "Epoch: 94, Iteration: 400, Loss: 0.14242287550587207\n",
      "Epoch: 94, Iteration: 500, Loss: 0.13442725758068264\n",
      "Epoch: 94, Iteration: 600, Loss: 0.13834101537941024\n",
      "Epoch: 94, Iteration: 700, Loss: 0.13491633703233674\n",
      "Epoch: 94, Train Loss: 0.005520241195044946, Test Loss: 0.0013720838411245494\n",
      "Epoch: 95, Iteration: 100, Loss: 0.13944675057427958\n",
      "Epoch: 95, Iteration: 200, Loss: 0.13997034344356507\n",
      "Epoch: 95, Iteration: 300, Loss: 0.13442822929937392\n",
      "Epoch: 95, Iteration: 400, Loss: 0.13723950780695304\n",
      "Epoch: 95, Iteration: 500, Loss: 0.13799470925005153\n",
      "Epoch: 95, Iteration: 600, Loss: 0.14239407423883677\n",
      "Epoch: 95, Iteration: 700, Loss: 0.13837545574642718\n",
      "Epoch: 95, Train Loss: 0.005520249464025255, Test Loss: 0.00137132823059801\n",
      "Epoch: 96, Iteration: 100, Loss: 0.13753102102782577\n",
      "Epoch: 96, Iteration: 200, Loss: 0.14086542971199378\n",
      "Epoch: 96, Iteration: 300, Loss: 0.13896867440780625\n",
      "Epoch: 96, Iteration: 400, Loss: 0.13642460090341046\n",
      "Epoch: 96, Iteration: 500, Loss: 0.14069024252239615\n",
      "Epoch: 96, Iteration: 600, Loss: 0.13935170928016305\n",
      "Epoch: 96, Iteration: 700, Loss: 0.1363765908172354\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch: 96, Train Loss: 0.005521091317932587, Test Loss: 0.0013729059664183297\n",
      "Epoch: 97, Iteration: 100, Loss: 0.13689585926476866\n",
      "Epoch: 97, Iteration: 200, Loss: 0.14439887506887317\n",
      "Epoch: 97, Iteration: 300, Loss: 0.13496237446088344\n",
      "Epoch: 97, Iteration: 400, Loss: 0.13851029117358848\n",
      "Epoch: 97, Iteration: 500, Loss: 0.13915904745226726\n",
      "Epoch: 97, Iteration: 600, Loss: 0.13755363057134673\n",
      "Epoch: 97, Iteration: 700, Loss: 0.13549137528752908\n",
      "Epoch: 97, Train Loss: 0.005518911436374765, Test Loss: 0.001372287209960632\n",
      "Epoch: 98, Iteration: 100, Loss: 0.13536822580499575\n",
      "Epoch: 98, Iteration: 200, Loss: 0.1355238316464238\n",
      "Epoch: 98, Iteration: 300, Loss: 0.13914573832880706\n",
      "Epoch: 98, Iteration: 400, Loss: 0.13751221197890118\n",
      "Epoch: 98, Iteration: 500, Loss: 0.14441275206627324\n",
      "Epoch: 98, Iteration: 600, Loss: 0.1424618368037045\n",
      "Epoch: 98, Iteration: 700, Loss: 0.13519062992418185\n",
      "Epoch: 98, Train Loss: 0.00551953205751488, Test Loss: 0.0013726065043010748\n",
      "Epoch: 99, Iteration: 100, Loss: 0.13582253665663302\n",
      "Epoch: 99, Iteration: 200, Loss: 0.13543156656669453\n",
      "Epoch: 99, Iteration: 300, Loss: 0.14183999970555305\n",
      "Epoch: 99, Iteration: 400, Loss: 0.13873730081832036\n",
      "Epoch: 99, Iteration: 500, Loss: 0.14141817629570141\n",
      "Epoch: 99, Iteration: 600, Loss: 0.13778908224776387\n",
      "Epoch: 99, Iteration: 700, Loss: 0.13575103710172698\n",
      "Epoch: 99, Train Loss: 0.005518671401659958, Test Loss: 0.001371167587058153\n",
      "Epoch: 100, Iteration: 100, Loss: 0.13791093253530562\n",
      "Epoch: 100, Iteration: 200, Loss: 0.13788424158701673\n",
      "Epoch: 100, Iteration: 300, Loss: 0.13496327976463363\n",
      "Epoch: 100, Iteration: 400, Loss: 0.14745778578799218\n",
      "Epoch: 100, Iteration: 500, Loss: 0.13827486470108852\n",
      "Epoch: 100, Iteration: 600, Loss: 0.13710021798033267\n",
      "Epoch: 100, Iteration: 700, Loss: 0.13565730082336813\n",
      "Epoch: 100, Train Loss: 0.005518800287100021, Test Loss: 0.0013716819285764358\n",
      "Epoch: 101, Iteration: 100, Loss: 0.1440512013505213\n",
      "Epoch: 101, Iteration: 200, Loss: 0.13269225391559303\n",
      "Epoch: 101, Iteration: 300, Loss: 0.137839536590036\n",
      "Epoch: 101, Iteration: 400, Loss: 0.14164026954676956\n",
      "Epoch: 101, Iteration: 500, Loss: 0.13624994968995452\n",
      "Epoch: 101, Iteration: 600, Loss: 0.13590418349485844\n",
      "Epoch: 101, Iteration: 700, Loss: 0.13986348814796656\n",
      "Epoch: 101, Train Loss: 0.005518445633060765, Test Loss: 0.0013717671023914591\n",
      "Epoch: 102, Iteration: 100, Loss: 0.13559384463587776\n",
      "Epoch: 102, Iteration: 200, Loss: 0.13889454526361078\n",
      "Epoch: 102, Iteration: 300, Loss: 0.1391479871235788\n",
      "Epoch: 102, Iteration: 400, Loss: 0.13646661641541868\n",
      "Epoch: 102, Iteration: 500, Loss: 0.1381098868441768\n",
      "Epoch: 102, Iteration: 600, Loss: 0.13950644934084266\n",
      "Epoch: 102, Iteration: 700, Loss: 0.14077892620116472\n",
      "Epoch   103: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch: 102, Train Loss: 0.005519123202539049, Test Loss: 0.00137460424361052\n",
      "Epoch: 103, Iteration: 100, Loss: 0.14219229068839923\n",
      "Epoch: 103, Iteration: 200, Loss: 0.1340184357832186\n",
      "Epoch: 103, Iteration: 300, Loss: 0.1406007442274131\n",
      "Epoch: 103, Iteration: 400, Loss: 0.13305217964807525\n",
      "Epoch: 103, Iteration: 500, Loss: 0.13999985822010785\n",
      "Epoch: 103, Iteration: 600, Loss: 0.1349985576234758\n",
      "Epoch: 103, Iteration: 700, Loss: 0.13872221013298258\n",
      "Epoch: 103, Train Loss: 0.005517649193352554, Test Loss: 0.0013746565268957056\n",
      "Epoch: 104, Iteration: 100, Loss: 0.1396525667514652\n",
      "Epoch: 104, Iteration: 200, Loss: 0.13832024193834513\n",
      "Epoch: 104, Iteration: 300, Loss: 0.1402380121871829\n",
      "Epoch: 104, Iteration: 400, Loss: 0.13800625380827114\n",
      "Epoch: 104, Iteration: 500, Loss: 0.13730581913841888\n",
      "Epoch: 104, Iteration: 600, Loss: 0.13773131265770644\n",
      "Epoch: 104, Iteration: 700, Loss: 0.13476270361570641\n",
      "Epoch: 104, Train Loss: 0.005518166128313169, Test Loss: 0.001372676571190823\n",
      "Epoch: 105, Iteration: 100, Loss: 0.13880864612292498\n",
      "Epoch: 105, Iteration: 200, Loss: 0.14196378336055204\n",
      "Epoch: 105, Iteration: 300, Loss: 0.13968606921844184\n",
      "Epoch: 105, Iteration: 400, Loss: 0.13448262424208224\n",
      "Epoch: 105, Iteration: 500, Loss: 0.13885157322511077\n",
      "Epoch: 105, Iteration: 600, Loss: 0.1402878275839612\n",
      "Epoch: 105, Iteration: 700, Loss: 0.13582759985001758\n",
      "Epoch: 105, Train Loss: 0.005519676001858898, Test Loss: 0.0013721468203584664\n",
      "Epoch: 106, Iteration: 100, Loss: 0.13817574916174635\n",
      "Epoch: 106, Iteration: 200, Loss: 0.13813618168933317\n",
      "Epoch: 106, Iteration: 300, Loss: 0.1380404755473137\n",
      "Epoch: 106, Iteration: 400, Loss: 0.1430694664013572\n",
      "Epoch: 106, Iteration: 500, Loss: 0.13825280411401764\n",
      "Epoch: 106, Iteration: 600, Loss: 0.1374358904431574\n",
      "Epoch: 106, Iteration: 700, Loss: 0.13566733815241605\n",
      "Epoch: 106, Train Loss: 0.005518374089733697, Test Loss: 0.0013715479816892185\n",
      "Epoch: 107, Iteration: 100, Loss: 0.14645872201072052\n",
      "Epoch: 107, Iteration: 200, Loss: 0.13916609808802605\n",
      "Epoch: 107, Iteration: 300, Loss: 0.13932242657756433\n",
      "Epoch: 107, Iteration: 400, Loss: 0.1373823278117925\n",
      "Epoch: 107, Iteration: 500, Loss: 0.13431869761552662\n",
      "Epoch: 107, Iteration: 600, Loss: 0.13784096256131306\n",
      "Epoch: 107, Iteration: 700, Loss: 0.1372168103698641\n",
      "Epoch: 107, Train Loss: 0.005518384425959084, Test Loss: 0.0013729609959409572\n",
      "Epoch: 108, Iteration: 100, Loss: 0.13948700088076293\n",
      "Epoch: 108, Iteration: 200, Loss: 0.1410786259220913\n",
      "Epoch: 108, Iteration: 300, Loss: 0.13871636101976037\n",
      "Epoch: 108, Iteration: 400, Loss: 0.13788880431093276\n",
      "Epoch: 108, Iteration: 500, Loss: 0.1370202981051989\n",
      "Epoch: 108, Iteration: 600, Loss: 0.136513001692947\n",
      "Epoch: 108, Iteration: 700, Loss: 0.1364009766257368\n",
      "Epoch: 108, Train Loss: 0.0055182765712379475, Test Loss: 0.0013713859554263764\n",
      "Epoch: 109, Iteration: 100, Loss: 0.1420857619959861\n",
      "Epoch: 109, Iteration: 200, Loss: 0.1371710835956037\n",
      "Epoch: 109, Iteration: 300, Loss: 0.13661199191119522\n",
      "Epoch: 109, Iteration: 400, Loss: 0.13657533639343455\n",
      "Epoch: 109, Iteration: 500, Loss: 0.1362462155520916\n",
      "Epoch: 109, Iteration: 600, Loss: 0.1396627189242281\n",
      "Epoch: 109, Iteration: 700, Loss: 0.13802740455139428\n",
      "Epoch: 109, Train Loss: 0.005517957392148673, Test Loss: 0.0013700659706955775\n",
      "Epoch: 110, Iteration: 100, Loss: 0.13554675207706168\n",
      "Epoch: 110, Iteration: 200, Loss: 0.13817491102963686\n",
      "Epoch: 110, Iteration: 300, Loss: 0.1392662674188614\n",
      "Epoch: 110, Iteration: 400, Loss: 0.1385492942063138\n",
      "Epoch: 110, Iteration: 500, Loss: 0.1386097670183517\n",
      "Epoch: 110, Iteration: 600, Loss: 0.1419143220409751\n",
      "Epoch: 110, Iteration: 700, Loss: 0.13720998383359984\n",
      "Epoch: 110, Train Loss: 0.005517458813264966, Test Loss: 0.001371747530065477\n",
      "Epoch: 111, Iteration: 100, Loss: 0.13487897184677422\n",
      "Epoch: 111, Iteration: 200, Loss: 0.14206227712566033\n",
      "Epoch: 111, Iteration: 300, Loss: 0.1424475078820251\n",
      "Epoch: 111, Iteration: 400, Loss: 0.1358321313164197\n",
      "Epoch: 111, Iteration: 500, Loss: 0.1409715905901976\n",
      "Epoch: 111, Iteration: 600, Loss: 0.13486889022169635\n",
      "Epoch: 111, Iteration: 700, Loss: 0.1395108439028263\n",
      "Epoch: 111, Train Loss: 0.00551806335861329, Test Loss: 0.001374322980409488\n",
      "Epoch: 112, Iteration: 100, Loss: 0.13805464969482273\n",
      "Epoch: 112, Iteration: 200, Loss: 0.13883029180578887\n",
      "Epoch: 112, Iteration: 300, Loss: 0.1364756606053561\n",
      "Epoch: 112, Iteration: 400, Loss: 0.13986910798121244\n",
      "Epoch: 112, Iteration: 500, Loss: 0.14193943195277825\n",
      "Epoch: 112, Iteration: 600, Loss: 0.1374015937326476\n",
      "Epoch: 112, Iteration: 700, Loss: 0.1384975165128708\n",
      "Epoch: 112, Train Loss: 0.005519062363600824, Test Loss: 0.001371369250700809\n",
      "Epoch: 113, Iteration: 100, Loss: 0.13608573999954388\n",
      "Epoch: 113, Iteration: 200, Loss: 0.14359860768308863\n",
      "Epoch: 113, Iteration: 300, Loss: 0.1340267486521043\n",
      "Epoch: 113, Iteration: 400, Loss: 0.13903075410053134\n",
      "Epoch: 113, Iteration: 500, Loss: 0.14208987477468327\n",
      "Epoch: 113, Iteration: 600, Loss: 0.13379174377769232\n",
      "Epoch: 113, Iteration: 700, Loss: 0.14067102078115568\n",
      "Epoch: 113, Train Loss: 0.005518601556832437, Test Loss: 0.00137271115090698\n",
      "Epoch: 114, Iteration: 100, Loss: 0.14036439271876588\n",
      "Epoch: 114, Iteration: 200, Loss: 0.1403792635537684\n",
      "Epoch: 114, Iteration: 300, Loss: 0.13426678447285667\n",
      "Epoch: 114, Iteration: 400, Loss: 0.13387236557900906\n",
      "Epoch: 114, Iteration: 500, Loss: 0.13694798777578399\n",
      "Epoch: 114, Iteration: 600, Loss: 0.1404121233499609\n",
      "Epoch: 114, Iteration: 700, Loss: 0.14154952979879454\n",
      "Epoch: 114, Train Loss: 0.005517410565225873, Test Loss: 0.0013724153547082097\n",
      "Epoch: 115, Iteration: 100, Loss: 0.13328006950905547\n",
      "Epoch: 115, Iteration: 200, Loss: 0.13873408100334927\n",
      "Epoch: 115, Iteration: 300, Loss: 0.13960504141869023\n",
      "Epoch: 115, Iteration: 400, Loss: 0.13819319353206083\n",
      "Epoch: 115, Iteration: 500, Loss: 0.14217992487829179\n",
      "Epoch: 115, Iteration: 600, Loss: 0.13521226780721918\n",
      "Epoch: 115, Iteration: 700, Loss: 0.13784651225432754\n",
      "Epoch: 115, Train Loss: 0.005518497439625208, Test Loss: 0.0013710530835669488\n",
      "Epoch: 116, Iteration: 100, Loss: 0.14146915276069194\n",
      "Epoch: 116, Iteration: 200, Loss: 0.13343367481138557\n",
      "Epoch: 116, Iteration: 300, Loss: 0.13848624343518168\n",
      "Epoch: 116, Iteration: 400, Loss: 0.13980411848751828\n",
      "Epoch: 116, Iteration: 500, Loss: 0.1369298783247359\n",
      "Epoch: 116, Iteration: 600, Loss: 0.13740138185676187\n",
      "Epoch: 116, Iteration: 700, Loss: 0.14018024899996817\n",
      "Epoch: 116, Train Loss: 0.005518142246874049, Test Loss: 0.0013719362689880654\n",
      "Epoch: 117, Iteration: 100, Loss: 0.14046014612540603\n",
      "Epoch: 117, Iteration: 200, Loss: 0.13659440947230905\n",
      "Epoch: 117, Iteration: 300, Loss: 0.14332177577307448\n",
      "Epoch: 117, Iteration: 400, Loss: 0.13788971939356998\n",
      "Epoch: 117, Iteration: 500, Loss: 0.13288536691106856\n",
      "Epoch: 117, Iteration: 600, Loss: 0.13750896480632946\n",
      "Epoch: 117, Iteration: 700, Loss: 0.13873659225646406\n",
      "Epoch: 117, Train Loss: 0.0055179614797816615, Test Loss: 0.0013702155541977846\n",
      "Epoch: 118, Iteration: 100, Loss: 0.13783218775643036\n",
      "Epoch: 118, Iteration: 200, Loss: 0.13775050482945517\n",
      "Epoch: 118, Iteration: 300, Loss: 0.14190952776698396\n",
      "Epoch: 118, Iteration: 400, Loss: 0.13716297718929127\n",
      "Epoch: 118, Iteration: 500, Loss: 0.13541798962978646\n",
      "Epoch: 118, Iteration: 600, Loss: 0.14071869244799018\n",
      "Epoch: 118, Iteration: 700, Loss: 0.1382676876964979\n",
      "Epoch: 118, Train Loss: 0.0055174374565831385, Test Loss: 0.0013715125611633995\n",
      "Epoch: 119, Iteration: 100, Loss: 0.13342327438294888\n",
      "Epoch: 119, Iteration: 200, Loss: 0.13997521687997505\n",
      "Epoch: 119, Iteration: 300, Loss: 0.14376522845122963\n",
      "Epoch: 119, Iteration: 400, Loss: 0.145121127308812\n",
      "Epoch: 119, Iteration: 500, Loss: 0.13775984611129388\n",
      "Epoch: 119, Iteration: 600, Loss: 0.13594569900305942\n",
      "Epoch: 119, Iteration: 700, Loss: 0.13637910888064653\n",
      "Epoch: 119, Train Loss: 0.005518715435464401, Test Loss: 0.0013704659583163449\n",
      "Epoch: 120, Iteration: 100, Loss: 0.13567051372956485\n",
      "Epoch: 120, Iteration: 200, Loss: 0.13866554584819824\n",
      "Epoch: 120, Iteration: 300, Loss: 0.1403242127271369\n",
      "Epoch: 120, Iteration: 400, Loss: 0.13986800995189697\n",
      "Epoch: 120, Iteration: 500, Loss: 0.13607119192602113\n",
      "Epoch: 120, Iteration: 600, Loss: 0.1339396986295469\n",
      "Epoch: 120, Iteration: 700, Loss: 0.14022645447403193\n",
      "Epoch: 120, Train Loss: 0.00551891684968723, Test Loss: 0.0013725179046741687\n",
      "Epoch: 121, Iteration: 100, Loss: 0.14080508914776146\n",
      "Epoch: 121, Iteration: 200, Loss: 0.1392635924858041\n",
      "Epoch: 121, Iteration: 300, Loss: 0.13570620835525915\n",
      "Epoch: 121, Iteration: 400, Loss: 0.13566434005042538\n",
      "Epoch: 121, Iteration: 500, Loss: 0.14062645059311762\n",
      "Epoch: 121, Iteration: 600, Loss: 0.13895631610648707\n",
      "Epoch: 121, Iteration: 700, Loss: 0.14049104071455076\n",
      "Epoch: 121, Train Loss: 0.005518014326225966, Test Loss: 0.001372228371037636\n",
      "Epoch: 122, Iteration: 100, Loss: 0.13778093457221985\n",
      "Epoch: 122, Iteration: 200, Loss: 0.1430626759538427\n",
      "Epoch: 122, Iteration: 300, Loss: 0.13368718465790153\n",
      "Epoch: 122, Iteration: 400, Loss: 0.13607648079050705\n",
      "Epoch: 122, Iteration: 500, Loss: 0.14091010595439002\n",
      "Epoch: 122, Iteration: 600, Loss: 0.1343632767093368\n",
      "Epoch: 122, Iteration: 700, Loss: 0.14308879454620183\n",
      "Epoch: 122, Train Loss: 0.005518232070608064, Test Loss: 0.001371672608947847\n",
      "Epoch: 123, Iteration: 100, Loss: 0.1366724607651122\n",
      "Epoch: 123, Iteration: 200, Loss: 0.13769551017321646\n",
      "Epoch: 123, Iteration: 300, Loss: 0.13797466654796153\n",
      "Epoch: 123, Iteration: 400, Loss: 0.13829614163842052\n",
      "Epoch: 123, Iteration: 500, Loss: 0.13747347344178706\n",
      "Epoch: 123, Iteration: 600, Loss: 0.13840189849724993\n",
      "Epoch: 123, Iteration: 700, Loss: 0.13852749910438433\n",
      "Epoch: 123, Train Loss: 0.005518983225629199, Test Loss: 0.0013737604481866583\n",
      "Epoch: 124, Iteration: 100, Loss: 0.13758811115985736\n",
      "Epoch: 124, Iteration: 200, Loss: 0.13413769420003518\n",
      "Epoch: 124, Iteration: 300, Loss: 0.1378794563934207\n",
      "Epoch: 124, Iteration: 400, Loss: 0.14045703096780926\n",
      "Epoch: 124, Iteration: 500, Loss: 0.13659169187303632\n",
      "Epoch: 124, Iteration: 600, Loss: 0.13908459583763033\n",
      "Epoch: 124, Iteration: 700, Loss: 0.13980509602697566\n",
      "Epoch: 124, Train Loss: 0.005518360739806667, Test Loss: 0.0013722800364485011\n",
      "Epoch: 125, Iteration: 100, Loss: 0.14008217444643378\n",
      "Epoch: 125, Iteration: 200, Loss: 0.1355442369240336\n",
      "Epoch: 125, Iteration: 300, Loss: 0.13976546382764354\n",
      "Epoch: 125, Iteration: 400, Loss: 0.13869465317111462\n",
      "Epoch: 125, Iteration: 500, Loss: 0.1369998076115735\n",
      "Epoch: 125, Iteration: 600, Loss: 0.13709142606239766\n",
      "Epoch: 125, Iteration: 700, Loss: 0.13811774167697877\n",
      "Epoch: 125, Train Loss: 0.005518535465525929, Test Loss: 0.0013718951426562854\n",
      "Epoch: 126, Iteration: 100, Loss: 0.13709069939795882\n",
      "Epoch: 126, Iteration: 200, Loss: 0.14357869612285867\n",
      "Epoch: 126, Iteration: 300, Loss: 0.1386604497092776\n",
      "Epoch: 126, Iteration: 400, Loss: 0.13412315904861316\n",
      "Epoch: 126, Iteration: 500, Loss: 0.14008347387425601\n",
      "Epoch: 126, Iteration: 600, Loss: 0.13662525167455897\n",
      "Epoch: 126, Iteration: 700, Loss: 0.13655751140322536\n",
      "Epoch: 126, Train Loss: 0.005518322559364606, Test Loss: 0.0013728661692584865\n",
      "Epoch: 127, Iteration: 100, Loss: 0.14182594208978117\n",
      "Epoch: 127, Iteration: 200, Loss: 0.13535845297155902\n",
      "Epoch: 127, Iteration: 300, Loss: 0.14035381033318117\n",
      "Epoch: 127, Iteration: 400, Loss: 0.14116516325157136\n",
      "Epoch: 127, Iteration: 500, Loss: 0.13755165511975065\n",
      "Epoch: 127, Iteration: 600, Loss: 0.1339573665172793\n",
      "Epoch: 127, Iteration: 700, Loss: 0.13762551551917568\n",
      "Epoch: 127, Train Loss: 0.005518164269451517, Test Loss: 0.0013727194702369161\n",
      "Epoch: 128, Iteration: 100, Loss: 0.1386536477948539\n",
      "Epoch: 128, Iteration: 200, Loss: 0.1318700077245012\n",
      "Epoch: 128, Iteration: 300, Loss: 0.13853275508154184\n",
      "Epoch: 128, Iteration: 400, Loss: 0.14208015718031675\n",
      "Epoch: 128, Iteration: 500, Loss: 0.13495639513712376\n",
      "Epoch: 128, Iteration: 600, Loss: 0.14054807781940326\n",
      "Epoch: 128, Iteration: 700, Loss: 0.14227168064098805\n",
      "Epoch: 128, Train Loss: 0.0055183138907887045, Test Loss: 0.001371402938675601\n",
      "Epoch: 129, Iteration: 100, Loss: 0.13533783296588808\n",
      "Epoch: 129, Iteration: 200, Loss: 0.13968067237874493\n",
      "Epoch: 129, Iteration: 300, Loss: 0.13675822224467993\n",
      "Epoch: 129, Iteration: 400, Loss: 0.1351734449272044\n",
      "Epoch: 129, Iteration: 500, Loss: 0.1427659197943285\n",
      "Epoch: 129, Iteration: 600, Loss: 0.13955379300750792\n",
      "Epoch: 129, Iteration: 700, Loss: 0.13838310353457928\n",
      "Epoch: 129, Train Loss: 0.0055183396130451005, Test Loss: 0.0013716221670620143\n",
      "Epoch: 130, Iteration: 100, Loss: 0.13649197376798838\n",
      "Epoch: 130, Iteration: 200, Loss: 0.13494606356834993\n",
      "Epoch: 130, Iteration: 300, Loss: 0.1397854138049297\n",
      "Epoch: 130, Iteration: 400, Loss: 0.14266611717175692\n",
      "Epoch: 130, Iteration: 500, Loss: 0.1375894954544492\n",
      "Epoch: 130, Iteration: 600, Loss: 0.1371336971060373\n",
      "Epoch: 130, Iteration: 700, Loss: 0.1408333809231408\n",
      "Epoch: 130, Train Loss: 0.0055183463465073145, Test Loss: 0.00137150896305684\n",
      "Epoch: 131, Iteration: 100, Loss: 0.1387421761173755\n",
      "Epoch: 131, Iteration: 200, Loss: 0.13852025021333247\n",
      "Epoch: 131, Iteration: 300, Loss: 0.13545647863065824\n",
      "Epoch: 131, Iteration: 400, Loss: 0.1407938806223683\n",
      "Epoch: 131, Iteration: 500, Loss: 0.1392406205413863\n",
      "Epoch: 131, Iteration: 600, Loss: 0.13891774573130533\n",
      "Epoch: 131, Iteration: 700, Loss: 0.1379812986124307\n",
      "Epoch: 131, Train Loss: 0.005517781805247068, Test Loss: 0.001371424340759404\n",
      "Epoch: 132, Iteration: 100, Loss: 0.1345801511197351\n",
      "Epoch: 132, Iteration: 200, Loss: 0.1431191058945842\n",
      "Epoch: 132, Iteration: 300, Loss: 0.13885716866934672\n",
      "Epoch: 132, Iteration: 400, Loss: 0.1362352505675517\n",
      "Epoch: 132, Iteration: 500, Loss: 0.13045553117990494\n",
      "Epoch: 132, Iteration: 600, Loss: 0.14422072609886527\n",
      "Epoch: 132, Iteration: 700, Loss: 0.13967969617806375\n"
     ]
    }
   ],
   "source": [
    "train_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "test_losses = {'loss_ae1': [], 'loss_ae2': [], 'loss_dyn': [], 'loss_total': []}\n",
    "\n",
    "patience = config[\"patience\"]\n",
    "\n",
    "for epoch in tqdm(range(config[\"epochs\"])):\n",
    "    loss_ae1_train = 0\n",
    "    loss_ae2_train = 0\n",
    "    loss_dyn_train = 0\n",
    "\n",
    "    current_train_loss = 0\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    warmup = 1 if epoch <= config[\"warmup\"] else 0\n",
    "\n",
    "    encoder.train()\n",
    "    dynamics.train()\n",
    "    decoder.train()\n",
    "\n",
    "    counter = 0\n",
    "    for i, (x_t, x_tau) in enumerate(train_loader,0):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        z_t = encoder(x_t)\n",
    "        x_t_pred = decoder(z_t)\n",
    "\n",
    "        z_tau_pred = dynamics(z_t)\n",
    "        z_tau = encoder(x_tau)\n",
    "        x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "        # Compute losses\n",
    "        loss_ae1 = criterion(x_t, x_t_pred)\n",
    "        loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "        loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "        loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "        # Backward pass\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        current_train_loss += loss_total.item()\n",
    "        epoch_train_loss += loss_total.item()\n",
    "\n",
    "        loss_ae1_train += loss_ae1.item()\n",
    "        loss_ae2_train += loss_ae2.item()\n",
    "        loss_dyn_train += loss_dyn.item() * warmup\n",
    "        counter += 1\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(\"Epoch: {}, Iteration: {}, Loss: {}\".format(epoch, i+1, current_train_loss))\n",
    "            current_train_loss = 0\n",
    "        \n",
    "    train_losses['loss_ae1'].append(loss_ae1_train / counter)\n",
    "    train_losses['loss_ae2'].append(loss_ae2_train / counter)\n",
    "    train_losses['loss_dyn'].append(loss_dyn_train / counter)\n",
    "    train_losses['loss_total'].append(epoch_train_loss / counter)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss_ae1_test = 0\n",
    "        loss_ae2_test = 0\n",
    "        loss_dyn_test = 0\n",
    "        epoch_test_loss = 0\n",
    "\n",
    "        encoder.eval()\n",
    "        dynamics.eval()\n",
    "        decoder.eval()\n",
    "\n",
    "        counter = 0\n",
    "        for i, (x_t, x_tau) in enumerate(test_loader,0):\n",
    "            # Forward pass\n",
    "            z_t = encoder(x_t)\n",
    "            x_t_pred = decoder(z_t)\n",
    "\n",
    "            z_tau_pred = dynamics(z_t)\n",
    "            z_tau = encoder(x_tau)\n",
    "            x_tau_pred = decoder(z_tau_pred)\n",
    "\n",
    "            # Compute losses\n",
    "            loss_ae1 = criterion(x_t, x_t_pred)\n",
    "            loss_ae2 = criterion(x_tau, x_tau_pred)\n",
    "            loss_dyn = criterion(z_tau, z_tau_pred)\n",
    "\n",
    "            loss_total = loss_ae1 + loss_ae2 + warmup * loss_dyn\n",
    "\n",
    "            epoch_test_loss += loss_total.item()\n",
    "\n",
    "            loss_ae1_test += loss_ae1.item()\n",
    "            loss_ae2_test += loss_ae2.item()\n",
    "            loss_dyn_test += loss_dyn.item() * warmup\n",
    "            counter += 1\n",
    "\n",
    "        test_losses['loss_ae1'].append(loss_ae1_test / counter)\n",
    "        test_losses['loss_ae2'].append(loss_ae2_test / counter)\n",
    "        test_losses['loss_dyn'].append(loss_dyn_test / counter)\n",
    "        test_losses['loss_total'].append(epoch_test_loss / counter)\n",
    "\n",
    "        if epoch >= patience and np.mean(test_losses['loss_total'][-patience:]) >= np.mean(test_losses['loss_total'][-patience:-1]):\n",
    "            break\n",
    "\n",
    "        if epoch >= config[\"warmup\"]:\n",
    "            scheduler.step(epoch_test_loss / counter)\n",
    "        \n",
    "    print(\"Epoch: {}, Train Loss: {}, Test Loss: {}\".format(epoch, epoch_train_loss / counter, epoch_test_loss / counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the models\n",
    "torch.save(encoder, os.path.join(config[\"model_dir\"], \"encoder.pt\"))\n",
    "torch.save(dynamics, os.path.join(config[\"model_dir\"], \"dynamics.pt\"))\n",
    "torch.save(decoder, os.path.join(config[\"model_dir\"], \"decoder.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if log_dir doesn't exist, create it\n",
    "if not os.path.exists(config[\"log_dir\"]):\n",
    "    os.makedirs(config[\"log_dir\"])\n",
    "\n",
    "# Save the losses as a pickle file\n",
    "with open(os.path.join(config[\"log_dir\"], \"losses.pkl\"), \"wb\") as f:\n",
    "    pickle.dump({\"train_losses\": train_losses, \"test_losses\": test_losses}, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
